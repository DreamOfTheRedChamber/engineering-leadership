- [Behavior](#behavior)
- [Cassandra](#cassandra)
  - [Multi-User chat room](#multi-user-chat-room)
  - [Ticket master](#ticket-master)
  - [Calendar](#calendar)
  - [TopK](#topk)
  - [Youtube](#youtube)
  - [Donation](#donation)
  - [Google drive](#google-drive)

# Behavior
请问亚麻recruiter嘉宾，能给个内推吗？感谢running away what?要慎用“running away from bad things” 吧听众能不能mute一下哪个老铁 mute一下啊😯Rong Yao弹琴的可以mu一下吗🙏就是可以你来我往的感觉吧Would you please give an example of a bar raising answer example?主持人可以先全部过一遍slice吗？而不是自己的follow up，这样子大家可以有个全局观，谢谢money incentive 可以搞笑的方式带过diversity and inclusion 的话题怎么回答我就怕我吹大劲儿我也是一定要提team work project 嗎？還是個人開發的也可以Frank, thank you very much! I appreciate it!吹牛就没输过个人开发可以，看你怎么讲了。比如说你参加solo hackathon，赢了，就很牛逼了所以做政府项目的要怎么描述謝謝Q: 对于刚毕业的学生，这些话题大部分都没有经历，HR或者recruiter如果面试NG， BQ questions他们应该会怎么问呢？ 还是按部就班的念稿吗，对于NG怎么准备BQ？因为没有工作经验，这些问题大部分都没有体会project或者side project的经验，都可以讲NG 可以说school project，里面也涉及到collaboration、communication 哇这种学校的东西和在工作中，完全不一样非要硬往上帖，只能编了实在不行参加一些hackathon吧，很真实的团队合作了，大部分hackathon 36-48小时。构思，分工，熬夜，meet deadline，pitching。等于是压缩了一个季度的工作能举个例子吗什么叫complex嗯嗯同问Paul 说到点子上了！Paul刚才说啥来着？错过了谢谢 Paul 补充！16 Leadership now!Shopify的特有吧Paul说尽量选一个大的project，能说明你的实力的，方便评级更高Project dive deep 是主要考察技术能力吧感谢ganxie可以讲完slide再一起提问吗今天有录像吗搞过sev0 算不算厉害厉害可以说 没有mistake吗？说了 会很尴尬吗：）如果sev0 不是你写出来的话会没有mistake不太真实blame game is on就说会尴尬么。。。这么答就挂了？有人这样回答我的，我选择没挂他这一题。。。这个故事好，背下来这个故事要强调tech context吗？这样来说， 总体 来说， 感觉我们就会招进来 差不多的人突然想 艺术类 面试 ，越奇葩，越个性 越好晚了，已经背下来了。能讲一下例子吗 fail的例子晚了，背完了已经不能背啥啊没人能crash我， 算法可以。不怕 找人多mock！ hoho戳中了说的就是我晚了，已经背了。上司的话需要analyze 但是你的customer 不管说什么必须要听例子太好了，深深印入了脑海里只有魔法才能打败魔法晚了，已经背下来了。这样会不会显得manager很笨你可以编一下，说是隔壁组的manager同问 感觉manager听到能挣钱很难会block你跟直接manager有conflict不太合适正常人早就同意了 你还和我争对啊有conflict一般都是 clarity不够，有个人没有拿到所有信息话说面试官是中国人的话是好事吗解释一下就行了，但是这样没scope啊。还是之前的故事好。都背下来背下来"有conflict一般都是 clarity不够，有个人没有拿到所有信息"
👍Wendy 牛！同样一件事情说到了manager level有没有5分钟中场休息，喝点水上个洗手间的?现在下半场都要结束了😃😂确实new grad太难了，很难讲leadership的Hr会给你发公司valuebq搞定亚麻 = 搞定一切公司对NG要求不会很高说实话，准备好了amazon的bq，其他公司的bq都稳了哈哈哈，Gigas也是也么想的上面说的可能有误导，我一会补充一下请问有过往其他行业工作经历的转专业学生，面试讲故事可以用以前的工作经历吗？还是必须要讲和tech有关的项目或经历？lol爱因斯坦是谁lol🤣太棒了好像是个政客爱泼斯坦他哥？夸父是个好同志大公司不需要招 那个level 的聪明人wendy好牛啊  wendy可以考虑开个小红书账号分享职场面试信息吗有时间的可以看看wiki和公司官网blog和公司的linkedin，然后我觉得每个glassdoor的positive评价也可以很好地看出一个公司地valuebe myself 我怕没人给offer爱因斯坦克be best myself 不用谢～👍 Paul Wendy 在理大牛👍哪家对人大于业务？麦霸leadership ownership ng可当作主人翁精神 肯定可以聊大家可以读一下Ben Horowitz写的一本书叫The hard thing about hard things，里面有一章节叫Take care of the people, the products and the profits - in that order职场俱乐部volunteer 很锻炼leadership 😀这是回应之前的问题“哪家对人大于业务？”又背过一个： 主动帮oncall engineer去解决livesite的incident其实每家公司都是“人大于业务”，也是“业务大于人”，取决于您的观察点对牛人，就是人大于业务， 如果您没有贡献，就是业务大于人，呵呵你就说系统本身的限制，不说别人的问题old system就是很菜觉得ownership跟之前的第二个category “tell me a project you are proud of.” 和第五个category “leadership”都有点overlap？ 老师们的例子听起来好像是有点overlap。请问怎样解决一个例子好像可以对应很多问题的情况呢？ 谢谢大家你还有第七页？earn trust?我参加的比较晚 请问能把ppt的几点title列一下吗？+1ppt没啥，没有老师讲的故事精彩反客为主6666这就是handle ambiguity把可以问回去amazon的interview feedback要写的很详细的，面试官主要是collect datapoint但是最后也不给feedback面试官会不会光听不写，然后回去编？😂不可能我从来没有见过这样的，那样问题就大了Amazon onsite behavior question  https://www.1point3acres.com/bbs/thread-307462-1-1.htmlearn trust是啥故事来着？比如我最近面试一个SDM， 光feedback都写了近2000 words请问我们有recording吗？来晚了，miss掉了前半部分😭如果来面Amazon， 最好真的好好读读LP，一定要不然很难过的What is LP?Leadership Principles每个问题对应的lp在这里找：Amazon onsite behavior question  https://www.1point3acres.com/bbs/thread-307462-1-1.html
From zhaozhonghao to Everyone: (8:59 PM)
想问一下，如果问到tell me about your weakness, 应该怎么说？
From Paul Lou to Everyone: (8:59 PM)
另一家重视BQ的公司是Apple，级别高一些的有一半或超过一半的面试是BQ
From Hobite Sun to Everyone: (8:59 PM)
https://www.kraftshala.com/blog/amazon-interview-questions/
From Hobite Sun to Everyone: (9:00 PM)
上面的链接是amzn问题对应的lp
From Betty Ho to Everyone: (9:00 PM)
謝謝
From Changzheng Rao to Everyone: (9:01 PM)
十分感谢🙏
From Kevin Wen to Everyone: (9:01 PM)
我们几乎不问“weakness” 了吧，如果被问了，就举一个真实的故事， 还有您怎么自己提高的， 或者如果是manager的话，也可以讲如何利用下属或者同事来check balance终归没有完美的人嘛
From Andrew to Everyone: (9:01 PM)
问的。我就被问到有什么hash feedback,harsh*
From Kevin Wen to Everyone: (9:02 PM)
Harsh feedback那个是“earn trust”的问题收到feedback，如果是又问题，怎么解决，最后earn the trust back
From Verity Chu to Everyone: (9:03 PM)
谢谢 Ken 老师
From lei chen to Everyone: (9:03 PM)
earn trust 是承认错误吗
From Tao Mao to Everyone: (9:03 PM)
很好的回答！
From Paul Lou to Everyone: (9:03 PM)
Kevin Wen很有经验
From Kevin Wen to Everyone: (9:03 PM)
如果是有问题，为嘛不承认？承认了改了就好
From W D to Everyone: (9:03 PM)
想问一下对于转码选手，讲的故事和cs不相关可以么？
From Anna to Everyone: (9:03 PM)
想问对于 handle tight deadline，Couldn’t finish tasks before deadline这类问题，有没有比较好的回答
From lei chen to Everyone: (9:04 PM)
就主要是不懂earn trust
From Kevin Wen to Everyone: (9:04 PM)
如果没有问题， 就是要怎么提供资料，对事不对人的解决
From lei chen to Everyone: (9:04 PM)
有点lost
From Kevin Wen to Everyone: (9:04 PM)
谢谢Paul
From Andrew to Everyone: (9:04 PM)
有错误也不能承认啊
From YL to Everyone: (9:04 PM)
help peers也是earn trust
From Kevin Wen to Everyone: (9:04 PM)
learnLearn  from your scar
From Becky to Everyone: (9:05 PM)
想请问一下bq和system design对定level哪个权重比较高呢？
From Verity Chu to Everyone: (9:05 PM)
谢谢老师们～
From Andrew to Everyone: (9:06 PM)
可不可以试试就知道。。
From Kevin Wen to Everyone: (9:06 PM)
如果您要到L6+， 一定要“Learn from your scar”
From Pencil to Everyone: (9:06 PM)
无关的意思不大
From Kevin Wen to Everyone: (9:06 PM)
不能是大错误哈
From Tony to Everyone: (9:07 PM)
亚麻最近coding题不算难
From Kevin Wen to Everyone: (9:07 PM)
您不能说我把AWS或者Azure搞down了
From Gigas to Everyone: (9:07 PM)
amazon 的大量是指多少呢
From Kevin Wen to Everyone: (9:07 PM)
那就麻烦了
From Yang to Everyone: (9:07 PM)
好像没有回答刚才的问题
From YL to Everyone: (9:07 PM)
500+吧
From May Zhu to Everyone: (9:07 PM)
earn trust 就是和团队契合，团队觉得你可以完成工作内容，还有会和团队相处融洽。基本上各种BQ都是考研你值不值得trust吧
From Andrew to Everyone: (9:07 PM)
500够吗
From Yang to Everyone: (9:07 PM)
问题问的是已经通过了coding test拿到面试的前提下，bq能不能用其他行业的工作经历来回答
From Kevin Wen to Everyone: (9:08 PM)
亚麻肯定可以
From jackson to Everyone: (9:08 PM)
可以的，就想有个人说组织meeting
From Tony to Everyone: (9:09 PM)
最近面亚麻每一轮都问behavior
From Yang to Everyone: (9:10 PM)
谢谢大家！
From Kevin Wen to Everyone: (9:10 PM)
亚麻BR对SDE一般还是要问一个简单的技术问题的
From Tao Mao to Everyone: (9:10 PM)
说没有！
From keira to Everyone: (9:10 PM)
谢谢老师们分享！ 辛苦！
From Betty Ho to Everyone: (9:11 PM)
謝謝今天的分享～受益良多
From Lu Li to Everyone: (9:11 PM)
谢谢老师们分享！两小时干货满满 收获很多！
From Isabella to Everyone: (9:11 PM)
谢谢大家
From Yang Bai to Everyone: (9:11 PM)
非常感谢老师！
From Kevin Wen to Everyone: (9:11 PM)
谢谢大家，我也很受教！！
From Sophie Chen to Everyone: (9:11 PM)
謝謝各位老师今天的分享～!!!
From May Zhu to Everyone: (9:12 PM)
非常感谢各位老师的分享，非常受用！
From Selena to Everyone: (9:12 PM)
对对 earn trust求解
From wz to Everyone: (9:12 PM)
Thank you Ken, Wendy & Frank! Thank you host!




# Cassandra
C* consistency levels: one -> quorum -> allmore options to chooseTypical, and more added later…CAP theorem: C vs. A tradeoffnot all company swap cassandra , some big company are still use itbut if you use it in critical data/transaction business, then cassandra is NOT the right choice请问 wide-column store 是啥意思这样讲听不懂没听到Cassandra的应用场景，请问早来的朋友点我几句Quote - “if you use it in critical data/transaction business, then cassandra is NOT the right choice”那cassandra有啥用。。Typical use cases: chat messages, logs, IoT data, etc.@Sh.W For cases that you need fast write: chat, event logs, etc用zookeeper保存的Service discovery感谢解答。那这个Cassandra跟mongodb啥的有啥区别，为啥要用cassandra存log没讨论过，后面可以问～行，谢谢。那我就认为存log吧。。。dns?你把前面放个reverse proxy？ lbhttps://teddyma.gitbooks.io/learncassandra/content/client/which_node_to_connect.html最近在看firebase，就是document db，和MongoDB一样可以很好地用来快速储存聊天信息active-passive lbactive-passive lb +10086这些东西应该都是cloud provider管吧？是的是ring0 infra管的跟application layer木啥关系https://www.ibm.com/docs/en/b2b-integrator/5.2?topic=system-installing-apache-cassandra-apache-zookeeper我觉得是问peer2peer怎么propogate 数据变化的query node 1， node1怎么决定找下一个node，如果本node无此数据。复制只是复制到replicatorvector 是啥version vectorquorum-based还能是LWW吗Cassandra没啥关系 看一下service discovery/service mesh一般怎么做能不能share 一下ppt我最后会发一个总结，slides会包含在里面，还有一些扩展阅读为什么update不行？？Cassandra released in 2008; dynamo in 2012 为啥说cassandra 抄dynamo？！Dynamo 不是dynamoDB请问可以share一下具体table长什么样吗？0基础有点看不懂啊。Dynamo在SOSP‘07发表的：https://www.allthingsdistributed.com/files/amazon-dynamo-sosp2007.pdf越删越多跟aws redshift有点像啊Ok cool 抄挺快！对 vacuumHBase 和RocksDB也都有类似合并机制，两者都是LSM treelog structure merge tree吗是的Log Structured Merge Treeprimary key = partition key + clustering key. clustering key is orderedDynamoDB   是single leader的 不是P2P根据CAP定理，只能同时满足两个，而由网络分区带来的分区错误风险是必然存在的，因此只能在CA中间选一个，Cassandra选择了AP那cassandra有什么应用场景，跟mongodb 应用场景有什么不同请问MySQL是什么呢？AC吗？@Neal yesCPCassandr一般用在高可用性的场景，即使整个集群就剩下一台机器了，也要能工作，用Raft等协议就保证不了这一点RAFT是啥  猴子哥跟 MySQL 集群具体的高可用和一致性方案有关吧？Raft一致性协议，比Paxos简化谢谢！系统设计如果遇到银行相关的得保证AC，也需要scalable，那他就没有partition tolerance吗？https://donggeitnote.com/2021/10/16/raft/我们前几周讲的raft这个讲raft讲的特别好
http://thesecretlivesofdata.com/raft/确切说，是newbie friendlynewbie dota best dota+1就是来听db选型的类似于sortkey的一个东西？denormalized吧column可以存json 吗column family, all columns within the same column family are stored sequentically on diskWikipedia has thiscolumn family, all columns within the same column family are stored sequentically on disk 牛逼牛逼A wide-column store is a type of NoSQL database. It uses tables, rows, and columns, but unlike a relational database, the names and format of the columns can vary from row to row in the same tablequery within the same column family is faster than row-based db.relational DB对column number有限制，Oracle是1000那为什么还要定义schema那这个wide-column和存json是一个意思吗？就是可以nest column吗？value can be json blob主要是在disk上的存储方式不同
[file: (null)]
Column family使用cluster key来定义的吗妈同事介绍 暴露了哈哈哈哈我也给你介绍一个好不好 答主大家还是注意保护好自己信息不要乱开玩笑哈列式存储同一个column family的data存成一个文件(key: value (key: value)) ?这个稳步不搞明白 不知道往里面存啥这个问题不搞明白 不知道往里面存啥NoSQL有好几种，也不一样parquet是可以的 col-orientedcolumnar storage format大佬最后可以讲一下为啥淘汰了😂找对人，
Hire and develop the best
哈哈
[file: (null)]
要deprecated 掉啊？业务导向Cassandra严重依赖gossip，这个有性能问题gossip好我们的Cassandra就10几个node。。超大规模的话 consistency就是bottleneck了答主负责。不用不乱说今天有个群里发过https://blog.softwaremill.com/what-is-wrong-with-apache-cassandra-materialized-views-a7a25431dad👍感谢感谢respect感谢 分享cogs？还是cost？Rebalancing 要花很久 我们也遇到过写烂了是个好的总结好几个小时印度人😂🤣现代分布式系统 不考虑consistency就是原罪那大部分nosql 都躺枪了spanner我上学的时候的proj就是simulate gossip。。。兰伯特论文写的烂我也想当个科学家 现在还来得及吗最近的道路是不是转数据科学家schemalessFB在MySQL上面包了一层: TAO为什么说Cassandra schemaless但是我们这里又定义了schema:Cassandra旧版本用的binary protocol Thrift-based API所以还支持schemaless。但是在新的实现里CQL+storage engine已经需要定义schema了。 https://stackoverflow.com/questions/63380973/in-cassandra-how-is-it-possible-to-save-data-in-a-column-name-while-leaving-the/63381339#63381339所以最好用的是不是mysql + shard？Tao这个名字很酷，难怪改名叫MetaThank you for praising my name.Tao， 66666666666666哈哈哈哈哈哈哈技术互喷transactional 是啥意思事务嘛事务是啥要么都成功，要么都失败，ACID甩锅失败 哈哈哈啊，ACID啊2pc懂了你的麦克风好像有点杂音transaction是指的DB的ACID有背景音风很大哈哈我挺赞同东哥的观点+1+1
From 非洲黑猴子 to Everyone: (7:58 PM)
+2
From weibo wang to Everyone: (7:58 PM)
同意
From Sh.W to Everyone: (7:59 PM)
东哥6666
From Zooey to Everyone: (7:59 PM)
教练：这叫意识流选手
From Sh.W to Everyone: (7:59 PM)
意识流是啥
From Mark Liu to Everyone: (7:59 PM)
头脑风暴，如果有怀疑的位置，提出来，然后暂停，继续后面。
From Michael Qiu to Everyone: (7:59 PM)
对 我们就互相默认大家都是菜鸡。
From YL to Everyone: (8:00 PM)
同意，都是学习的
From Sh.W to Everyone: (8:00 PM)
同意，都是学习的
From Zooey to Everyone: (8:00 PM)
我联动的活动我自己都不懂
From Sh.W to Everyone: (8:00 PM)
respect
From Zooey to Everyone: (8:00 PM)
🤣
From Sh.W to Everyone: (8:00 PM)
666666666999999999999
From 非洲黑猴子 to Everyone: (8:00 PM)
讨论才是大头，主要就从讨论中学习
From Cheng Jing to Everyone: (8:00 PM)
名词全都不知道的有✋
From Sh.W to Everyone: (8:00 PM)
6翻了
From Bei Z to Everyone: (8:00 PM)
respect+1
From Sh.W to Everyone: (8:00 PM)
讨论才是大头，主要就从讨论中学习
From Kevin Hu to Everyone: (8:01 PM)
谢谢！
From Sh.W to Everyone: (8:01 PM)
我也讨论时候学的东西好多append only妈？
From Becky to Everyone: (8:02 PM)
和mysql mvcc比较一下
From 非洲黑猴子 to Everyone: (8:03 PM)
SQL是从单机进化来的，不是原生分布式的，所以容易支持事务
From 木仓馆长 to Everyone: (8:04 PM)
transaction里面那个isolation也非常重要


## Multi-User chat room
From Michael to Everyone: (7:12 PM)
没声音？
From 2002079 Xi Zhou to Everyone: (7:12 PM)
有呀
From Qian Teng to Everyone: (7:12 PM)
+1
From lining to Everyone: (7:12 PM)
有
From yyh Ace to Everyone: (7:12 PM)
你要join audio吧
From A to Everyone: (7:13 PM)
啥时候开始。。
From Michael to Everyone: (7:13 PM)
OK 不小心点错了
From Ken to Everyone: (7:15 PM)
meeting notes: https://docs.google.com/document/d/1Hfnhg09v9ISJ20151u7PDTpvzqjpp5ajiNI5h-KevY0/edit#
From YL to Everyone: (7:16 PM)
这字体这么fancy吗
From Jian Zhu to Everyone: (7:17 PM)
看着好难受 - -
From Becky to Everyone: (7:19 PM)
@group member
From Jiabei Luo to Everyone: (7:19 PM)
开始时候介绍用的slide能share一下嘛？具体就是l4 l5 criteria的那一页。谢谢~
From A to Everyone: (7:20 PM)
又要开始算算术了吗
From Weida to Everyone: (7:20 PM)
感觉面试官不大兴奋？
From 2002079 Xi Zhou to Everyone: (7:21 PM)
lol
From Jiabei Luo to Everyone: (7:21 PM)
can you add additional people to a 1:1 chat?And turn it into group chat
From xinz to Everyone: (7:21 PM)
面试官是senior 吗？
From A to Everyone: (7:21 PM)
这是在reverse engineering 微信吗
From xing wang to Everyone: (7:22 PM)
大多数面试官都这样，把舞台交给应试者
From A to Everyone: (7:22 PM)
请向答主要个表情包feature，谢谢。
From 2002079 Xi Zhou to Everyone: (7:23 PM)
Edit  已经发送的信息 feature
From tomdi to Everyone: (7:23 PM)
2min内可以recall message
From Becky to Everyone: (7:23 PM)
Online status 要不要
From v to Everyone: (7:24 PM)
感觉这边很多聊天软件设计理念和wechat区别挺大。。。总以wechat作为出发点 可能非国人会confuse
From Eddie菜 to Everyone: (7:24 PM)
正在偷人......
From ningdi to Everyone: (7:24 PM)
要需求的这个聊天方式好棒
From Ken to Everyone: (7:24 PM)
Soft Skills:1: requirements gathering2: make decisions and justify tradeoffs3: describe the solution using clear presentation, concise language and accurate technical termsHard Skills:1: design quality; scalability, reliability, efficiency etc (L4, L5)2: basic facts about existing software solutions and hardware capabilities (L4 - partly, L5)3: project lifecycle awareness, e.g. how a project is developed and maintained (L5)
From Jiabei Luo to Everyone: (7:24 PM)
Edit or delete message (like discord) ?
From A to Everyone: (7:24 PM)
谁在偷人
From Will to Everyone: (7:24 PM)
请问record在哪里能看到？
From A to Everyone: (7:24 PM)
这么刺激
From AAA to Everyone: (7:25 PM)
exciting
From Anita Chen to Everyone: (7:25 PM)
一般來說和面試官confirim req應該佔多長時間呀？
From Robin to Everyone: (7:25 PM)
会不会聊太多requirement了，这些很难能在45min内聊清楚吧，我觉得能深入说清楚1:1chat和group chat就很不错了
From Weilong Ding to Everyone: (7:26 PM)
少于十分钟吧
From Becky to Everyone: (7:26 PM)
Retrieve chat history from local storage or remote?
From A to Everyone: (7:26 PM)
同感，我觉得requirement时间有点长
From Feng Gao to Everyone: (7:26 PM)
有以往的mock recording吗
From xing wang to Everyone: (7:26 PM)
计算存储量了吗？
From Richard Tu to Everyone: (7:26 PM)
确实感觉，如果45分钟总时间的话，更符合真实interview
From A to Everyone: (7:26 PM)
计算存储量为啥？硬盘很贵吗
From yingzhu to Everyone: (7:26 PM)
没见过这么久clarify的。。。
From kk to Everyone: (7:27 PM)
同意。感觉确认mvp就好了。
比如图片功能等等在最后optimization/extension的时候再聊吧
From 非洲黑猴子 to Everyone: (7:27 PM)
原来不就是个Multi user的聊天室吗？现在微信快设计全了，除了朋友圈
From YL to Everyone: (7:27 PM)
1+1， group，notification就可以讲很久了
From 2002079 Xi Zhou to Everyone: (7:27 PM)
一般clarification  大概多少分钟
From Feng Gao to Everyone: (7:27 PM)
NR 一般还会有个HA吧
From xinz to Everyone: (7:27 PM)
聊天信息的顺序要保证吧
From A to Everyone: (7:27 PM)
NR是啥，HA是啥
From Michael to Everyone: (7:27 PM)
real-time message应该是要的吧
From xinz to Everyone: (7:27 PM)
还有availability 也要保证吧
From Feng Gao to Everyone: (7:28 PM)
HA: High availability
NR: Non-functional requirement
From A to Everyone: (7:28 PM)
non-functional requirement直接跳过吧
From v to Everyone: (7:28 PM)
顺序只能保证每个人看到的order一样吧。。没办法global 保证顺序？
From A to Everyone: (7:28 PM)
说不说的吧
From YL to Everyone: (7:29 PM)
这咋能跳过呢..
From Jiabei Luo to Everyone: (7:29 PM)
顺序在毫秒级别差别应该没那么重要吧
From Michael to Everyone: (7:29 PM)
+1
From v to Everyone: (7:29 PM)
顺序不对会有些逻辑上的错误
From Sean Gao to Everyone: (7:29 PM)
顺序是不是在 server 端加 ts 作为 truth ？
From A to Everyone: (7:30 PM)
为啥不能跳过呢？ NR 有啥用呢？
From ningdi to Everyone: (7:30 PM)
server端加上也没办法保证ordering
From v to Everyone: (7:30 PM)
比如三个人群聊。。在c看来 a对b的回答比b对a的提问先到
From xing wang to Everyone: (7:30 PM)
45分钟吗？
From Sean Gao to Everyone: (7:31 PM)
@v 有道理
From Michael to Everyone: (7:31 PM)
感觉snowflake的id就已经满足大部分要求了。happend-before relation和绝对的顺序感觉还挺难的。
From YL to Everyone: (7:31 PM)
顺序只能在自己端看到的是一样的
From Fei to Everyone: (7:31 PM)
顺序无所谓的
From v to Everyone: (7:31 PM)
顺序能不能用vector clock来解决？
From A to Everyone: (7:31 PM)
顺序感觉不重要大差不差就得
From Fei to Everyone: (7:32 PM)
只要保证partial order就可以了
From A to Everyone: (7:32 PM)
partial ordre是啥意思
From AAA to Everyone: (7:32 PM)
以前微信好像也会出现信息错位的问题
From A to Everyone: (7:32 PM)
微信现在也有
From AAA to Everyone: (7:32 PM)
所以不用确保完全正确吧
From ningdi to Everyone: (7:32 PM)
顺序都保证不了。。 就不是聊天了。。
From Fei to Everyone: (7:32 PM)
就是A说了话，引起B说话，显示的时候A在B前面，因果关系
From Sean Gao to Everyone: (7:33 PM)
@v 想了想， b对a 的response，一定晚于 a 的问题。如果server 端排序，那不可能b比a先到。
From A to Everyone: (7:33 PM)
有时候信息都丢了
From ray to Everyone: (7:33 PM)
it's the hardest problem to ensure the high consistency
From Michael to Everyone: (7:33 PM)
@v 我觉得不用vector clock,简单的lamport clock就行。但是要是面试会不会太复杂。
From ningdi to Everyone: (7:33 PM)
我们以前做过测试，一个群里看到的消息确实可能不是一个顺序
From Ken to Everyone: (7:33 PM)
Meeting notes with QR code to join WeChat group (if you have not joined yet): https://docs.google.com/document/d/1Hfnhg09v9ISJ20151u7PDTpvzqjpp5ajiNI5h-KevY0/edit#
From zepengzhao to Everyone: (7:33 PM)
duplicate 那些是不是reliability的问题呢
From ray to Everyone: (7:33 PM)
duplicate is still consistency issue I think
From A to Everyone: (7:33 PM)
b的response一定晚于a的问题啊。如果a的问题没有deliver，b问题都没看到，怎么会发response？
From xing wang to Everyone: (7:33 PM)
用什么db讲了吗？
From A to Everyone: (7:33 PM)
这和设计没关系。。基本法啊
From Ender to Everyone: (7:34 PM)
但是deliver到c的顺序可能是b在a前面
From ray to Everyone: (7:34 PM)
I didn't see any multi thread topic popped yet
From v to Everyone: (7:34 PM)
对于a和b是没问题。。。对于c来说。。。 顺序是乱的
From YL to Everyone: (7:35 PM)
a和b同时向对方发送信息，两个人的顺序就是不一样的
From Sean Gao to Everyone: (7:35 PM)
@v c如果严格读取 group 的 ts，就不会
From v to Everyone: (7:35 PM)
顺序可能是乱的。。比如a的问题发给c的时候延时特别大
From Sean Gao to Everyone: (7:35 PM)
@yl 他说的是 b回复a 的信息，是有关联的。
From v to Everyone: (7:35 PM)
服务器的时间是不准确的可能会有回调
From Weilong Ding to Everyone: (7:35 PM)
timestamp是不可靠的
From v to Everyone: (7:35 PM)
除非用version id类似lambo clock
From Sean Gao to Everyone: (7:35 PM)
回调确实会
From A to Everyone: (7:36 PM)
这个delivery顺序没法保证，除非牺牲latency。 上一条msg没收到，你就不发下一条信息。这样太扯了。
From ningdi to Everyone: (7:36 PM)
那种延迟也是对于client端来说的，但是对于server端，不应该存在servers有不同的 ordering。
From A to Everyone: (7:36 PM)
而且delivery 有quality of service要求的。要求不高的直接qos=0发了就不管了，drop and go
From 姚剣楠 to Everyone: (7:37 PM)
用什么协议 需不需要提一下？websocket？
From Jerry to Everyone: (7:37 PM)
服务器时间一般什么情况会回调？
From Feng Gao to Everyone: (7:37 PM)
message应该就是用服务器端的timestamp 吧，存DB
From zepengzhao to Everyone: (7:37 PM)
我觉得要踩到点吧
From ray to Everyone: (7:37 PM)
I remember the wechat use the multi paxos
From zepengzhao to Everyone: (7:37 PM)
real time messaging
From A to Everyone: (7:37 PM)
这种message topic/queue都append only的吧，server发了， 爱收到不收到。。
From zepengzhao to Everyone: (7:37 PM)
paxos 是一致性的
From Weilong Ding to Everyone: (7:37 PM)
建议去看ddia有讲
From Feng Gao to Everyone: (7:37 PM)
message也要存数据库的吧
From ningdi to Everyone: (7:37 PM)
不同server的ts可能不一致。。感觉总是要把group放在一个server上 才能真的rely on ts
From Jackie G to Everyone: (7:38 PM)
Do we need to expand on UserMeta? What kind of metadata is stored?
From bernini to Everyone: (7:38 PM)
Ddia是书还是视频？
From Lucas Li to Everyone: (7:38 PM)
要对比一下通信方式，HTTP Polling, Long Polling, WebSocket之间的区别么
From zepengzhao to Everyone: (7:38 PM)
要很多machine maintain tcp connection （websocket）
From Weilong Ding to Everyone: (7:38 PM)
书
From Lucas Li to Everyone: (7:38 PM)
这种面试，是不是每道题目都要提前准备一下啊
From xing wang to Everyone: (7:38 PM)
用什么协议 需不需要提一下？websocket？这是重点，应该开始就讲
From zepengzhao to Everyone: (7:38 PM)
trade off 比较
From bernini to Everyone: (7:38 PM)
我感觉要来不及了。。。
From zepengzhao to Everyone: (7:39 PM)
基本上requirement 讲太久了
From Ken to Everyone: (7:39 PM)
presentation: https://docs.google.com/presentation/d/1pWuOkQrxk_Eib3oBwEGccXnqSZXisuoeSfdfToYLK7w/edit?usp=sharing
From Michael to Everyone: (7:39 PM)
websocket scale也得注意
From lining to Everyone: (7:39 PM)
用NTP同步Server时间不就行了
From 姚剣楠 to Everyone: (7:39 PM)
Nginx + WebSocket这里 有个坑 就是6w 端口限制 这块设计好了 会是加分
From zepengzhao to Everyone: (7:39 PM)
像fb 45分钟
From xing wang to Everyone: (7:39 PM)
要对比一下通信方式，HTTP Polling, Long Polling, WebSocket之间的区别么，，，，同意！
From v to Everyone: (7:39 PM)
websocket scale有啥问题？
From 姚剣楠 to Everyone: (7:39 PM)
nginx能支撑的websocket连接数最大只有 65535吧
From Sean Gao to Everyone: (7:40 PM)
对，如果server time 同步了， 还有 ts 的问题么 ？
From 姚剣楠 to Everyone: (7:40 PM)
》 websocket scale有啥问题？
From zepengzhao to Everyone: (7:40 PM)
10分钟 requirement 10分钟high level，然后剩下20分钟deep dive
From 姚剣楠 to Everyone: (7:40 PM)
Websocket 文件描述符数量的调整下吧
From Sean Gao to Everyone: (7:40 PM)
tcp 链接是 5元组 判定唯一， 65535好像不是平静。
From ningdi to Everyone: (7:40 PM)
Server ts咋同步。。 每时每刻都syc吗。。 多个server如何确定谁是master的ts
From 姚剣楠 to Everyone: (7:40 PM)
每打开一个tcp链接 占用一个文件描述符
From Sean Gao to Everyone: (7:40 PM)
瓶颈
From Michael to Everyone: (7:41 PM)
@v 你得记录下哪个client在哪个websocket server上或者，client连上websocket server之后得subscript一个topic
From Sean Gao to Everyone: (7:41 PM)
@ningdi 好像有专门的协议
From lining to Everyone: (7:41 PM)
NTP
From v to Everyone: (7:41 PM)
我之前看好像一个server可以hold up to 1million websocket connection？
From zepengzhao to Everyone: (7:41 PM)
要先有个server discover 吧
From Sean Gao to Everyone: (7:41 PM)
@v 那个是 WhatsApp 的 erlang
From v to Everyone: (7:41 PM)
对。。需要存下connection的信息
From ningdi to Everyone: (7:42 PM)
@Sean, 那time draft的情况也会发生吧 难道还能修复已经persist 到db的records？
From Sean Gao to Everyone: (7:42 PM)
@ningdi 细节我不懂。。。
From v to Everyone: (7:42 PM)
erlang是个类似websocket的协议么
From 姚剣楠 to Everyone: (7:42 PM)
Websocket的连接量不是瓶颈 百万应该也没问题 但是前面要是有nginx 那nginx的6w5端口数 就是瓶颈了
From ray to Everyone: (7:42 PM)
right
From Yijie Shen’s iPhone to Everyone: (7:42 PM)
Scale 可以弄多个websocket handler 吗？
From Sean Gao to Everyone: (7:43 PM)
thanks Jiannan
From ningdi to Everyone: (7:43 PM)
Nginx会成为battleneck？ 是因为websocket 这个协议导致的吗？
From Lucas Li to Everyone: (7:43 PM)
websocket连的是http 服务器一个机器大概50K个连接左右？
From 姚剣楠 to Everyone: (7:44 PM)
https://blog.51cto.com/u_15300443/3091841 这里有人也处理过这个坑
From ray to Everyone: (7:44 PM)
how many tcp connections are available for one server? theoretically?
From Lucas Li to Everyone: (7:44 PM)
有个著名的10K问题
From ningdi to Everyone: (7:44 PM)
666 感谢
From YL to Everyone: (7:44 PM)
这不应该发送到group然后再发送给每个user吗
From Lucas Li to Everyone: (7:45 PM)
后来有100K,1M50K应该没有问题
From zepengzhao to Everyone: (7:45 PM)
好像后端可以做成pub/sub
From 姚剣楠 to Everyone: (7:45 PM)
how many tcp connections are available for one server? theoretically? 如果你内存 cpu够大 几百万是完全没问题的
From A to Everyone: (7:45 PM)
group chat 必然是push啊
From Sean Gao to Everyone: (7:45 PM)
NTP server time sync
NTP is intended to synchronize all participating computers to within a few milliseconds of Coordinated Universal Time (UTC). It uses the intersection algorithm, a modified version of Marzullo's algorithm, to select accurate time servers and is designed to mitigate the effects of variable network latency.
From zepengzhao to Everyone: (7:45 PM)
group chat里面的参与者都subscribe到某个conversation
From A to Everyone: (7:45 PM)
肯定是pub、sub，一个groupchat就是一个topic
From 姚剣楠 to Everyone: (7:45 PM)
我试过 把websocket服务器的文件描述符改成200w 一台也能处理
From ray to Everyone: (7:46 PM)
google global database use time stamp for strong consistency
From tomdi to Everyone: (7:46 PM)
whatspp 一个server可以 5M connection
From zepengzhao to Everyone: (7:46 PM)
要clarify 不会有很多人
From 姚剣楠 to Everyone: (7:46 PM)
2. 文件描述符数量   可能需要调整内核参数，文件描述符的数量其实也是和内存相关的，因为每打开一个tcp连接，就得占用一个文件描述符。   内核参数：fs.file-max   这是和系统资源相关的，也不会是瓶颈
From zepengzhao to Everyone: (7:46 PM)
fanout太多人的话就会有performance问题
From ray to Everyone: (7:46 PM)
NTP is quite an important module for server
From 姚剣楠 to Everyone: (7:46 PM)
搬运工 供参考
From zepengzhao to Everyone: (7:46 PM)
这其实跟new feed的道理差不多fanout
From v to Everyone: (7:47 PM)
感觉最开始提需求挖坑太多了
From Robin to Everyone: (7:47 PM)
+1 挖坑太多了
From Lucas Li to Everyone: (7:47 PM)
需求跟面试官都确认过的吧
From zzb to Everyone: (7:47 PM)
是的 应该就简单的 use case 开始做
From ningdi to Everyone: (7:47 PM)
nginx最多只能维持(65535*后端服务器IP个数)条websocket的长连接-> 意思是 我加很多台机器 其实也不算是啥瓶颈咯。 正常来说希望一个机器处理多少connection比较合适呢？
From A to Everyone: (7:47 PM)
IBM 的chatting broker可以handle 最多100万个session
From Lucas Li to Everyone: (7:48 PM)
是不是面试官点头的，都要讨论啊
From A to Everyone: (7:48 PM)
为啥用websocket？websocket协议有啥优势吗？
From xing wang to Everyone: (7:48 PM)
超时了吗？有人记录时间吗？
From Lucas Li to Everyone: (7:48 PM)
双向通信
From Andrew Hou to Everyone: (7:48 PM)
好奇的问下 multi user chat的系统设计 不应该是focus on 系统设计上吗，感觉现在是在说多人聊天的功能逻辑
From zepengzhao to Everyone: (7:48 PM)
而且很重要一点，好像把面试官当coworker会比较好， 而不是给他找个solution
From A to Everyone: (7:48 PM)
这里面没必要双向通信
From ningdi to Everyone: (7:48 PM)
说实话 面试官给的这个requirement 我都觉得没必要用websocket了  long pulling貌似都能处理的了
From zepengzhao to Everyone: (7:48 PM)
T5
From kk to Everyone: (7:49 PM)
聊天为什么没必要双向。。
From zepengzhao to Everyone: (7:49 PM)
要求drive， 还有如何应对feedback
From v to Everyone: (7:49 PM)
long pulling和web socket之前的pro con分部是啥？
From A to Everyone: (7:49 PM)
因为你是在和server 双向通信，不是sender和receiver
From zepengzhao to Everyone: (7:49 PM)
都是lantency
From Yijie Shen’s iPhone to Everyone: (7:49 PM)
Rea time 是不是用web socket 比较好
From bernini to Everyone: (7:49 PM)
开销太大？
From A to Everyone: (7:49 PM)
可以decouple sender and receiver
From v to Everyone: (7:49 PM)
啥情况下long pulling比较好？
From zepengzhao to Everyone: (7:49 PM)
websocket latency最短
From Lucas Li to Everyone: (7:49 PM)
间隔短了服务器吃不消，间隔长了体验差
From kk to Everyone: (7:49 PM)
并没有long pulling比较好情况。long pulling直接http，比较容易实现。懒人专用。、
From bernini to Everyone: (7:50 PM)
发太多request了
From ningdi to Everyone: (7:50 PM)
这个requirement 感觉有conflict 一方面real time 一方面又 不login 不收消息。。。
From Robin to Everyone: (7:50 PM)
wesocket开销小，不需要每条message都要新开connection
From 姚剣楠 to Everyone: (7:50 PM)
作chat room ，websocket 或者SSE都可以，long pulling 没有优势吧
From v to Everyone: (7:50 PM)
是啊。。感觉websocket总是比long pulling好
From Feng Gao to Everyone: (7:50 PM)
感觉没时间设计storage了
From zepengzhao to Everyone: (7:50 PM)
不login不收notification
From 姚剣楠 to Everyone: (7:50 PM)
Websocket 熟悉框架的话 其实开发也很快
From Andrew Hou to Everyone: (7:50 PM)
多人聊天肯定 不是real time 第一 设计 notification 第二 异步推送
From ray to Everyone: (7:50 PM)
why not real time for group chat
From A to Everyone: (7:50 PM)
你看她的设计，明显这个backend是作为一个broker出现的
From kk to Everyone: (7:50 PM)
Long pulling ne
From A to Everyone: (7:50 PM)
没必要双向通信
From Lucas Li to Everyone: (7:50 PM)
这里的login应该和online两码事
From kk to Everyone: (7:50 PM)
能做到的，ws都能做到。
From Jerry to Everyone: (7:51 PM)
group chat是不是要分在线的和离线的两波用户讨论
From Michael to Everyone: (7:51 PM)
发送给在线user和离线user应该不一样吧
From v to Everyone: (7:51 PM)
为啥要用message queueKafka能支持这么多topic么
From Lucas Li to Everyone: (7:51 PM)
解耦
From lining to Everyone: (7:51 PM)
1 to 1要用pub、sub吗？
From kk to Everyone: (7:51 PM)
没必要。
From 姚剣楠 to Everyone: (7:51 PM)
Kafka能支持这么多topic么 同样的疑问
From A to Everyone: (7:51 PM)
解耦，高端词汇
From v to Everyone: (7:52 PM)
这为啥需要decouple
From Feng Gao to Everyone: (7:52 PM)
我也觉得有点奇怪，message为啥不放DB
From zzb to Everyone: (7:52 PM)
这里用不用socket 是具体实现问题 面试candidate 应该把重心放在模块上面 有哪些 数据类型 哪些数据DB 怎么跟系统交互 这里讲清楚
From v to Everyone: (7:52 PM)
感觉这么多topic kafka性能会有影响
From 姚剣楠 to Everyone: (7:52 PM)
每个聊天室 或者 1 对1的聊天 都抽象成websocket里面的一个channel
From ray to Everyone: (7:52 PM)
maybe use redis for the in-cache mem
From lining to Everyone: (7:52 PM)
如果 1 to 1 用pub, sub,那得多少topic
From Lucas Li to Everyone: (7:52 PM)
MQ先进先出，吞吐量大
From zepengzhao to Everyone: (7:52 PM)
可以做一个conversation 啊， 聊天参与者是conversation subscribers
From kk to Everyone: (7:52 PM)
chat server用mq，是没必要的。
From Yijie Shen’s iPhone to Everyone: (7:53 PM)
对方离线的时候 可以把msg 放到kafka, 在线的时候用websocket
From zepengzhao to Everyone: (7:53 PM)
101， group chat都可以吧
From ningdi to Everyone: (7:53 PM)
1:1也topic n^2 topic
From v to Everyone: (7:53 PM)
没必要啊。。。直接query一下就行了
From A to Everyone: (7:53 PM)
chat server 用mq很常见
From ray to Everyone: (7:53 PM)
too many topics
From yingzhu to Everyone: (7:53 PM)
有websocket了是不是没必要mq了？
From v to Everyone: (7:53 PM)
没必要缓存啊
From ningdi to Everyone: (7:53 PM)
ws跟mq不冲突吧
From ray to Everyone: (7:53 PM)
message queues looks like used for service to service deliverynot for the user to user chat :)
From zepengzhao to Everyone: (7:53 PM)
too many topics可以infra解决吧
From kk to Everyone: (7:53 PM)
mq用在聊天很常见？
From Lucas Li to Everyone: (7:54 PM)
service来不及怎么处理啊
From bernini to Everyone: (7:54 PM)
实测wechat也丢消息
From kk to Everyone: (7:54 PM)
我表示怀疑。
From sherry的 iPhone to Everyone: (7:54 PM)
如果mq jam了怎么办
From zepengzhao to Everyone: (7:54 PM)
too many topic有问题吗
From A to Everyone: (7:54 PM)
mq可以1. 吧那些没有及时处理的请求存在mq里 2. 把messagelog存进queue里，用来做之后的分析 和存储，作为一个append only log存在
From bernini to Everyone: (7:54 PM)
离线sync写出过问题
From Lucas Li to Everyone: (7:54 PM)
水平切分
From A to Everyone: (7:54 PM)
水平切分是啥意思
From xing wang to Everyone: (7:55 PM)
今天的面试官太nice了
From ray to Everyone: (7:55 PM)
maybe the user pull the messages directly from the in-mem cache,?
From zepengzhao to Everyone: (7:55 PM)
而且没有问消息要存云端不
From A to Everyone: (7:55 PM)
horizontal sharding?
From zepengzhao to Everyone: (7:55 PM)
都没说好
From A to Everyone: (7:55 PM)
要cache干啥？
From 姚剣楠 to Everyone: (7:55 PM)
直接存用户的聊天记录 不会有法律问题吗
From v to Everyone: (7:55 PM)
这设计write fanout amplification也太大了。。。五百人的群 每个消息都写500份到kafka
From Yijie Shen’s iPhone to Everyone: (7:55 PM)
面试官输出的比较少，大多数面试都是这样吗？
From ningdi to Everyone: (7:56 PM)
1:1要是用了mq， 想象一下2个人加了好友，然后发消息，现场创建topic？ 如果是pre set topic， 那么你有5m的用户，他们每个人都有1000个好友，你要创建5km的topic
From zepengzhao to Everyone: (7:56 PM)
500 fanout不算大吧主要怕millions比较川普粉丝mllions
From A to Everyone: (7:56 PM)
用户信息必须存啊
From YL to Everyone: (7:56 PM)
我觉得要看你怎么和面试官交流吧
From zepengzhao to Everyone: (7:56 PM)
500 用多个worker就可以了
From kk to Everyone: (7:56 PM)
这样topic一多。。
From sherry的 iPhone to Everyone: (7:56 PM)
不需要创造500个topic吧 500个user subscribe一个topix不结了
From 姚剣楠 to Everyone: (7:56 PM)
1:1要是用了mq， 想象一下2个人加了好友，然后发消息，现场创建topic？ 如果是pre set topic， 那么你有5m的用户，他们每个人都有1000个好友，你要创建5km的topic。 同意 kafka大部分时间在忙着建立和删除topic
From Lucas Li to Everyone: (7:56 PM)
一个用户一个topic就可以了吧
From kk to Everyone: (7:56 PM)
kafka顶得住？我表示怀疑。
From A to Everyone: (7:56 PM)
你要不放心就hash一下，想看的时候偷偷看
From ray to Everyone: (7:57 PM)
kafka ding bu zhu
From A to Everyone: (7:57 PM)
一个用户就是一个topic
From ray to Everyone: (7:57 PM)
how many memory it would be for only creating one new topic?
From ningdi to Everyone: (7:57 PM)
一个用户一个topic 也不现实。
From A to Everyone: (7:57 PM)
topic是tree 结构的，/a/b/c/d/e/牛逼闪闪
From Jiayue(Hubert) Wu to Everyone: (7:57 PM)
不需要每个消息存500份吧 ，存一份再push到所有人
From YL to Everyone: (7:57 PM)
面试官好像笑出来了
From zepengzhao to Everyone: (7:57 PM)
topic多infra解决，时间到了
From Lucas Li to Everyone: (7:58 PM)
想象1000台机器，每台机器10000个topic
From 姚剣楠 to Everyone: (7:58 PM)
Web socket 应该很容易解决上面kafka那些问题。。 直接用socket不好吗
From ningdi to Everyone: (7:58 PM)
kafka broker已经可以处理这么多topic了？
From Jerry to Everyone: (7:58 PM)
但是用户离线的情况怎么办
From Lucas Li to Everyone: (7:58 PM)
http server来不及处理怎么办啊
From YL to Everyone: (7:58 PM)
离线通过notification sever解决吧
From Michael to Everyone: (7:59 PM)
browser没法用socket
From Lucas Li to Everyone: (7:59 PM)
notification server来不及处理怎么办啊
From Feng Gao to Everyone: (7:59 PM)
image/video应该是没法放进DB的。需要blob storage
From xing wang to Everyone: (7:59 PM)
讲了为啥不用kv吗
From Richard Tu to Everyone: (7:59 PM)
不好意思，我可能错过了什么documentDB干嘛的？
From bernini to Everyone: (7:59 PM)
一般都是socket，法request太昂贵了
From 姚剣楠 to Everyone: (7:59 PM)
是不是预估一下网络traffic比较好 10m用户同时在线 你发一条消息，会广播给同组的人 最多扩大500倍 其实用户多起来 traffic也是蛮大
From bernini to Everyone: (7:59 PM)
kv range search太贵
From 非洲黑猴子 to Everyone: (7:59 PM)
附件用对象存储就好，还能压缩转码
From 姚剣楠 to Everyone: (7:59 PM)
我们组的服务上周刚被 4gps每秒的攻击 挤爆网络
From Lucas Li to Everyone: (8:00 PM)
多媒体发送给S3，消息里面放个链接就可以了
From ningdi to Everyone: (8:00 PM)
你们组没有黑名单吗。。
From Yijie Shen’s iPhone to Everyone: (8:00 PM)
Attachment 比如image 或者video 是不是用S3比较好啊
From Enze to Everyone: (8:00 PM)
mqtt 可以支持大量topics
From Feng Gao to Everyone: (8:00 PM)
这个backend就一个service。。。。
From A to Everyone: (8:00 PM)
终于有人提mqtt了我就知道lol
From ray to Everyone: (8:01 PM)
how does the message delivered cross the region, like one person send at asia, and another one received at northamerica, is there any replication in this case?
From A to Everyone: (8:01 PM)
附件用对象存储就好，还能压缩转码 ，猴子哥，对象存储是什么？
From A to Everyone: (8:01 PM)
object storage吗？ s3？
From Lao luo to Everyone: (8:01 PM)
是group chat的话，说大概要多少个group吗？10 M user 不说明group就多吧
From 非洲黑猴子 to Everyone: (8:02 PM)
存文件的，kv，拿着key就能找到云上的文件
From ningdi to Everyone: (8:02 PM)
上来就aws全家桶 基本啥也不是瓶颈 啥也不是问题了 哈哈哈
From Lao luo to Everyone: (8:02 PM)
Kafka确实有topic
From Sean Gao to Everyone: (8:03 PM)
@ray 存到 global NoSQL， reqplicate 到 其region ？
From xing wang to Everyone: (8:03 PM)
上来就aws全家桶 基本啥也不是瓶颈 啥也不是问题了 哈哈哈，，，有取巧之嫌
From YL to Everyone: (8:03 PM)
45分钟的话时间已经到了…实际应该只有不到半小时吧
From Lucas Li to Everyone: (8:03 PM)
S3设计又是一道面试题
From ningdi to Everyone: (8:03 PM)
gfs
From ray to Everyone: (8:04 PM)
there should be some sycronize process to ensure the consistency of messages for group chat
From A to Everyone: (8:04 PM)
s3好啊，背过了，多谢非洲黑猴子哥
From Lucas Li to Everyone: (8:04 PM)
实在不行把多媒体的需求略过
From Richard Tu to Everyone: (8:04 PM)
我觉得上面也是我想问的问题。架设你作为candidate，对aws全家桶特别熟，但是从面试官角度来说，是面试官想要的吗
From A to Everyone: (8:04 PM)
附件dedup也要做一下
From bernini to Everyone: (8:04 PM)
实测所媒体跨区域sync延迟极大
From Zhengguan Li to Everyone: (8:04 PM)
receiver不就是MQ consumer?
From ray to Everyone: (8:04 PM)
there is tunnelfor cross region network
From A to Everyone: (8:05 PM)
tunnel是啥
From ray to Everyone: (8:05 PM)
it would be supper high speed
From Lucas Li to Everyone: (8:05 PM)
In the first test, we set up a Kafka cluster with 5 brokers on different racks. In that cluster, we created 25,000 topics, each with a single partition and 2 replicas, for a total of 50,000 partitions. So, each broker has 10,000 partitions. We then measured the time to do a controlled shutdown of a broker. The results are shown in the table below.https://blogs.apache.org/kafka/entry/apache-kafka-supports-more-partitions
From Becky to Everyone: (8:05 PM)
Receiver service 是单机流吗
From A to Everyone: (8:06 PM)
那是个aws asg，里面有1万个vm
From ray to Everyone: (8:06 PM)
we are using the chat function of zoomLOL
From v to Everyone: (8:06 PM)
lol
From ray to Everyone: (8:06 PM)
and we can deliver real time video audio globally
From A to Everyone: (8:06 PM)
zoom牛逼
From zepengzhao to Everyone: (8:06 PM)
message -> using conversation_id to get chat participants -> get participant’s web socket sessions -> push message to participant’s machine -> ws -> user device
From Lucas Li to Everyone: (8:07 PM)
ZOOM同时在线的用户有限
From ray to Everyone: (8:08 PM)
there is roles and permissions setup for the chatespecially for the group
From A to Everyone: (8:08 PM)
IAM 答主没说
From ray to Everyone: (8:08 PM)
the admin to create group, delete group
From A to Everyone: (8:08 PM)
要了那么多requirement，来不及说了
From Jerry to Everyone: (8:09 PM)
consume那一步都做什么操作
From lining to Everyone: (8:09 PM)
requirement数量要自己控制吗？
From Zhengguan Li to Everyone: (8:09 PM)
请问一下 面试的时候也有条件画图嘛
From ray to Everyone: (8:09 PM)
if it is message queue,  it is pulling message from the topic
From YL to Everyone: (8:09 PM)
画的
From Jerry to Everyone: (8:09 PM)
group的聊天记录接受者要用什么DB存取
From ray to Everyone: (8:10 PM)
so it is quite a drawback of pulling, since it is hard to balance the produce and consume
From Sean Gao to Everyone: (8:10 PM)
lsm 应该是
From Lucas Li to Everyone: (8:10 PM)
pub/sub是不是可以给服务发消息啊
From ray to Everyone: (8:10 PM)
there is push style
From YL to Everyone: (8:10 PM)
我感觉还是应该用wide column DB
From Jerry to Everyone: (8:10 PM)
message queue会有容量上限吗? 到了上限这么办
From aaa to Everyone: (8:11 PM)
时间差不多到了
From ray to Everyone: (8:11 PM)
If everyone likes to use aws, we can have interview like how to design aws servce like s3, documentDB
From YL to Everyone: (8:11 PM)
时间都超了
From Andrew Hou to Everyone: (8:11 PM)
我觉得这个是一个设计流程图 而不是设计系统
From Ken to Everyone: (8:11 PM)
Started 7:13
From shawnzech to Everyone: (8:11 PM)
....
From Zhengguan Li to Everyone: (8:12 PM)
有上限 queue depth(# of messages), 或者size(e.g. 5M)啥的
From Jiabei Luo to Everyone: (8:12 PM)
Why not use noSQL DB?
From ray to Everyone: (8:12 PM)
good point
From Sean Gao to Everyone: (8:12 PM)
一定是吧，写多，少edit，时间排序
From ray to Everyone: (8:12 PM)
and there is also blacklist friend functionality of wechat, LOL
From Richard Tu to Everyone: (8:13 PM)
1小时正好
From lining to Everyone: (8:13 PM)
到点了
From Jerry to Everyone: (8:13 PM)
这好像还没说message存储是怎么partition的吧?
From Peng Wang to Everyone: (8:14 PM)
meta data是什么？
From YL to Everyone: (8:14 PM)
除了by UserId还有什么partition的方法？
From A to Everyone: (8:14 PM)
就是facebook的data
From ray to Everyone: (8:14 PM)
metadata is like some control plane data, like configuration of the message queue
From lining to Everyone: (8:15 PM)
LOL
From ray to Everyone: (8:15 PM)
facebook has no face anymore, lol
From 非洲黑猴子 to Everyone: (8:15 PM)
为了元宇宙，脸都不要了
From A to Everyone: (8:15 PM)
给大包也得去啊
From AAA to Everyone: (8:16 PM)
😂
From A to Everyone: (8:16 PM)
求原宇宙公司内推
From Pencil to Everyone: (8:16 PM)
That’s a good question
From AAA to Everyone: (8:16 PM)
求抱大腿
From bernini to Everyone: (8:16 PM)
上次E5，fb挂我sd，加面过了都不给去， 结果几个月后大放水
From A to Everyone: (8:16 PM)
e5大佬 wow
From AAA to Everyone: (8:17 PM)
大佬什么背景
From Jiabei Luo to Everyone: (8:17 PM)
放水怎么说？
From A to Everyone: (8:17 PM)
放水都不要我😭要求降下来了现在还放水吗在吧？E5 要求变E4?我疫情刚爆发时候面的312要求变E4，被拒绝，找你就是要在找E5132132132132132132321132132132132132132132132132132132123123How much time should we spend in gathering requirements?123123123123123123213
From Peijin to Everyone: (8:19 PM)
213
From 非洲黑猴子 to Everyone: (8:19 PM)
213
From Kevin to Everyone: (8:19 PM)
123
From 2002079 cici to Everyone: (8:19 PM)
213
From lining to Everyone: (8:19 PM)
213
From A to Everyone: (8:19 PM)
1
From johnc to Everyone: (8:19 PM)
213
From Patrick to Everyone: (8:19 PM)
21
From Spin to Everyone: (8:20 PM)
21
From ningdi to Everyone: (8:20 PM)
2
From Qianwen Huang to Everyone: (8:20 PM)
21
From ray to Everyone: (8:20 PM)
hard skill 213
From yinghuaguan to Everyone: (8:20 PM)
2
From ningdi to Everyone: (8:21 PM)
拿需求那步真的秀。 学到了
From A to Everyone: (8:21 PM)
面试官应该scope down到chat service only
From lining to Everyone: (8:22 PM)
可能对wechat太熟了
From A to Everyone: (8:22 PM)
答主问了太多requirement clarification，面试官需要控制一下
From Jian Zhu to Everyone: (8:22 PM)
这些应该是面试者自己控制啊
From bernini to Everyone: (8:22 PM)
需求花了太多时间了
From ningdi to Everyone: (8:22 PM)
可能要我 我就focus在消息送达 上面了。。 其他的根本不care 😂
From 非洲黑猴子 to Everyone: (8:22 PM)
面试官控场一下
From AAA to Everyone: (8:23 PM)
抱歉，发错了
From A to Everyone: (8:23 PM)
需要面试官控场
From AAA to Everyone: (8:23 PM)
有回放么其实我觉得真实面试情况下，还是不能依靠面试官。如果面试官本身就不postive，candidate还是得尽量自己控制时间面试官，自信点。是！如果自己不问需求，会不会失分我觉得ballpark 计算dau之类的感觉没啥用 面试时能跳过吗？我面一次amazon 真的2场遇见风格完全不一样的面试官。。一个小白哥贼积极，一个亚裔就根本不鸟我，全是我在bb要是我以后当面试官 我觉得ballpark计算可以直接跳过了大多数面试官都这样，把舞台交给应试者ballpark计算最没用直接跳过浪费时间我也觉得那些计算没啥用可以直接问面试官 我不说会不会扣分吗？貌似完全没有api design 😂除非对硬件的capability有数。要不然就是白费劲如果后面设计db的时候要用到，可以再提一下对哦
From Zhengguan Li to Everyone: (8:26 PM)
protocol是Http long polling, websocket啥的嘛
From Jiabei Luo to Everyone: (8:26 PM)
没有api design只有object design hh
From Michael to Everyone: (8:26 PM)
应该是receiver端real time chat
From Michael Qiu to Everyone: (8:26 PM)
chat needs bi-directional communication
From bernini to Everyone: (8:27 PM)
websocket不用一直握手，开销小
From 非洲黑猴子 to Everyone: (8:27 PM)
底层netty可以考虑
From Sean Gao to Everyone: (8:28 PM)
java 估计性能还是差了点， cpp or c or erlang ？
From A to Everyone: (8:28 PM)
netty是啥 猴子哥
From bernini to Everyone: (8:28 PM)
nosql？
From v to Everyone: (8:28 PM)
面试官讲一下kafka在这里用可以么？ 这么多的topic可以支持么
From bernini to Everyone: (8:28 PM)
那重新登录的时候拉history会不会很慢？
From Michael to Everyone: (8:28 PM)
是那个java 的 nio 模型framework?
From 非洲黑猴子 to Everyone: (8:29 PM)
底层用来做RPC层的框架，spark、flink、dubbo都在用
From Sean Gao to Everyone: (8:29 PM)
是的 nio framework
From 非洲黑猴子 to Everyone: (8:29 PM)
Kafka高冷，人家自己写的网络层
From A to Everyone: (8:29 PM)
rpc的protocol 很轻量吗 ？
From zepengzhao to Everyone: (8:30 PM)
group chat fanout 到底在哪里好可以讨论下吗
From Jiabei Luo to Everyone: (8:30 PM)
history 不能in memory cache 一段 吗
From zepengzhao to Everyone: (8:31 PM)
是不是有五十个人的chat room， 我们把一条message fanout成50条，然后每条conv_id一样，然后reciepient不一样呢
From A to Everyone: (8:31 PM)
history为什么要放在cache
From 非洲黑猴子 to Everyone: (8:31 PM)
可以，在client端都可以缓存historymessage
From ningdi to Everyone: (8:31 PM)
Fanout说白了就是 有n个消失 for each 写给m个人消息*
From A to Everyone: (8:31 PM)
你的chat history都是存在本地的吧
From Jiabei Luo to Everyone: (8:31 PM)
我是说clientside localstorage 这种对
From Zhengguan Li to Everyone: (8:31 PM)
protocol和kafka这两个点怎么关联起来?
From zepengzhao to Everyone: (8:32 PM)
chat history要requirement的时候讲清楚
From Lao luo to Everyone: (8:32 PM)
有谁解决过动态创建topic太多的问题了吗？我们现在就碰到类似的问题
From xinz to Everyone: (8:33 PM)
内部service 之间的通信是用RPC 吗？
From A to Everyone: (8:33 PM)
非洲黑猴子 大神，大家挺好大家听好请问 token 是存在哪里的token存在本地没错所以还是用的topic吗？ 每个group chat一个topic？SQL too expensive我觉得要pub/sub + message handler + message queue比较robust我还是持反对意见what? SQL?user必须用sql啊Message 用sql 写太慢了吧message queue只有一个subscriber我第一次听说NoSQL 存uder tablemessage可以存noSQLmessage用sql不好吧user group可以存SQL分开呀message写sql，能慢到哪去呢，我不觉得会特别慢Message 用nonsqldb又不贵，多来几个dbnosql用bigtable稳得很，widecolumemessage 用 NoSQL, 图片用s3， 其它用sqlsql主要是scalability不太好, message太多，分片存NoSQL挺好的对 存历史消息 可以用非同期处理why the user table has to use sql?sorry I didn't get itIs an object store a database?Cuz user -group needs relational associationThere is no point for SQL unless you need to join multiple tables当你需要查看你的微信群里都有谁的时候，叫啥，长得好看不好看，你需要sql我不同意任何说SQL分片或者scalability的意见，因为在现在，任何nosql能做的分片/scalebility，sql都能做Every time send/receive a message need to read/write a row from the SQL database@Richard 那 写的效率呢 ？？为什么要从db读？只是写而已。msg要放在append only的nosql db里，user，auth这些必然sql。我一般都用一个例子，就是dynamoDB用的是mySQL作为单点storage node是， uber内部的也是 MySQL 改的，存实时地理数据Normalization不过这样的话，那区别是什么呢 ？user-group table needs secondary key?
From ray to Everyone: (8:40 PM)
dynamodb use the mysql database engine
From Enze to Everyone: (8:41 PM)
数据量大时怎么join？
From A to Everyone: (8:41 PM)
data normalization 是有好处的
From Jackson to Everyone: (8:41 PM)
firebase的firestore，太少人用了，说实话firebase虽然是NoSQL，但是他能做到sql的对应关系。用firebase做例子不好
From bernini to Everyone: (8:41 PM)
会溢出？
From YL to Everyone: (8:41 PM)
那这样讲的话所有的都可以用sql+good partition吗
From Yue Liang to Everyone: (8:42 PM)
^我也这么理解。Tao 后台也是用mysql
From Richard Tu to Everyone: (8:43 PM)
对，虽然现在engine改成myrocket了
From ningdi to Everyone: (8:43 PM)
也就是说 如果没有partition limit是10k
From A to Everyone: (8:43 PM)
什么是topic partition？
From ningdi to Everyone: (8:43 PM)
那么最多也就10k个topic
From Lucas Li to Everyone: (8:43 PM)
分片联合查询？
From Lucas Li to Everyone: (8:44 PM)
In the first test, we set up a Kafka cluster with 5 brokers on different racks. In that cluster, we created 25,000 topics, each with a single partition and 2 replicas, for a total of 50,000 partitions. So, each broker has 10,000 partitions. We then measured the time to do a controlled shutdown of a broker. The results are shown in the table below.

https://blogs.apache.org/kafka/entry/apache-kafka-supports-more-partitions
From Lao luo to Everyone: (8:44 PM)
Kafka topic太多性能就下来了
From ningdi to Everyone: (8:44 PM)
3秒的delay也太大了
From Lao luo to Everyone: (8:44 PM)
和partion有关
From bernini to Everyone: (8:46 PM)
我们都拿firebase当cache用

## Ticket master
From Jun to Everyone: (7:17 PM)
这个画图是什么网站？
From Laoluo to Everyone: (7:17 PM)
肯定是秒杀相关但是是不是象12306一样有不同的站
From Ken to Everyone: (7:17 PM)
We have notes for previous meeting.  Please scan the QR code on top of the notes doc in this doc: https://docs.google.com/document/d/11hsGVxwAzfBPR6coFB-RiXmokUgqKbnFQ1R7urE6m_s/edit# to join our WeChat group
From Laoluo to Everyone: (7:17 PM)
有不同的站会复杂不少
From Skit to Everyone: (7:19 PM)
什么是p0, p3
From Qiang Lu to Everyone: (7:20 PM)
is it priority?
From Christie to Everyone: (7:20 PM)
Priority 0?
From 非洲黑猴子 to Everyone: (7:20 PM)
需求是不是最急需的
From Eric Che to Everyone: (7:20 PM)
这题的考点应该是类秒杀设计，怎么保证在大并发下能保证票不会超卖，并且能handle大并发量。
From 非洲黑猴子 to Everyone: (7:20 PM)
P0基本就是MVP了
From Skit to Everyone: (7:23 PM)
callback function?
From Tekken to Everyone: (7:23 PM)
直接开始接口设计了吗
From Alan to Everyone: (7:23 PM)
can customer send out and order and payment info, or customer can make an order first and then pay within an hourand payment info, assume payment system
From Laoluo to Everyone: (7:25 PM)
比较好奇最近几次好象都是男的出题，女的做题
From Erwin to Everyone: (7:25 PM)
is non functional requirements skipped for this one?
From Sun Anna to Everyone: (7:25 PM)
+1 where is the non functional requires?
From Jackie G to Everyone: (7:27 PM)
Sorry, What is “item”?
From Skit to Everyone: (7:28 PM)
i think she meant ticket "name" or description
From YL to Everyone: (7:28 PM)
每一次order都要更新所有的ticket吗？
From Yanbin Li to Everyone: (7:28 PM)
请问这个ticket系统是卖什么票的，这个聊了吗
From Richard Tu to Everyone: (7:29 PM)
同问，这个不用考虑什么座位号之类的吗
From bill.wang to Everyone: (7:29 PM)
TicketMaster--mostly they are concert tickets
From YL to Everyone: (7:29 PM)
所有ticket都是一样的
From Erwin to Everyone: (7:29 PM)
seat num should be needed in tickermasterso that user could select the seat they want
From Richard Tu to Everyone: (7:30 PM)
从我角度，应该有个event之类的event table mapping multiple tickets
From anna to Everyone: (7:30 PM)
求问个题外话，这是什么画图软件？ 感觉好好用，拖拉拽超级方便
From YL to Everyone: (7:30 PM)
感觉可以考虑成音乐节的门票之类的，都站票
From Tekken to Everyone: (7:30 PM)
需求分析后 直接跳到接口设计 这时候面试官是不是控场一下更好 资源预估 Data Flow, Service讨论 系统设计图 这些都是要先于接口设计做吧
From Richard Tu to Everyone: (7:31 PM)
可以是可以，但是requirement有说过吗，是我miss了什么需求吗针对 > 感觉可以考虑成音乐节的门票之类的，都站票
From YL to Everyone: (7:32 PM)
所有ticket没有分级，都是一样的
From iPad to Everyone: (7:32 PM)
Requirement说all tickets are the same，是不是就是说的是没有座位的分别啊
From Erwin to Everyone: (7:32 PM)
I bought tickets from Ticketmaster and there are different types of tickets for one event
From Yanbin Li to Everyone: (7:32 PM)
需求分析后做API design没啥问题吧，我理解首先通过API design明确你的service提供什么服务，后面才好设计为了提供这些服务怎么设计数据模型和系统架构
From Jilong Chen to Everyone: (7:32 PM)
User table should include credit card or other payment methods
From Christie to Everyone: (7:32 PM)
Ticket service, order service 分别在哪呀
From Yufei Qian to Everyone: (7:33 PM)
QPS, TPS分析了吗
From 非洲黑猴子 to Everyone: (7:33 PM)
应该是后面的数据库表不一样，一个是ticket另一个是order
From Christie to Everyone: (7:34 PM)
謝謝！
From Yufei Qian to Everyone: (7:34 PM)
这么多User直接hit relational会击穿吧
From YL to Everyone: (7:34 PM)
ACID优先
From Erwin to Everyone: (7:35 PM)
where do we store payment related info?
From 非洲黑猴子 to Everyone: (7:35 PM)
看怎么设计了，设计好了不会击穿，多拿流量都能给你扛下来
From Yufei Qian to Everyone: (7:35 PM)
目前的设计没有cache
From renyuming to Everyone: (7:37 PM)
cache一个需要考虑的是时效性
From YL to Everyone: (7:38 PM)
这里加cache的话应该存啥， available ticket？
From ds awsome to Everyone: (7:38 PM)
10m user ～ 100 qps，这样一台server就够了吧
From Richard Tu to Everyone: (7:38 PM)
所以这个设计，是更偏向秒杀？10m qps?
From ds awsome to Everyone: (7:38 PM)
是不是根本不用分布式系统啊
From 非洲黑猴子 to Everyone: (7:38 PM)
先看面试官咋说
From Shihao Zhong to Everyone: (7:38 PM)
10m user应该是都在一个时间左右买票，所以可能qps要10m 吧
From H.B. to Everyone: (7:38 PM)
一定要保证用户看到的ticket 数量是最新的？
From Erwin to Everyone: (7:38 PM)
have we discussed peak qps before?
From Shihao Zhong to Everyone: (7:38 PM)
10m 不是平均的啊，
From 非洲黑猴子 to Everyone: (7:39 PM)
用户一定看到最新的，但是下单的时候一定是检查最新的
From leo zhang to Everyone: (7:39 PM)
10m user如何推出 100 qps的? 这是秒杀10m 不是dau
From 非洲黑猴子 to Everyone: (7:39 PM)
用户不一定看到最新的，但是下单的时候一定是检查最新的
From Laoluo to Everyone: (7:40 PM)
SQL应该可以的，Cache或读写分离
From Erwin to Everyone: (7:40 PM)
also each ticket should have a uuid? so that we could refer to the payment/user related info?
From v to Everyone: (7:40 PM)
就算用了redis 还是有100k的qps？
From 非洲黑猴子 to Everyone: (7:40 PM)
Reds支持秒级10万并发
From v to Everyone: (7:41 PM)
就算用了redis 还是会有100k的qps到mysql因为有100k的票
From ds awsome to Everyone: (7:41 PM)
要是10m秒杀，那至少要10个 redis？
From Shihao Zhong to Everyone: (7:41 PM)
你可以一次发n张票到redis啊 redis不是有原子操作么
From iPad to Everyone: (7:41 PM)
一开始都没想到这是个flash sale的题目
From renyuming to Everyone: (7:41 PM)
redis有cluster也是可以scale up的到mysql的100K需要shard ticket了
From 非洲黑猴子 to Everyone: (7:42 PM)
看了不一定买，写请求怎么处理且看面试者怎么设计
From renyuming to Everyone: (7:42 PM)
每一张ticket都应该是一个record
From ds awsome to Everyone: (7:42 PM)
那这个scale mysql撑不住吧
From iPad to Everyone: (7:42 PM)
现在的设计怎么防止超卖呢？
From renyuming to Everyone: (7:42 PM)
redis在前面挡着每张ticket也会有write lock吧
From 非洲黑猴子 to Everyone: (7:42 PM)
锁定库存
From Richard Tu to Everyone: (7:43 PM)
至少得加个mq，异步吧
From v to Everyone: (7:43 PM)
Total quantity可以分开维护么。。这样就不用每次query 数据库来算count了？
From Richard Tu to Everyone: (7:43 PM)
削峰限流
From Jerry to Everyone: (7:43 PM)
要到最后成功支付成功或者取消才算确认吧
From Alan to Everyone: (7:43 PM)
双十一抢购
From renyuming to Everyone: (7:44 PM)
支付和book感觉可以分成两部分，book之后有一定时间去pay
From Christie to Everyone: (7:44 PM)
是不是可以先放 MQ ，payment 成功才更新 available_quantity?
From renyuming to Everyone: (7:44 PM)
因为pay一般做不到ms级
From ds awsome to Everyone: (7:44 PM)
Redis的性能是每秒10万 还是1M啊？
From 非洲黑猴子 to Everyone: (7:44 PM)
阿里自己二开了MySQL，增加了写请求排队功能
From renyuming to Everyone: (7:44 PM)
available_quantity 我感觉可以直接用redis + lua，只跟book相关，不跟pay相关
From v to Everyone: (7:45 PM)
这个设计会有thundering herd吧
From renyuming to Everyone: (7:45 PM)
也只保存quantity不是加了mq？
From Jerry to Everyone: (7:46 PM)
pay的过程中要锁定这部分库存吧但是要加个time out
From Yufei Qian to Everyone: (7:46 PM)
thundering herd无法避免，traffic pattern就是这样，需要设计去handle
From Shihao Zhong to Everyone: (7:47 PM)
为什么会有thundering herd，没有理解
From Laoluo to Everyone: (7:47 PM)
掉坑里了
From YL to Everyone: (7:47 PM)
1s就抢没了
From Christie to Everyone: (7:47 PM)
10 sec 東西賣光了
From leo zhang to Everyone: (7:47 PM)
抢票不刷新就没了啊
From Alan to Everyone: (7:47 PM)
request进来是不是要先做order request然后排队
From Alan to Everyone: (7:48 PM)
如果拿到ticket，那就create order
From 非洲黑猴子 to Everyone: (7:48 PM)
读的时候读不到最新的没关系，pay的时候不出错就好
From Yufei Qian to Everyone: (7:48 PM)
那样用户体验会比较差
From Jerry to Everyone: (7:49 PM)
限购的需求是不是还没加
From Yufei Qian to Everyone: (7:49 PM)
是的，限购需求没有讨论
From Alan to Everyone: (7:49 PM)
每个ticket system 有个cache， ticket先产生，买个ticket 系统分配一定数量ticket在 cache从cache拿ticket需要synchronous如果需求不多，应该很快，需求多就要排队，因为synchrounous
From tomdi to Everyone: (7:51 PM)
payment 和 ticket count update 做一个 transaction 事务， cache write upate只在每个transcation commit之后
From Laoluo to Everyone: (7:51 PM)
cache里保证所有的ticket不断地减少，不能不变
From Alan to Everyone: (7:51 PM)
ticket是事先产生的啊肯定不能做ticket count啊，笑死人
From Erwin to Everyone: (7:53 PM)
如果这里ticket service的一个server挂了，有没有什么办法保证这个service对应的tickets可以被其他server利用？
From Alan to Everyone: (7:53 PM)
data race是难点
From Skit to Everyone: (7:53 PM)
ticket先create row,然后book把
From tomdi to Everyone: (7:53 PM)
cache只读，db transaction update后再 update cache, cache读data可以有滞后
From Skit to Everyone: (7:54 PM)
这样不需要count
From v to Everyone: (7:54 PM)
用redis的话 如果不写到disk 会有data loss的风险。。。如果写到disk的话 write throughput会很不好吧？
From Ken to Everyone: (7:54 PM)
5 more minutes
From H.B. to Everyone: (7:54 PM)
他好像没说啥时候create session?
From Alan to Everyone: (7:54 PM)
有，每个ticket，保存分配到哪个server信息
From H.B. to Everyone: (7:54 PM)
session 不是一个小时吗
From Tekken to Everyone: (7:54 PM)
这是道老题目了 如果事前稍微准备下 油管上能找到很多很成熟的设计方案
From Alan to Everyone: (7:54 PM)
如果那个server crash， 他的那些没被order的ticket从新被放回去
From leo zhang to Everyone: (7:55 PM)
可能事先不知道题目
From YL to Everyone: (7:55 PM)
知道吧
From Laoluo to Everyone: (7:56 PM)
cache自行减少，不用更新数据库，蛤payment的service来更新ticket 的数量就行了payment量少很多，cache来读数据做同步
From H.B. to Everyone: (7:56 PM)
为了读的块
From Kasey to Everyone: (7:57 PM)
为啥不能直接用redis？
From 非洲黑猴子 to Everyone: (7:57 PM)
好几张表要join
From H.B. to Everyone: (7:57 PM)
卡住了
From Christie to Everyone: (7:57 PM)
所以不用 ticket mysql 的表了？
From Kasey to Everyone: (7:57 PM)
就把所有ticket 存redis里面不行么
From Laoluo to Everyone: (7:58 PM)
关键点没有讨论，特别是怎样削峰
From Hao to Everyone: (7:58 PM)
问个问题，应该什么时候减库存呢？是pay成功才减库存？
From Kasey to Everyone: (7:58 PM)
肯定吧
From Laoluo to Everyone: (7:59 PM)
+1
From H.B. to Everyone: (7:59 PM)
肯定pay 成功后
From Ender Li to Everyone: (7:59 PM)
Pay成功才减库存不会超卖吗
From wantong jiang to Everyone: (7:59 PM)
这怎么避免超卖呢？
From leo zhang to Everyone: (7:59 PM)
不会啊
From tomdi to Everyone: (7:59 PM)
pay成功和减库存是一个  transaction
From Kasey to Everyone: (7:59 PM)
pay成功和失败是两个情况
From H.B. to Everyone: (7:59 PM)
你说的库存是mysql里的？
From Zhengguan Li to Everyone: (7:59 PM)
pay成功前也可以呃减啊 事先锁定嘛
From H.B. to Everyone: (7:59 PM)
还是他说redis里的
From leo zhang to Everyone: (7:59 PM)
pay的时候检查库存, 放一个tracnsaction
From renyuming to Everyone: (8:00 PM)
应该是book就lock库存，之后pay失败了就恢复，pay成功了就减掉了
From leo zhang to Everyone: (8:00 PM)
order成功没付款还有有风险不能proceeed
From Kasey to Everyone: (8:00 PM)
嗯
From renyuming to Everyone: (8:00 PM)
transaction感觉很慢？尤其带3 party的api的？
From lily liu to Everyone: (8:01 PM)
order service的时候要减db库存并更新cache了吧，然后payment 成功的时候再调整一次
From H.B. to Everyone: (8:01 PM)
312
From Zhengguan Li to Everyone: (8:01 PM)
321
From Zidong to Everyone: (8:01 PM)
321
From 非洲黑猴子 to Everyone: (8:01 PM)
跟钱相关的不得不transaction
From Spin to Everyone: (8:01 PM)
132
From leo zhang to Everyone: (8:01 PM)
付款 lantency不是最紧急的需求吧?
From Yufei Qian to Everyone: (8:01 PM)
132
From Simon Z to Everyone: (8:01 PM)
321
From david to Everyone: (8:01 PM)
132
From Christie to Everyone: (8:01 PM)
312
From Jackie G to Everyone: (8:01 PM)
132
From leo zhang to Everyone: (8:01 PM)
accuracy更重要
From Kj to Everyone: (8:01 PM)
312
From x to Everyone: (8:01 PM)
312
From xinz to Everyone: (8:02 PM)
132
From 非洲黑猴子 to Everyone: (8:02 PM)
312
From johnc to Everyone: (8:02 PM)
312
From Xiaoqin Fu to Everyone: (8:02 PM)
312
From Julie Long to Everyone: (8:02 PM)
312
From anna to Everyone: (8:02 PM)
132
From YL to Everyone: (8:02 PM)
312
From Qiang Lu to Everyone: (8:02 PM)
312
From HW to Everyone: (8:02 PM)
321
From Peiwen Tian to Everyone: (8:02 PM)
312
From Xuexin Chen to Everyone: (8:02 PM)
312
From Richard Cao to Everyone: (8:02 PM)
312
From TBL to Everyone: (8:02 PM)
132
From christie Yu to Everyone: (8:02 PM)
312
From lily liu to Everyone: (8:02 PM)
132
From Ken to Everyone: (8:03 PM)
Hard skill
From Zhengguan Li to Everyone: (8:03 PM)
312
From YL to Everyone: (8:03 PM)
21
From christie Yu to Everyone: (8:03 PM)
213
From H.B. to Everyone: (8:03 PM)
21
From Jackie G to Everyone: (8:03 PM)
213
From lining to Everyone: (8:03 PM)
21
From Spin to Everyone: (8:03 PM)
21
From Mark Liu to Everyone: (8:03 PM)
213
From johnc to Everyone: (8:03 PM)
21
From Christie to Everyone: (8:03 PM)
21
From xinz to Everyone: (8:03 PM)
21
From Xiaoqin Fu to Everyone: (8:03 PM)
21
From david to Everyone: (8:03 PM)
21
From HW to Everyone: (8:03 PM)
21
From Shihao Zhong to Everyone: (8:04 PM)
这个3还是没有了解是什么
From TBL to Everyone: (8:04 PM)
21
From Charlie to Everyone: (8:04 PM)
只发数字顺序看不出那个好坏程度，还是每个指标打分更合适
From H.B. to Everyone: (8:05 PM)
嗯嗯我也觉得 每个都给1-5打分
From leo zhang to Everyone: (8:07 PM)
+1.
From H.B. to Everyone: (8:08 PM)
考官？
From Kasey to Everyone: (8:08 PM)
考官哈哈哈
From lining to Everyone: (8:08 PM)
😀
From Zhengguan Li to Everyone: (8:09 PM)
面试者: 考官竟是我自己..哈哈
From Jackie G to Everyone: (8:14 PM)
弱问一下： 如果所有票都一样，为什么还要一张ticket一行呢？直接一行ticket和count不行吗？
From x to Everyone: (8:14 PM)
对，其实面试者说的意思就是只需要一行
From Peijin Sun to Everyone: (8:15 PM)
Ticket 是不是其实是event
From Richard Tu to Everyone: (8:15 PM)
他这个表就是event稍微有点儿confusing
From lining to Everyone: (8:17 PM)
对
From Zidong to Everyone: (8:18 PM)
5000个是可以refilll吗
From leo zhang to Everyone: (8:19 PM)
限流没问题啊
From Zidong to Everyone: (8:19 PM)
like 一个buket？
From iPad to Everyone: (8:19 PM)
P0: buy ticket, cap = 2 tickets / user, all tickets the same
From leo zhang to Everyone: (8:19 PM)
多放点到后面就是但是限流可能是需要的，因为商品只有这么多
From Lu to Everyone: (8:20 PM)
想问下大家知道newSQL吗 听说了这个concept，好像又可以ACID，又可以horizontally scale
From leo zhang to Everyone: (8:20 PM)
放10M的流量到后面去没有意义
From Shihao Zhong to Everyone: (8:20 PM)
面试用newsql好么。
From Lu to Everyone: (8:21 PM)
没试过 😄
From Jerry to Everyone: (8:21 PM)
限流的话就是假设每个request只能买一张票
From Brave to Everyone: (8:21 PM)
搞个message queue存requests，然后异步处理，异步处理可以merge requests（比如同一类的票可以合并）避免访问太多次数据库
From Jerry to Everyone: (8:21 PM)
多张票要多个requests吗
From Richard Tu to Everyone: (8:21 PM)
用呗，别说什么newSQL，提具体的db名字
From Lu to Everyone: (8:21 PM)
Google Spanner？
From leo zhang to Everyone: (8:22 PM)
10 M限制成 120k的流量, 这就是huge win
From Jerry to Everyone: (8:22 PM)
那面试者的POST url就不能有number_items了吧
From Shihao Zhong to Everyone: (8:22 PM)
贵啊
From Jackie G to Everyone: (8:23 PM)
Redis 的replication支持strong consistency吗？还是eventual consistency？
From Jerry to Everyone: (8:23 PM)
Redis 有AOF log恢复不过这题redis好像不需要恢复数据库重读更新就可以吧
From Lu to Everyone: (8:25 PM)
同意 直接cache在API GATEWAY…
From leo zhang to Everyone: (8:27 PM)
写120K /hour只要前面filter了, 后面写不是问题
From Zhengguan Li to Everyone: (8:30 PM)
redis可以设置一个timeout?
From leo zhang to Everyone: (8:30 PM)
MQ有 exact-once sementic跟consumer配合
From Ender Li to Everyone: (8:32 PM)
那对redis的-1操作必须要锁redis是吗？那每次只有一个人可以update redis，这个不会成为性能瓶颈吗
From Shihao Zhong to Everyone: (8:32 PM)
redis有cas 原子操作不会成为瓶颈这里
From Sean Gao to Everyone: (8:33 PM)
CAS 能包括 send to Kafka 么 ？
From Sean Gao to Everyone: (8:33 PM)
或者 CAS 包括 persistence 步骤 ？
From Brave to Everyone: (8:35 PM)
春节买火车票就是在那排队
From Ender Li to Everyone: (8:36 PM)
CAS不就是说大量并发去更新redis只有一个会成功，其他都需要重试吗？因为别的old value都不对，更新会失败，是吗？
From Sean Gao to Everyone: (8:36 PM)
@ender  redis全内存操作， 性能损失不大 。 也不是完全lock，是 CAS。
From hobite to Everyone: (8:36 PM)
输入完信用卡信息，商家完成与银行间的认证，用户点submit的瞬间，update
From Mark Liu to Everyone: (8:37 PM)
不对吧，Payment按照12306会给你20分钟左右的操作Payment在20分钟内不成功，才会失败吧
From hobite to Everyone: (8:38 PM)
comit redis， 同时send payment and clean up task to queue. 这也涉及到如果放到queue里的期望，就是我们expect 送到queue里的task会99.9%的可能性成功，除非系统崩溃。
From Sean Gao to Everyone: (8:39 PM)
“comit redis， 同时send payment and clean up task to queue.” -- 这个是能全部放入 transacation么 ？
From hobite to Everyone: (8:40 PM)
另外一个solution, 是用户在query的时候锁住一个seat，submit的时候update或者release lock另外一个用户在lock的期间默认这个seat不available
From Jerry to Everyone: (8:43 PM)
redis的数据定期去db里同步更新可以吗

## Calendar
https://docs.google.com/document/d/1Zod6Cz0-KGJ-BDeZJoqEFrMQ4sdsSiJ-qT8Jkegwd-I/edit#Meeting notes: https://docs.google.com/document/d/1Zod6Cz0-KGJ-BDeZJoqEFrMQ4sdsSiJ-qT8Jkegwd-I/edit#For friends who just joined zoom: Meeting notes: https://docs.google.com/document/d/1Zod6Cz0-KGJ-BDeZJoqEFrMQ4sdsSiJ-qT8Jkegwd-I/edit#For friends who just joined zoom: Meeting notes: https://docs.google.com/document/d/1Zod6Cz0-KGJ-BDeZJoqEFrMQ4sdsSiJ-qT8Jkegwd-I/edit#For friends who just joined zoom: Interview will take 45 minutes: 6:15-7pm PSTMeeting notes: https://docs.google.com/document/d/1Zod6Cz0-KGJ-BDeZJoqEFrMQ4sdsSiJ-qT8Jkegwd-I/edit#已经30min了For friends who just joined zoom: Interview will take 45 minutes: 6:15-7pm PSTMeeting notes: https://docs.google.com/document/d/1Zod6Cz0-KGJ-BDeZJoqEFrMQ4sdsSiJ-qT8Jkegwd-I/edit#Are User and Calendar in one-to-one relationship?based on the requirement, no ^ThanksDoes the event attendance status need to be consistent? Eg user A updates yes, is it OK to sometimes get the incorrect status?eventual consistency should be okStupid idea, please critique:When the read requests exceeds the number that a SQL server can handle. Can we split out read request to read -pnly replica instead of using a cache?Read-only*of course yes这是mvcc的问题，用sql的话是可以保证strongconsistency, read要么读到 A update之前的status，要么读到 A update之后的status, 取决于read的time stamp是在write commit之前还是write commit之后客户端到现在还闲着呢，最近的事件可以缓存在客户端，可以有效减小服务端的读取压力这里是不是会涉及 跨shard join 的问题？ 有没有什么指导原则 ？我是觉得这个东西对一致性要求没那么高，应该问题不大话说是不是到时间了，今天是45分钟来着？有道理Yes. Time is up now.Consistency guarantees depend on which part of calendar - updating attendance might be OK with eventual but event privacy (public vs private) likely needs strong consistency.Score比之前只打相对分的靠谱多了稍等，投票再哪？Where is the poll?弹出来的没看到where is the poll?没有有看到pop upNo pop-up for meCould you send one more time?zoom这里是不是用的 eventual consistency.......strong consistency是sql免费送的，NoSQL要考虑consistency的问题用量的大的时候 这个就得考虑k/v storage，像 aggregation event的 join costing 就大太多了，所以SQL最好就别用了，Shard的时候我觉得Shard key 也是应该根据 event time 来做indexing👍1. Requirement gathering - meets1 requirement gathering: exceed, meet, needs1m1 meet2.needs2nn2 n2n看见poll应该要升级zoom 客户端3m3m3n3m3m3 n3n看不到，被盖住了看不到看不到ok可以ok要不用一個poll master 做？https://doodle.com/poll-maker好像hard skill target L5不一定吧热数据相当于cache？冷库不是还要做shardinginterview summary google doc的链接能再分享一下嘛应该还是要做sharding冷库也可以不走cacheMeeting notes: https://docs.google.com/document/d/1Zod6Cz0-KGJ-BDeZJoqEFrMQ4sdsSiJ-qT8Jkegwd-I/edit#
From Eric Che to Everyone: (7:25 PM)
引用kafka？
From Xinyu Zhang to Everyone: (7:26 PM)
shedule紧急meeting前几秒钟 延迟几秒 嗯。。
From Sean Gao to Everyone: (7:26 PM)
小概率
From Richard Tu to Everyone: (7:27 PM)
^sheculde meeting 几秒，这概率也太小了。那就完全可以用另一套workflow了
From Xinyu Zhang to Everyone: (7:28 PM)
假如发一个invent, 邀请了全公司所有人，每个人点accept或者propose to new time都算修改么？sorry, invite*
From Cheng Jing to Everyone: (7:29 PM)
算修改吧
From Xinyu Zhang to Everyone: (7:29 PM)
那CEO给全公司发一个，那修改量不小啊
From Sean Gao to Everyone: (7:29 PM)
batch 处理 write request，应该还好吧
From Xinyu Zhang to Everyone: (7:30 PM)
恩恩
From Quan to Everyone: (7:30 PM)
为什么1million的用户，有10 million的calendar
From Cheng Jing to Everyone: (7:30 PM)
倒是不用都在同一个时间发invite，我觉得
From Sean Gao to Everyone: (7:30 PM)
不过我也不懂， 1个 sql 改1000行， 和 1000个 sql 每个改一行，性能差别多大？
From Richard Tu to Everyone: (7:31 PM)
accept我觉得不能算update吧？至少不会update event本身
From Xinyu Zhang to Everyone: (7:31 PM)
对了 好奇 有必要设计calendar table么？ 直接用event table可以么？
From Sean Gao to Everyone: (7:31 PM)
accept算吧，因为你以后还能读出来。  应该是 update 的 是  user 和 event 的 relation。
From Richard Tu to Everyone: (7:32 PM)
那relation的表，key肯定不一样，所以应该不会造成hot key
From Sean Gao to Everyone: (7:33 PM)
对的，不是只对着一个 event写。如果 NoSQL 可能就不一样了。  也许用 redis ？
From Zoom user to Everyone: (7:34 PM)
1000 sql written in 1 batch vs 1000 sql transactions are the same from consistency guarantee perspective. But I wonder if there's perf overhead due to locks
From Xinyu Zhang to Everyone: (7:36 PM)
感觉sql这里有很多优点， 但这个 data structure 是一定是定死的么
From Eric Che to Everyone: (7:42 PM)
不能当做是一个event来看待吗？

## TopK
https://docs.google.com/document/d/1YYrcTZ5Spbz2gauu-U8PgZLv7bsQuH69kml8cY3hO38/edithigh availability是指时间上的概念（i.e. 24/7 available）还是multiplatform？24/7high availability是service  availability吧？500ms 不算low latency了吧？这里的low latency是啥意思。。。 一个trending service关心的不都是real time的trending吗。。 不懂这个low latency在这干嘛的。。。 模版吗。。好像所有的面试回答第一条都是high availability,然后就没有下文了，就是保证server 24/7运行 + backup server in case primary server failure?感觉从头到尾都是模板这几个term好像都是模板感觉design还没开始打开YouTube之后需要加载五秒钟还是挺影响体验的是0.5s吧？开始了喊我0.5s我的意思是这道题low latency还是很有必要的Target是l4 大牛们稍安勿躁请教一下： high available和fault-tolerent是不是重复的概念？estimation这里是不是时间花太多了我觉得是保证server不断电low latency是 get topk的时候 快速返回 还是说 我的last 24 hrs trending是 real time的 还是 有1-2 小时延迟。。后者是consistency吧这种情况design一个function in existing system，我们是不是需要先问问什么已经有了low latency是 get top的时候 快速返回fault tolerance  = high availability + proper failure handling谢谢同意 我觉得是不是能assume已经有了一个counting system感觉模板不好用了 连观众都不买帐了我感觉，last 24hrs trending，经常可以延迟产生的
我觉得没人会关心这个trending是不是实时刷新500ms不是low latency了吧500ms也能算low吧。500ms对这个top k应该够了，个人意见吧I see感觉500ms有点卡对于YouTube是有点卡，哈哈100m的 dau 然后每个人的点击都会影响trending。。 如何收集 咋收集。。都是个问题 😂无所谓了，你说500ms，100ms最后design出来不都是一套系统。。24hours trending看你用batch还是streaming processing, 一般5到10分钟的delay是可以实现的其实没有必要在意这些细节，毕竟这只是设计，不是实现trending不用实时更改，观众不会那么关心试试更改Trending其实我一直没搞懂traffic estimate的意义在哪里这个时间安排45分钟不够了吧traffic estimation是grokking模版里，我个人认为性价比最低的part，聊胜于无没有意义，只要问一下每秒有多少个view就行了traffic estimation是grokking模版里，我个人认为性价比最低的part，聊胜于无

同意大家面试会跳过么？还行dau开始算，没必要traffic estimation是grokking模版里，我个人认为性价比最低的part，聊胜于无

同意‘所以面试的时候 如果考官没有提，是不是可以直接略过这个traffic estimation? 不会成为扣分点吧？有大神指导我说，traffix estimation的一个意义是：
选择哪种数据库，是选择sql或nosql我个人，一般traffic大概估算一下，还有storage那块，主要是数据库怎么设计.... 爲啥traffic 和 db選擇 有關係啊estimation 3-5分钟快速搞完？完全沒有關聯啊求细节，怎么个估算法？怎么选数据库？不会真的有面试官期望你设计一个not distributed system吧数据多就NoSQL？看 read 和 write的啊怎麽可能是看traffic视频基数到底多大 才是top k的基本问题吧。。 nlogk 你好歹要知道n是多少呀。。 难不成30亿的n吗。。事务多就SQL?对，是Read write怎么看read write选数据库？这里的count min是count min sketch吗簡直了嗯，事物也是誰亂講的哪种数据库不能read  write呢？重点是ratio去看ddiastreaming system 都有log的，如果从existing system开始讲， 可能会容易点lolddia万能啊面试时候我也这么说哈哈，我瞎讲，多半我就记得不对我感覺top video 可以用 url在 redis 存你想想disk IO，write heavy 做sql 刺激嗎？schema on write 的時候用sql 是什麽體驗payment service: ??所以如果用NoSQL会比较好嘛不用sql用啥如果能用的话选择NoSQL的原因是什么？如果做olap，用 NoSQL 的 join 是不是很刺激因为别人都说用nosql，所以大部分人选了nosql讲道理，你distributed sql的join和nosql join有什么区别。。Count-min 中间插个storage是干什么用的。data collection phase -》 data calculation phase -〉 result read phase。。 这道题是想考的到底是哪几个？NoSQL写的快，但是无法join，事务性也比较难

sql最容易，最好用，但不能handle那么多write“这里的count min是count min sketch吗” - count min sketch一般用来统计频率，unique topk一般用hyperloglog👍只要sql shard出现，就跟joint没啥关系了东欧大哥的topK用的count min sketch东欧大哥，哈哈哈东欧大哥是？LOLyoutuber东欧大哥是？请教Chanel名字谢谢Shouldn’t we have a data schema, then API?這個 fast processors Count-Min 我看不懂https://www.youtube.com/watch?v=kx-XDoPjoHw那是top K的频率 而不是view count,否则数学上推到不了谢谢楼上有点像伯恩 LOL我最近看了一篇google napa data warehousing的paper。主要就是根据不同时间片段来分级aggregate。感觉能用在topK的case。http://vldb.org/pvldb/vol14/p2986-sankaranarayanan.pdffast processors是什么，是个service吗？data warehouse，就不是实时了吧5 mins is a good choice我看里面ingestion是可以streaming进来的，应该可以保证个near-realtimeNapa supports database freshness of near-real time to a few hours小时级别是不是不大够用这5 mins的 设计不cover很多corner case吧Schema感觉又问题，应该存frequency吧trending这种那就1 min 更新一次？micro batch应该也行我感觉B站大概30s一次？real time那更好了这个distributed MQ是干啥的Update latest view奥，makes sense这个也需要设计吗？existing system不就有吗这在东欧大哥视频里是重点只有fast的吗？求个东欧大哥的link我去听听正确答案睡觉去了https://www.youtube.com/watch?v=kx-XDoPjoHw感谢女神我一直以为他是毛子帮我留个feedback，感谢。1. load estimate 时间太长了 后面也没用到 2.面试加油。這也是我的問題到底這是count 啥，爲啥是 minCount-min要花点时间解释的面试环境和自己想还是不一样的。林老师已经给了很多hint了，但面试的人太紧张就会get不到。也许换一个环境他就会说的很好，但面试的时候难说了。L4不用考system design，能做到这样，我觉得可以了只是谷歌不考而已只是google不考吧，别的公司还考呢时间真快，45min了amazon L4 SD不考Amzn L4 = Google L5Sry反了哥這個接近L5 水品了Amzn L4 = Google L3這已經很厲害了一般这个群备注的等级，都是按google来的我觉得这题应该有个MR的solution吧？？我面某家公司，就给的是spark solutionMR是啥map reduce地图-降低这题跟collect metric的区别在哪里Which one is for senior level, L4 or L5 ?https://docs.google.com/document/d/1YYrcTZ5Spbz2gauu-U8PgZLv7bsQuH69kml8cY3hO38/edit#如何体现project lifecycle awareness?  加monitoring?  metrics?怎么evaluate project lifecycle awareness?如何iterate project ?  系统不可能3个月就做完了是吧alarm metricsagree@Ken林老师待会儿可以讲讲project awareness 吗能给个例子么？怎么计算机器数量？怎么根据qps估算机器数量？有公式吗？dau --> qps --> 根据一台机器的qps处理能力，来估计需要几台机器1000 qps你就算一台 一般都没问题所以要背mongoDB, Cassandra的throughput？太卷了吧这个只是算的web server吧nosql 10k tps ， sql 1k tps ， memory 100k不需额外背吧1000 QPS is a lambda, 一台机大概50000‘1000 qps你就算一台’多大RAM?几个Core?Kafka, SQS?你真的要把时间放在 machine几个core上吗。。。我刚才没听见fast processor是run在什么东西上的直接说 10k qps 我给30台机器你觉得行吗；； 没人会纠结这个的。。 没人会卡你这个 给你ram 给你core然后让你算他的capability。。我没听明白他这个是怎么calculate top K viewed videos这个design真的是workable的吗。。cross team dependency 
handling unusal spike of traffic 
scale upread flow没有讲过这种情况下面试官会期望答案跟组里一致么能走一个case吗。。 就是他如何读topk的？ YouTube有30亿个video 假设1个亿的video 在过去5分钟被人view过。。 nlogk的n是1个亿啊。咋store 咋sort。
From Liang Tan to Everyone: (8:24 PM)
请教一下如果用了MR了, 这里的 MQ 还是需要的嘛， 是不是放在GFS上就行了。
From Zhao to Everyone: (8:28 PM)
Batch处理可以直接读log file，结果会比较准确。实时的request话不需要每一个都处理，可以做一下sampling，比如从1亿个request降到1M，然后接一个queue来处理
From Weizhe Liang to Everyone: (8:29 PM)
也有道理多一個queue 去reduce queue 的話會好做點
From bambloo to Everyone: (8:33 PM)
LRU
From Zhao to Everyone: (8:34 PM)
redes
From bambloo to Everyone: (8:34 PM)
可不可以用LFU？
From ningdi to Everyone: (8:36 PM)
终于有人问这个了。。
From Spin to Everyone: (8:36 PM)
这样没法防刷单吧？
From ningdi to Everyone: (8:36 PM)
Sampling 确实是个解决办法。。。能大量减少unique id 只有一个1 view
From james to Everyone: (8:41 PM)
Mongo DB seems a good choice
From Zhao to Everyone: (8:42 PM)
我觉得DB里可以分级存，比如daily数据可以留365的，一天，hourly的留24*30的，5min的留一周的，这样无论你要什么granularity 都能满足
From admin to Everyone: (8:42 PM)
离线+实时计算  hive+flink
From james to Everyone: (8:42 PM)
Each video has its own document
From jao to Everyone: (8:42 PM)
要求多长时间刷新排行榜？每五分钟吗
From Spin to Everyone: (8:43 PM)
怎样保证一个unique user的count只计算一回？
From ningdi to Everyone: (8:44 PM)
unique user的count 值计算一次 可以在client 端做去重比较简单会不准确 但是我觉得most case应该是work的
From admin to Everyone: (8:46 PM)
Click事件可以异步发送kafka 然后保存数仓里面
From Zhao to Everyone: (8:46 PM)
同意tomdi说的系统设计不是唯一解，没必要争论，我看scott shi的mock里面只要能讲的通好像就可以
From Lixuan Zhu to Everyone: (8:54 PM)
https://www.youtube.com/watch?v=kx-XDoPjoHw
From Ender to Everyone: (9:00 PM)
请教一下topK这个问题有什么点或者follow up是俄罗斯大哥的视频没cover到的吗？

## Youtube

From Ken to Everyone: (6:16 PM)
Starting around: 6:15Approximate completion time: 7:00Welcome to our event.  I am taking notes in https://docs.google.com/document/d/1XLpTHQxYaZBDJzRTyVgfHFfxBHD9G79hNFDvNlRwty4/edit#
From Ken to Everyone: (6:20 PM)
If you have not joined Ming Dao WeChat group, you can join using the QR code on the top of the documentApproximate Start time: 6:15. End time: 7:00
From Ken to Everyone: (6:24 PM)
Welcome to our event.  I am taking notes in https://docs.google.com/document/d/1XLpTHQxYaZBDJzRTyVgfHFfxBHD9G79hNFDvNlRwty4/edit#If you have not joined Ming Dao WeChat group, you can join using the QR code on the top of the documentApproximate Start time: 6:15. End time: 7:00
From Jackie G to Everyone: (6:27 PM)
Does “width” mean “throughput”?
From Bam to Everyone: (6:27 PM)
Bandwidth I guess
From Jackie G to Everyone: (6:27 PM)
thanks
From A to Everyone: (6:28 PM)
感谢楼主算算术，来晚了，设计还没开始
From ningdi to Everyone: (6:28 PM)
Hahah :)
From Yue's iPad to Everyone: (6:30 PM)
7G per second for video upload 是怎么来的?
From Ken to Everyone: (6:30 PM)
Welcome to our event.  I am taking notes in https://docs.google.com/document/d/1XLpTHQxYaZBDJzRTyVgfHFfxBHD9G79hNFDvNlRwty4/edit#If you have not joined Ming Dao WeChat group, you can join using the QR code on the top of the documentApproximate Start time: 6:15. End time: 7:00
From Bot to Everyone: (6:30 PM)
这estimate那个1:200让我想到了educative.io那个
From Jackie G to Everyone: (6:31 PM)
Why does uploadVideo accept a videoId? I thought video id is generated by the system when the video is uploaded. Does he mean “video title” instead? Thanks
From ningdi to Everyone: (6:31 PM)
压根没有downloading的 req 但是模版背多了直接就来了个ratio。。
From Charlie to Everyone: (6:31 PM)
storage 683T/day 是根据什么算的
From 非洲黑猴子 to Everyone: (6:32 PM)
传offset可能不太行。一旦传了offset给服务端，那不就意味着文件上传下载就需要经过服务端server？其网关或者LB的网卡可能成为瓶颈
From Charles  to Everyone: (6:33 PM)
Upload rate
From A to Everyone: (6:33 PM)
这是在背诵educative的solution吗
From Spin to Everyone: (6:33 PM)
这是指对client的，last viewed position?
From A to Everyone: (6:33 PM)
我去看看答案
From ningdi to Everyone: (6:33 PM)
一个3小时长的视频，后段可能是cut成很多小的chunks,然后offset可以快速定位到具体哪个chunk你要去load
From ningdi to Everyone: (6:33 PM)
我觉得是这样吧。。
From A to Everyone: (6:35 PM)
啥av
From Sean Gao to Everyone: (6:35 PM)
@猴子哥   offset 我感觉 GFS 类似系统可以提供吧？  或者 server 先把 offset 在 metadata 里面 解析成 GFS 的chunk 地址，再从 GFS 传。
From Bot to Everyone: (6:35 PM)
avi
From Sean Gao to Everyone: (6:36 PM)
这里 aws 提供 api 来fetch 一部分的 file： https://stackoverflow.com/questions/36436057/s3-how-to-do-a-partial-read-seek-without-downloading-the-complete-file
From Ken to Everyone: (6:36 PM)
Welcome to our event.  I am taking notes in https://docs.google.com/document/d/1XLpTHQxYaZBDJzRTyVgfHFfxBHD9G79hNFDvNlRwty4/edit#If you have not joined Ming Dao WeChat group, you can join using the QR code on the top of the documentApproximate Start time: 6:15. End time: 7:00
From Robin to Everyone: (6:37 PM)
是的，我觉得(videoID + offset) 对应到一小段视频，这个offset是某种预设的granularity，比如可能后端只支持按分钟分块
From Yi to Everyone: (6:37 PM)
不需要offset, client 直接chunk 成小块，upload这些data blob，服务器对每一个blob返回一个hash id，然后client把这些id 拼接起来commit到metadata service
From Erwin to Everyone: (6:38 PM)
client不是用presigned url upload到s3吗？
From ningdi to Everyone: (6:38 PM)
这里只是再说 play video需要offset
From Sean Gao to Everyone: (6:38 PM)
那你 getVideo 要从中间chunk读起来， server 怎么知道从哪个 blob 开始传给你 ？
From Yi to Everyone: (6:38 PM)
看错了，我以为是upload。。
From Sean Gao to Everyone: (6:38 PM)
ack
From ningdi to Everyone: (6:40 PM)
这个encode service在这里是干嘛的请问。。。 都用了s3了。。。 s3直接返回video地址了不是吗。。
From Robin to Everyone: (6:41 PM)
或许支持多种分辨率？
From Sean Gao to Everyone: (6:41 PM)
post processing
From Erwin to Everyone: (6:41 PM)
应该是encode到不同的resolution
From Shihao Zhong to Everyone: (6:41 PM)
应该是把视频转换成不同的格式或者分辨率以方便不同的设备吧
From ningdi to Everyone: (6:41 PM)
啊 那确实可能。。
From Zhengguan Li to Everyone: (6:41 PM)
各种不同的播放格式和分辨率吧 手机格式 电脑模式
From Robin to Everyone: (6:41 PM)
但是确实requirement里没提到多种分辨率这点
From 非洲黑猴子 to Everyone: (6:41 PM)
S3能把文件翻译成消息发到MQ？S3这么听话吗？
From Shihao Zhong to Everyone: (6:42 PM)
说到了各种不同的设备嘛
From ningdi to Everyone: (6:42 PM)
这是典型的 知道答案来考试。 然后题目跟答案有点不搭了 😂
From Shihao Zhong to Everyone: (6:42 PM)
刚才说的是双写，不是S3给发消息，而且MQ应该也可以用SQS来做，这样S3的消息也可以监控到
From 非洲黑猴子 to Everyone: (6:43 PM)
还不如直接用国内的七牛云，人家自带各种视频转码和图片缩放
From Shihao Zhong to Everyone: (6:43 PM)
S3的事件，比如get put
From Erwin to Everyone: (6:43 PM)
s3本身就可以generate event到sqs/sns
From Sean Gao to Everyone: (6:43 PM)
change capture
From Erwin to Everyone: (6:43 PM)
https://docs.aws.amazon.com/AmazonS3/latest/userguide/notification-how-to-event-types-and-destinations.html#supported-notification-destinations
From ningdi to Everyone: (6:43 PM)
系统设计面试中能用这么多aws全家桶吗？ 基本啥都傻瓜化了。。。 储存s3 通知sqs。。。 基本没啥需要设计的吧。。
From Shihao Zhong to Everyone: (6:44 PM)
不知道啊，我也很好奇这个问题。 但如果用别的组host的infra其实也差不多吧，无非就变成了hdfs和Kafka？
From Laoluo to Everyone: (6:44 PM)
不建议，除非你面的是AWS，然后迎合面试官。但你要说得出所以然来
From Kasey to Everyone: (6:44 PM)
一般是先设计完然后一些具体实现之后可以细致的说
From Richard Tu to Everyone: (6:44 PM)
当然可以用，面试官也会问你了不了解里面的细节
From tomdi to Everyone: (6:44 PM)
s3只是storage, mq应该是upload service trigger
From Kasey to Everyone: (6:44 PM)
不然不用上
From 非洲黑猴子 to Everyone: (6:44 PM)
哈哈，个人感觉国内的水平更高，毕竟并发啥的不是跟北美一个量级的，而且那边粥少僧多，竞争激烈
From ningdi to Everyone: (6:44 PM)
主要是我没用过aws的全家桶。。 在考虑要不要去补一套。。
From Sean Gao to Everyone: (6:44 PM)
这里 metadata svc 和 s3 的consistency 如何保证 ？ ？
From Richard Tu to Everyone: (6:44 PM)
不然，就知道个名词就不好了
From Ken to Everyone: (6:45 PM)
Welcome to our event.  I am taking notes in https://docs.google.com/document/d/1XLpTHQxYaZBDJzRTyVgfHFfxBHD9G79hNFDvNlRwty4/edit#If you have not joined Ming Dao WeChat group, you can join using the QR code on the top of the documentApproximate Start time: 6:15. End time: 7:00About 15 minutes to go.
From Sean Gao to Everyone: (6:45 PM)
这里 metadata svc 和 s3 的consistency 如何保证 ？ ？ 要重试么如果 s3 fail
From 蔡海林 to Everyone: (6:46 PM)
meta service 保存的是video的meta信息，和original s3之间的关系是通过返回给client段的upload url来联系在一起的
From ningdi to Everyone: (6:46 PM)
我唯一用过的就是s3  s3上传不成功会告诉你上传失败的。。
From Kasey to Everyone: (6:46 PM)
不用S3 用hdfs一样的吧
From 蔡海林 to Everyone: (6:46 PM)
upload部分一定有重试机制的
From Kasey to Everyone: (6:46 PM)
hdfs都是storage服务有什么不同么
From 蔡海林 to Everyone: (6:46 PM)
而且upload的时候基本要进行chunk话，否则很难在upload partial fail之后重新传
From Kasey to Everyone: (6:47 PM)
他这里很重要的CDN没说吧。。。
From 蔡海林 to Everyone: (6:47 PM)
还早呢lb也都没有
From Sean Gao to Everyone: (6:47 PM)
@蔡 谢谢。  meta db 里面应该也有 upload status， 然后如果完全失败了，提醒用户手动重试。
From 蔡海林 to Everyone: (6:48 PM)
嗯，
From Sean Gao to Everyone: (6:48 PM)
YouTube 网页关闭后，不然没法retry
From ningdi to Everyone: (6:49 PM)
请问 用了s3了 还需要cdn？
From Laoluo to Everyone: (6:49 PM)
要的
From Sean Gao to Everyone: (6:49 PM)
需要
From 蔡海林 to Everyone: (6:49 PM)
upload状态实际上可以考虑在本地localstorage保存一份，在上传完成之后通知服务端就好
From Kasey to Everyone: (6:49 PM)
嗯要的
From 蔡海林 to Everyone: (6:49 PM)
s3速度不行的前面一定要cdn
From Ryan to Everyone: (6:49 PM)
s3 bucket 有region
From Kasey to Everyone: (6:49 PM)
而且CDN可以用多级的
From Sean Gao to Everyone: (6:49 PM)
localstorage 的问题是，multi device 无法 sync
From ningdi to Everyone: (6:49 PM)
那么cdn在这里是 client端去做 还是 s3去做？
From Richard Tu to Everyone: (6:49 PM)
CloudFront + S3
From Kai Z. to Everyone: (6:50 PM)
storage需要节省吗
From Ryan to Everyone: (6:50 PM)
+1 cloudfront
From Yumin Gui to Everyone: (6:50 PM)
真的不考虑成本吗？你用aws s3，你怕不会一天花掉10亿美元。都有100M的active user了，这种情况下还不自建存储服务？
From Sean Gao to Everyone: (6:50 PM)
需要节省吧storage
From Yi to Everyone: (6:50 PM)
自建不一定比s3 便宜
From Ryan to Everyone: (6:51 PM)
s3 glacier
From ningdi to Everyone: (6:51 PM)
S3不是有一个什么叫 glacier吗
From Laoluo to Everyone: (6:51 PM)
glacier是archive
From Kasey to Everyone: (6:51 PM)
glacier是做archive的
From ningdi to Everyone: (6:51 PM)
长时间没有read的 只寸low resolution的version在s3 其他的放进glacier。。。 算是省钱的一种吧。。。
From Kasey to Everyone: (6:52 PM)
你可以设置life cycle的
From ningdi to Everyone: (6:52 PM)
还真就aws全家桶设计一切了。。。 😂
From Sean Gao to Everyone: (6:52 PM)
glacier 意思是 压缩存储么 ?
From Shihao Zhong to Everyone: (6:52 PM)
不至于吧 这个700T /天 纯粹S3的话现在0.02 gb/M 如果按里面存5年的data来算2500w/月左右
From ningdi to Everyone: (6:52 PM)
据说是 响应速度满。
From Richard Tu to Everyone: (6:52 PM)
glacier用的hdd
From ningdi to Everyone: (6:53 PM)
请问视频播放有cache吗？
From Sean Gao to Everyone: (6:53 PM)
cdn
From ningdi to Everyone: (6:53 PM)
这种file 文件的cache。。。
From Kasey to Everyone: (6:53 PM)
cloudfront不就是CDN
From Shihao Zhong to Everyone: (6:53 PM)
如果你80%放到archive的话大概600万/月 好像也没有特别高
From ningdi to Everyone: (6:54 PM)
看来我需要好好查查看cdn了。。
From A to Everyone: (6:54 PM)
s3 glacier是cold storage，存在锤子
From Zhengguan Li to Everyone: (6:54 PM)
600w不高嘛？
From Sean Gao to Everyone: (6:55 PM)
对 youtube不高吧
From Bam to Everyone: (6:55 PM)
居然只有五分钟了
From Ender Li to Everyone: (6:56 PM)
search是不是还没design呢
From Richard Tu to Everyone: (6:56 PM)
Glacier的get SLA是按小时来的。视频文件从里面取，估计用户都跑光了
From Ryan to Everyone: (6:56 PM)
tiktok 好像是自建的storage
From Kasey to Everyone: (6:57 PM)
non popular可以设置life cycle么
From Mossaka to Everyone: (6:57 PM)
CDN会自动提供DASH服务吗？
From Ender Li to Everyone: (6:57 PM)
请教下是所有视频都放CDN吗？还是只有hot/popular的放在cdn
From 蔡海林 to Everyone: (6:58 PM)
不可能所有放到cdn storage, cdn storage也是很贵的 :)
From Ken to Everyone: (6:58 PM)
2 minutes to goStart time: 6:15, end time ~7:00pm
From Ryan to Everyone: (6:58 PM)
为啥要cache comments...
From Kasey to Everyone: (6:58 PM)
看用户对延迟的要求
From Ryan to Everyone: (6:59 PM)
loading video 肯定latency 更高呀
From Kasey to Everyone: (6:59 PM)
youtube这种的话我觉得要放挺多在CDN的
From 蔡海林 to Everyone: (6:59 PM)
comments如果需要broadcast到所有看同样视频的用户，那个就是另外的设计了
From Shihao Zhong to Everyone: (6:59 PM)
一个很复杂的comment 用什么数据库存比较好呢，尤其是很多层的那种？
From Zhengguan Li to Everyone: (6:59 PM)
“Glacier的存档检索延迟（档案在3-5小时后可用）“意思是找一个东西要3-5小时
From Erwin to Everyone: (6:59 PM)
S3 Glacier Instant Retrieval 的get latency也是miliseconds级别的
From Zhengguan Li to Everyone: (6:59 PM)
？
From Zhao to Everyone: (6:59 PM)
如何决定什么视频存在哪个CDN?
From Ken to Everyone: (7:00 PM)
Time is up
From Sean Gao to Everyone: (7:00 PM)
youtube 的comment 应该不是 broadcast 的
From 蔡海林 to Everyone: (7:00 PM)
有几种方法，1）统计视频播放的热度；2）根据预先的估计，比如很热门的电视剧出了新的season，那么就一定要搞到cdn去了
From Sean Gao to Everyone: (7:01 PM)
reddit 的 comment好像直接存的 MySQL ？
From Ryan to Everyone: (7:01 PM)
broadcast 完全是另一个topic了吧
From 非洲黑猴子 to Everyone: (7:01 PM)
Redis有RDB和AOF可以落盘
From Shihao Zhong to Everyone: (7:01 PM)
那comment要是叠个很多层岂不是query mysql直接挂了还是他业务上就不允许很多层的comment
From 蔡海林 to Everyone: (7:02 PM)
3）还可以根据不同地域用户的观看习惯把video push到相应地域的cdn去
From 非洲黑猴子 to Everyone: (7:02 PM)
给面试官说，Redis可以做缓存数据库个MQ
From First Last to Everyone: (7:02 PM)
time is over.
From 非洲黑猴子 to Everyone: (7:03 PM)
主从复制
From Jerry to Everyone: (7:03 PM)
getVideo的服务是不是还没设计
From 蔡海林 to Everyone: (7:03 PM)
是啊
From Bam to Everyone: (7:03 PM)
设计了，CDN
From 蔡海林 to Everyone: (7:03 PM)
漏掉了不少东西
From Jerry to Everyone: (7:04 PM)
怎么记录播放进度的
From Yi to Everyone: (7:04 PM)
主要面试官也没有深入问那里
From Bam to Everyone: (7:04 PM)
这个没提
From Charlie to Everyone: (7:04 PM)
系统设计到什么程度算是过关呢？
From Sean Gao to Everyone: (7:04 PM)
感觉应该过关了吧  ？
From 蔡海林 to Everyone: (7:04 PM)
至少能够自圆其说no, 我觉得没过关
From kevin to Everyone: (7:05 PM)
这个最好能讨论一下bar
From 蔡海林 to Everyone: (7:05 PM)
毕竟是l5
From Sean Gao to Everyone: (7:05 PM)
哪里不够格？
From Kasey to Everyone: (7:05 PM)
L5的话有点困难
From kevin to Everyone: (7:05 PM)
我觉得很vague 这个bar
From Spin to Everyone: (7:05 PM)
差不多吧
From kevin to Everyone: (7:05 PM)
有没有资深的给个hint过没过bar
From Kasey to Everyone: (7:05 PM)
但首先YouTube就是hard system design question
From J to Everyone: (7:06 PM)
那哪些question是简单 哪些是hard 求问
From Shihao Zhong to Everyone: (7:06 PM)
可以说下easy system design和hard system design的例子么
From Bam to Everyone: (7:06 PM)
tinyurl
From Kasey to Everyone: (7:06 PM)
tinyurl
From Ping Lu to Everyone: (7:09 PM)
请问一下，这个画图软件是什么？
From Kai Z. to Everyone: (7:12 PM)
decision呢
From Shihao to Everyone: (7:13 PM)
这个选什么云的服务怎么回答啊 S3 啊 azure blob不都差不多
From J to Everyone: (7:14 PM)
L5 这题如果答得好应该是怎么用的样*
From Liang Tan to Everyone: (7:14 PM)
请问db design 应该放在是跟service上边画图边做，还是放到service前或者后比较好。是不是用一个upload service就好了猴子哥说的网卡是什么呢？实际的production上没有从orginal取的，全都是从cdn取。cdn费用肯定比 server便宜。如果是冷门视频呢 ？如果upload或者download的数据经过自己的service，则会打满后者的网卡的风险牛upload和download数据和信令都是分离的。冷门数据也要用cdnCDN上啥都有，那为啥还要S3呢Xing Wang 大佬🐂🍺s3存orignal👍牛面试官期待面试者自己deep dive topic吗，我觉得deep dive是需要面试官去问出来的吧Jane Liu 您的建议是先口述一个user journey，再问面试官面试关注的feature是吗streaming基本上都是从cdn从streaming的，建议看看dash和hls streaming arcS3 good for video streaming:  https://stackoverflow.com/questions/3505612/amazon-s3-hosting-streaming-videoYou can send 3,500 PUT/COPY/POST/DELETE or 5,500 GET/HEAD requests per second per prefix in an Amazon S3 bucket. There are no limits to the number of prefixes that you can have in your bucket.https://aws.amazon.com/premiumsupport/knowledge-center/s3-request-limit-avoid-throttling/你们都过于依赖aws了，事实上所有的video chunks都是依赖于cdn的。netflix的核心竞争力是他的自建cdn，而不是依赖在aws上。冷门视频如何处理呢 ？所有的streaming相关的公司，cdn都是他们成本考虑的最重要因素。CDN 挡了90%的流量aws只handle信令和meta data，video chunk从来都不经过micro seevicestaotao 说  “所有chunks”从ops角度讲，cdn得挡99%的流量
From Sean Gao to Everyone: (7:42 PM)
true
From Ning to Everyone: (7:42 PM)
记得看过Netflix 说是90%
From Joselyn phone to Everyone: (7:42 PM)
如果冷门的视频，是不是也是从cdn读比较好
From Sean Gao to Everyone: (7:43 PM)
冷门视频可能不在cdn里面
From Jia to Everyone: (7:43 PM)
有大神能recap下upload，download该如何scale吗？sorry刚没听清。download用自建的cdn cache或aws cloudfront类似的service，upload用queue？
From Joselyn phone to Everyone: (7:43 PM)
那冷门视频从哪里读，就是直接分布文件系统吗？
From Bam to Everyone: (7:44 PM)
肯定有视频不在CDN里，比如刚上传的视频，或者10年没人看的视频
From nz to Everyone: (7:44 PM)
you drive
From kabuka to Everyone: (7:45 PM)
我面过一个公司的SD 其中一个feedback就是面试者要proactively drive interview
From Taotao to Everyone: (7:48 PM)
netflix关于视频所有的存储都是自建的，
From Richard Tu to Everyone: (7:48 PM)
碰肯定能碰到概率问题
From nz to Everyone: (7:49 PM)
communication skill
From Sean Gao to Everyone: (7:49 PM)
👍
From Zhao to Everyone: (7:51 PM)
我觉得可以把design 面试看成你跟自己公司architect review design的过程。一般先说一下high level, 然后deep dive，中间经常问问feedback，Qs, etc. 然后说说 positive path，negative path，如何scale up
From nz to Everyone: (7:53 PM)
no right or wrong answer. you should present solution and alternative solutions. what are the tradeoff
From Sean Gao to Everyone: (7:53 PM)
解耦，异步，削峰，填谷
From Shihao Zhong to Everyone: (7:54 PM)
削峰填谷英文咋说
From Sean Gao to Everyone: (7:54 PM)
shift loading
From kabuka to Everyone: (7:54 PM)
上传视频怎么async? 例如上传一个1GB的视频。带宽是1MB/s 的话怎么样也要等1024秒吧
From 非洲黑猴子 to Everyone: (7:54 PM)
解耦异步、削峰填谷
From Shihao Zhong to Everyone: (7:54 PM)
可以的
From nz to Everyone: (7:54 PM)
buffer
From Ning to Everyone: (7:55 PM)
这个有专门的协议来处理吧
From Zhao to Everyone: (7:56 PM)
推荐去看看微信的技术类公众号，有很多好的文章，特别适合了解国内高并发处理的常用方案，大厂案例
From Bam to Everyone: (7:57 PM)
求推荐公众号
From Laoluo to Everyone: (7:57 PM)
可以发上来给大家参考一下大家都提高了以后讨论的水平就慢慢上来了
From Zhao to Everyone: (7:57 PM)
我经常看51CTO技术栈的
From Sean Gao to Everyone: (7:57 PM)
google 就能搜到很多
From Taotao to Everyone: (7:57 PM)
因该叫transcoding 才对
From 石登辉 to Everyone: (8:02 PM)
视频再用http的gzip没啥意义了
From 石登辉 to Everyone: (8:02 PM)
一般是文本文件
From Zhao to Everyone: (8:06 PM)
问一下大家，是不是可以把一些细节讨论放后面。在讲完HLD后，可以把Failure Handling 和scale up 先大致讲讲，然后再看面试官态度决定深挖哪个，以及schema design，etc? 有没有人用过这个策略？
From Taotao to Everyone: (8:06 PM)
现在讨论的这些都没啥意义，去看看dash 和hls的规范才好。现在的设计更像是民科有专门的协议的
From First Last to Everyone: (8:07 PM)
求link
From Sean Gao to Everyone: (8:07 PM)
关键面试不考 dash
From Zhao to Everyone: (8:07 PM)
😅
From Taotao to Everyone: (8:13 PM)
bookmark的sync也是重点考察的一方面
From xing wang to Everyone: (8:14 PM)
多谢分享！
From Sean Gao to Everyone: (8:14 PM)
谢谢大家
From John to Everyone: (8:14 PM)
谢谢！
From Laoluo to Everyone: (8:14 PM)
谢谢！
From Yvonne to Everyone: (8:14 PM)
谢谢
From 非洲黑猴子 to Everyone: (8:14 PM)
谢谢

## Donation
From Ken to Everyone: (7:12 PM)
7:12Meeting notes: https://docs.google.com/document/d/19wMqh91tdvcTw9UqeWljFZSxbVVl2KRw_FX2ZC13974/edit
From ningdi to Everyone: (7:14 PM)
原来是 round up amount 捐钱啊 😂 以为是捐食物呢
From Ken to Everyone: (7:18 PM)
Interview 7:12->7:57. Meeting notes: https://docs.google.com/document/d/19wMqh91tdvcTw9UqeWljFZSxbVVl2KRw_FX2ZC13974/edit
From ningdi to Everyone: (7:25 PM)
Hahah food order基本就集中在某几个小时
From james to Everyone: (7:25 PM)
1M/Day = 12qps !
From ningdi to Everyone: (7:25 PM)
Qps不应该这么算，很容易爆
From shawnzheng to Everyone: (7:26 PM)
刚加入 donation和doordash有什么关系？
From ningdi to Everyone: (7:26 PM)
Check out的时候给选项想你要钱捐给你指定的charities
From shawnzheng to Everyone: (7:28 PM)
Hmm 但是很多人都不会捐款吧 算order的qps我有点confuse
From ningdi to Everyone: (7:28 PM)
😂 是的
From Cheng Jing to Everyone: (7:28 PM)
有道理唉，我基本没捐过🤦‍♂️
From ningdi to Everyone: (7:29 PM)
建议面试前捐一点。。
From kevin to Everyone: (7:29 PM)
shawnzheng说到点子上了
From Hu to Everyone: (7:29 PM)
没懂为什么除以3600就行了 不需要除以24
From fengxiong to Everyone: (7:29 PM)
因为peak hour才有人要吃饭
From ningdi to Everyone: (7:29 PM)
这个还是比较好理解的吧，虽然不代表每个order都会有捐款，但是这个不就是跟order分不开的。
From Cheng Jing to Everyone: (7:30 PM)
是说，doordash的order，基本都集中在饭点，而不是平均到每个小时
From Yanbin Li to Everyone: (7:30 PM)
面试官刚才纠正了，是每小时3million，只不过note上没改
From ningdi to Everyone: (7:30 PM)
。。。 dd用户量这么大了？
From kevin to Everyone: (7:31 PM)
假设
From Yanbin Li to Everyone: (7:31 PM)
感觉问qps这部分，有一个很重要的数没问，就是第三方payment能承受的qps是多少，这个直接影响solution还有第三方的latency
From fengxiong to Everyone: (7:32 PM)
既然可以承受 点单，那么donation也可以
From ningdi to Everyone: (7:32 PM)
你是第三方的client， latency你需要关心，但是第三方的qps又不是你一个人在用，关心他干嘛。。
From kevin to Everyone: (7:32 PM)
Yanbin给你加分
From Yanbin Li to Everyone: (7:32 PM)
这种集成一般都有SLA，不是你想怎么call就怎么call的
From kevin to Everyone: (7:32 PM)
说的非常好
From Ken to Everyone: (7:32 PM)
Interview 7:12->7:57. Meeting notes: https://docs.google.com/document/d/19wMqh91tdvcTw9UqeWljFZSxbVVl2KRw_FX2ZC13974/edit
From ningdi to Everyone: (7:33 PM)
Sla是 latency， 第三方支持的qps不是你一个人独享，你拿到了也没用吧？
From Ender Li to Everyone: (7:34 PM)
一般大客户都是要说好的，我大概每秒会call多少，银行payment这种api和普通面向网站的是不一样的，不是假设多少量都要接着的
From claire lin to Everyone: (7:35 PM)
你这个怎么告诉饭店 订单下了，啥时候饭菜送到？
From ningdi to Everyone: (7:35 PM)
那在这道题里面，会出现 难点 第三方qps不支持你的order量？emmmm。。
From kevin to Everyone: (7:36 PM)
一般来说都假设第三方能够承受这样的流量，但是面试者最好问一下，这样说明面试者考虑比较周全
From ningdi to Everyone: (7:37 PM)
话两头说吧。。。 思虑周全 跟 over design 之间，没有界限 😂
From claire lin to Everyone: (7:37 PM)
如果用async call，那你怎么confirm ？
From Bam to Everyone: (7:38 PM)
所以订菜和捐款是两个API么？
From kevin to Everyone: (7:38 PM)
没有design的事情，就是说一下你assume第三方api能够handle，仅此而已，除非面试官说不是这样告诉你具体的信息
From ningdi to Everyone: (7:38 PM)
看起来是把 food order跟 donation order彻底分开了。
From Bam to Everyone: (7:38 PM)
我感觉不大好，应该一起来，both or nothing
From ningdi to Everyone: (7:39 PM)
前面verify了 不能让donation amount 影响regular order
From Ender Li to Everyone: (7:39 PM)
你在澄清需求的时候考虑到了问出来，就叫思虑周全。问都没问就假设第三方支持不了，然后一顿设计，就叫over design
From kevin to Everyone: (7:39 PM)
ender说的对
From ningdi to Everyone: (7:39 PM)
啊对对对
From Ender Li to Everyone: (7:40 PM)
个人感觉第三方支持不了你的qps是个很好的follow up
From kevin to Everyone: (7:40 PM)
是的可以是个follow up而且不仅仅是技术方面可以涉及到产品的设计
From Bam to Everyone: (7:41 PM)
话说订菜失败，捐款成功的话怎么办
From Ender Li to Everyone: (7:42 PM)
定菜成功后再place捐款的订单做成workflow
From Bam to Everyone: (7:42 PM)
这个可以有
From kevin to Everyone: (7:42 PM)
非常好
From fengxiong to Everyone: (7:42 PM)
nb
From kevin to Everyone: (7:43 PM)
处理这个case会加分说明有深入的思考
From claire lin to Everyone: (7:44 PM)
一般是先hold钱，然后order，order成功charge，否则cancel hold
From Ender Li to Everyone: (7:45 PM)
不好意思没听到，payment method是个啥
From Sebastian Su to Everyone: (7:45 PM)
apiToken 是JWT之类的嘛
From Laoluo to Everyone: (7:45 PM)
是不是原则上订餐和捐款同一个transaction?
From Bam to Everyone: (7:46 PM)
不是，只有捐款失败则不rollback
From Kai’s iPhone to Everyone: (7:46 PM)
不应该啊吧
From Peng Su to Everyone: (7:46 PM)
请问apiToken是干啥用的
From s to Everyone: (7:46 PM)
donation 的qps应该远小于订餐qps吧，这样的话第三方handle不了的可能性是不是不大 ？
From kevin to Everyone: (7:46 PM)
可以是，但不是必须
From ningdi to Everyone: (7:46 PM)
这个design目前看起来像我们面试experience的rest api 的考点。。
From Laoluo to Everyone: (7:46 PM)
API token实际上没有必要单独列出来，不同的实现会有不同的参数象secretkey
From Kai’s iPhone to Everyone: (7:46 PM)
你捐款不能影响主业务啊万一捐款api挂了 你order不就全挂了
From Laoluo to Everyone: (7:46 PM)
有道理，订餐为主
From kevin to Everyone: (7:47 PM)
这个要clarify，是不是捐款失败，订餐还可以成功
From Ender Li to Everyone: (7:47 PM)
感觉是不是一个transaction取决于agreement怎么写的，不过一般公司都会倾向于用户接受订餐成功捐款失败的条款吧。这个面试的时候推荐问一下面试官
From Laoluo to Everyone: (7:47 PM)
这里讨论是订餐后，单独把捐款另外做？
From Ken to Everyone: (7:48 PM)
Interview 7:12->7:57. Meeting notes: https://docs.google.com/document/d/19wMqh91tdvcTw9UqeWljFZSxbVVl2KRw_FX2ZC13974/edit
From Zhao to Everyone: (7:48 PM)
捐款不一定需要当场实现，可以是公司每个周/月汇总了再捐。所以点菜+捐款做成一个transaction，只要记录一下捐款量就好。
From xiao to Everyone: (7:48 PM)
Update_ts用来干嘛呀
From james to Everyone: (7:49 PM)
Sql能处理多少ups?pqs
From Dingwen Chen to Everyone: (7:49 PM)
订餐transaction包括了payment和donation了吗？
From Ender Li to Everyone: (7:49 PM)
我觉得面试题如果这样问比较有意思：现在doordash想增加donation功能，你怎么设计？
From fengxiong to Everyone: (7:49 PM)
用到消息队列了
From ningdi to Everyone: (7:49 PM)
代码实现的时候 可不想把 donation的代码写进 正常food order的code里面， 出了问题一起完蛋。 还是从系统上去区分跟互相影响吧。 额外做个mq去坚挺payment success 的消息 然后去get order里面有没有捐款，做aysnc捐款
From james to Everyone: (7:49 PM)
多少qps以上就不能用sql?
From kevin to Everyone: (7:50 PM)
Zhao说的太好了，能从业务角度去思考，大加分
From Cheng Jing to Everyone: (7:50 PM)
1000qps以上吧
From Dingwen Chen to Everyone: (7:50 PM)
放在payment里面，有cash back， donation， tips选项
From Pu Wang to Everyone: (7:50 PM)
没有这种说法，1000QPS是single node的sql db，sql db也是scable的
From ningdi to Everyone: (7:50 PM)
Sql一个是1000 如果读写都有的话 纯写的话不知道了
From Ken to Everyone: (7:51 PM)
Interview 7:12->7:57. Meeting notes: https://docs.google.com/document/d/19wMqh91tdvcTw9UqeWljFZSxbVVl2KRw_FX2ZC13974/edit
From Kai’s iPhone to Everyone: (7:51 PM)
有道理因为order可能被取消
From Dingwen Chen to Everyone: (7:51 PM)
加多一个VIP ID for donation
From Kai’s iPhone to Everyone: (7:51 PM)
所以donate可能被rollbqck
From ningdi to Everyone: (7:52 PM)
其实你们下单donate了之后，你们银行里面是几个transaction？
From Shihao Zhong to Everyone: (7:52 PM)
肯定是俩啊
From kevin to Everyone: (7:52 PM)
Ender的想法非常好，算是给面试官的feedback
From Bam to Everyone: (7:52 PM)
这个图是怎么生成的？
From Dingwen Chen to Everyone: (7:52 PM)
一个吧
From ningdi to Everyone: (7:52 PM)
这个说 肯定是俩的 是真的例子 还是想当然啊。。。没捐过目前 不知道到底是几个
From Ender Li to Everyone: (7:53 PM)
收款人一个是doordash，一个是慈善组织，没法一个吧
From Shihao Zhong to Everyone: (7:53 PM)
对啊。
From ningdi to Everyone: (7:53 PM)
都是doordash hold钱月底结账给商家。。
From Bam to Everyone: (7:53 PM)
也可以dd汇总之后每月打款
From Pu Wang to Everyone: (7:54 PM)
是的，一般只是record下，再处理donate
From ningdi to Everyone: (7:54 PM)
直接给钱的话，投诉啥的，扣不了钱。。
From Bam to Everyone: (7:54 PM)
那一笔转账更好点，否则transaction fee受不了捐2毛，被银行收1毛
From kevin to Everyone: (7:55 PM)
是的
From Dingwen Chen to Everyone: (7:55 PM)
类似tips， 如果不是不同的payment method， 就是一个transaction
From Bam to Everyone: (7:55 PM)
而且可以assume用户用的是同一张卡吧，要么一起成功一起失败
From Ender Li to Everyone: (7:55 PM)
这个点很赞，所以doordash按月汇总捐款更合理
From Shihao Zhong to Everyone: (7:56 PM)
有道理诶
From Bam to Everyone: (7:56 PM)
应该不会有人特地用两张卡结账吧
From kevin to Everyone: (7:56 PM)
给zhao点赞
From Ender Li to Everyone: (7:56 PM)
前面做成一个给doordash的transaction就可以，逻辑还简单了
From Eric Che to Everyone: (7:56 PM)
为什么要用kafka，而不是mq？Kafka虽然可以当mq，但并不能完全取代mq
From kevin to Everyone: (7:57 PM)
一种实现二用
From christie Yu to Everyone: (7:57 PM)
SQL db 是不是不容易做sharding啊？
From kevin to Everyone: (7:57 PM)
一种实现而已
From ningdi to Everyone: (7:57 PM)
zhao 到底说了啥。。
From ningdi to Everyone: (7:57 PM)
啥一种实现2用。。。 我还往上翻。。
From Jing to Everyone: (7:58 PM)
为什么read heavy？order应该是write heavy吧
From kevin to Everyone: (7:58 PM)
Zhao说捐款是每月汇总结算
From tomdi to Everyone: (7:58 PM)
order cache 不太需要
From xiao to Everyone: (7:58 PM)
Order cache怎么用啊这里
From Sebastian Su to Everyone: (7:59 PM)
order cache 确实不太需要。
From Sean Gao to Everyone: (7:59 PM)
查询order ？
From ningdi to Everyone: (7:59 PM)
读status的时候 尤其是recent status 读比较频繁
From tomdi to Everyone: (7:59 PM)
1 master
From Yibin to Everyone: (7:59 PM)
1 master for consistency
From Shihao Zhong to Everyone: (7:59 PM)
用mysql怎么俩master啊。
From Sean Gao to Everyone: (7:59 PM)
多数据中心的话，是不是 1 master per colo ?
From ningdi to Everyone: (8:00 PM)
2个master也可以多consistency 只需要你保证某个信息 只会被/永远只会 被其中一个master update对于某个record他是 single master就行。。
From Shihao Zhong to Everyone: (8:01 PM)
喔，那需要加个中间件
From Bam to Everyone: (8:01 PM)
Mysql 有Read/Quorum么？
From christie Yu to Everyone: (8:01 PM)
没有吧read/write quorum 只有leaderless replication 有吧
From Am to Everyone: (8:02 PM)
L4是sde2？
From ningdi to Everyone: (8:02 PM)
他说的都是gg的
From Am to Everyone: (8:02 PM)
thx
From Ken to Everyone: (8:06 PM)
https://docs.google.com/document/d/19wMqh91tdvcTw9UqeWljFZSxbVVl2KRw_FX2ZC13974/edit#
From Yibin to Everyone: (8:09 PM)
考官可以讲一下如果是L5的话还有哪方面需要改进的吗
From Sean Gao to Everyone: (8:11 PM)
+1
From Kai’s iPhone to Everyone: (8:13 PM)
+1
From Jiayue(Hubert) Wu to Everyone: (8:18 PM)
Database hook是什么呀，搜了一下好像没搜到
From kevin to Everyone: (8:18 PM)
我猜是数据库触发器
From Cheng Jing to Everyone: (8:18 PM)
sql triggers?
From Sebastian Su to Everyone: (8:20 PM)
+1 没懂db hook 是什么，做什么的
From Mark to Everyone: (8:23 PM)
如果是L5 是hire吗？
From Lucas Li to Everyone: (8:23 PM)
这位美女可以上L5么
From Chris to Everyone: (8:23 PM)
题目太简单了
From tomdi to Everyone: (8:23 PM)
L5可以加面一轮
From Chris to Everyone: (8:23 PM)
l5会问比较难的idempotency
From Kai’s iPhone to Everyone: (8:26 PM)
面试官经验丰富
From Mark to Everyone: (8:26 PM)
一般考官都不知道
From ningdi to Everyone: (8:27 PM)
debrief的时候 那个图内容这么少 能back up吗。。
From james to Everyone: (8:29 PM)
能谈谈sql还是no-sql的选择吗？
From Joselyn phone to Everyone: (8:30 PM)
同问db hook
From Jiashen to Everyone: (8:31 PM)
可以share 一下总结的doc吗
From Savannah Tong to Everyone: (8:32 PM)
https://docs.google.com/document/d/19wMqh91tdvcTw9UqeWljFZSxbVVl2KRw_FX2ZC13974/edit#
From ningdi to Everyone: (8:32 PM)
😂 茶壶煮饺子
From Sean Gao to Everyone: (8:32 PM)
👍
From ningdi to Everyone: (8:32 PM)
好比喻啊
From Taotao to Everyone: (8:32 PM)
当前这瓢冷水泼的很好，这些点说的都挺好，听得很有帮助。但是我觉得面试者communication很好了
From Yibin to Everyone: (8:33 PM)
谢谢分享！很有帮助！
From Kai’s iPhone to Everyone: (8:33 PM)
感谢
From Mark to Everyone: (8:34 PM)
谁知道面试官看重什么
From Sean Gao to Everyone: (8:37 PM)
但是涉及 分布式事务吧
From ningdi to Everyone: (8:37 PM)
面试前没题啊。 😂我以为最好的面试flow是 high level图画出来 然后面试官想展开那个module 再详细讲。。 不知道对不对。。。
From Mark to Everyone: (8:39 PM)
从business角度上看 哪些材料更好学习
From Chris to Everyone: (8:48 PM)
看题目 有些题目需要estimate
From s to Everyone: (8:48 PM)
跟solution还是有关系的qps很小的话，可能都不需要你scale的
From Sebastian Su to Everyone: (8:50 PM)
一般多少qps 以内是不用distributed
From s to Everyone: (8:50 PM)
这个很容易考察你是不是真有经验
From Chris to Everyone: (8:51 PM)
qps 不高，都不用queue了spofactive passive
From Savannah Tong to Everyone: (8:56 PM)
db hook https://orientdb.com/docs/2.2.x/Hook.html
From Lucas Li to Everyone: (9:09 PM)
不能用异步的队列吧
万一信用卡信息不对呢

## Google drive
From Tekken to Everyone: (7:06 PM)
第一次做mock面试官 做的不足的地方 大家多指教 🙏
From 老黄瓜 to Everyone: (7:06 PM)
老哥谦虚了 请开始你的表演！
From Ken to Everyone: (7:15 PM)
https://docs.google.com/document/d/19WtV88EbH8_t5J8bxZ6tAbMJz0iO6EtsKetzun55UC8/edit#
From Yu Zheng to Everyone: (7:17 PM)
google drive has desktop version too...
From Li to Everyone: (7:17 PM)
+1
From jun to Everyone: (7:17 PM)
+1
From Xinyu Zhang to Everyone: (7:17 PM)
+1
From 老黄瓜 to Everyone: (7:17 PM)
So what’s the difference between dropbox vs google drive?In terms of user functionality
From Ken to Everyone: (7:18 PM)
https://docs.google.com/document/d/19WtV88EbH8_t5J8bxZ6tAbMJz0iO6EtsKetzun55UC8/edit#includes meeting notes and QR codeTime: 7:16-8:01
From jun to Everyone: (7:19 PM)
Thanks
From Ken to Everyone: (7:19 PM)
Calendar for future events: https://www.designclub.mingdaoschool.com/calendar.html
From HW to Everyone: (7:20 PM)
Tom是interviewee吗？
From jun to Everyone: (7:20 PM)
Delete files?
From 老黄瓜 to Everyone: (7:20 PM)
Update files?
From Xinyu Zhang to Everyone: (7:20 PM)
多人实时更新文件功能 为什么算是bonus部分啊？
From Yu Zheng to Everyone: (7:20 PM)
是啊。。为啥都是面试人自己写 functionality
From EE to Everyone: (7:21 PM)
这个设计更像是file system
From xiaonan to Everyone: (7:21 PM)
因为卷...
From Zhengguan Li to Everyone: (7:22 PM)
...
From HW to Everyone: (7:22 PM)
req collection应该是考察的一部分
From jun to Everyone: (7:22 PM)
+1
From Yu Zheng to Everyone: (7:22 PM)
是啊。。
From James to Everyone: (7:22 PM)
share files 不管了？面试官提了好几回
From 应Jianghong to Everyone: (7:22 PM)
3个9也太低了
From EE to Everyone: (7:22 PM)
Drive also makes it easy for others to edit and collaborate on files
From Tony Y to Everyone: (7:22 PM)
面试的小伙不要看chat哈
From Xinyu Zhang to Everyone: (7:23 PM)
而且多人会同时修改文件 还要处理consistent
From Peng Su to Everyone: (7:23 PM)
真实的面试我从来没碰到过严格按照这套流程走的都是有来有往的讨论
From Yu Zheng to Everyone: (7:23 PM)
因为这个题目是提前知道的。。所以准备的时候倾向过了点
From jun to Everyone: (7:23 PM)
It is driven by the interviewee now
From 老黄瓜 to Everyone: (7:23 PM)
@Peng Su 能说下是咋样的来往讨论呢？
From Peng Su to Everyone: (7:23 PM)
一开始给的题目也会有更多的细节，不会是就给个5个单词的题目
From la s er to Everyone: (7:24 PM)
面试者 是不是 比较太 aggressive了？还是我的错觉
From lw to Everyone: (7:24 PM)
要我没用过难道不让面了。。
From EE to Everyone: (7:24 PM)
mock的senior level？
From Yu Zheng to Everyone: (7:24 PM)
没用过就去跟面试官聊 user case
From James to Everyone: (7:24 PM)
+1 没用过就去跟面试官聊 user case
From Ken to Everyone: (7:24 PM)
https://docs.google.com/document/d/19WtV88EbH8_t5J8bxZ6tAbMJz0iO6EtsKetzun55UC8/edit#includes meeting notes and QR codeTime: 7:16-8:01
From lw to Everyone: (7:24 PM)
对呀。感觉还是和面试官聊吧。
From xiaonan to Everyone: (7:25 PM)
根据我的经验，没用过基本就是挂了。除非你是天才
From 老黄瓜 to Everyone: (7:25 PM)
啊 没用过正常吧 design tinder 你咋说
From lw to Everyone: (7:25 PM)
那没用的多了。投票里不是还有stock exchange么。
From 应Jianghong to Everyone: (7:26 PM)
tinder还是有可能用过的
From Yu Zheng to Everyone: (7:26 PM)
因为 feature 是对方给的。。你只要知道 feature 对应的 user case 就可以了。。。
From Ken to Everyone: (7:26 PM)
Ming Dao School event calendar:https://www.designclub.mingdaoschool.com/calendar.html Vote for popular questionshttps://www.designclub.mingdaoschool.com/popular-interview.htmlhttps://docs.google.com/document/d/19WtV88EbH8_t5J8bxZ6tAbMJz0iO6EtsKetzun55UC8/edit#includes meeting notes and QR codeTime: 7:16-8:01
From iPad to Everyone: (7:26 PM)
没用过很正常
From 应Jianghong to Everyone: (7:26 PM)
有的面试题就是没productionize的feature
From EE to Everyone: (7:26 PM)
cloud storage和Google drive的痛点不一样
From Peng Su to Everyone: (7:27 PM)
一般是先设计一个MVP，不考虑scale，然后再根据面试官的提问，往不同的方向走
From xiaonan to Everyone: (7:27 PM)
没用过你基本上只能往通用的一些点靠，从而失去了特定题目的特点。往往要考察的通常是这些特点
From Peng Su to Everyone: (7:27 PM)
比如有的会问resiliency，有的问scale
From Xinyu Zhang to Everyone: (7:27 PM)
这个bandwidth没必要算的具体吧 和interviewer聊聊差不多的级别就可以了吧
From xiaonan to Everyone: (7:27 PM)
不是所有面试官都有来有往的
From lw to Everyone: (7:27 PM)
可以问面试官吧。这才是沟通。不然不是背答案。。
From Yu Zheng to Everyone: (7:27 PM)
没用过产品不代表没用过类似的。。我没用过 pin 但是可以用过竞争对手的
From xiaonan to Everyone: (7:27 PM)
L5基本上你要drive整个过程你当然可以问does it make sense这种问题
From Cory Wang to Everyone: (7:28 PM)
drive没错，但是scope requirements的时候还是要问问interviewer吧
From xiaonan to Everyone: (7:28 PM)
但是我遇到的考官就是不给你任何提示
From Yu Zheng to Everyone: (7:28 PM)
drive 的是 design。。不是 requirement 吧。。
From xiaonan to Everyone: (7:29 PM)
咱得做好最坏的打算
From Peng Su to Everyone: (7:29 PM)
对，drive的意思是有很多条路，根据需求选一条
From EE to Everyone: (7:29 PM)
这就是了cloud file system不是Google drive
From jun to Everyone: (7:29 PM)
20 files
From Yu Zheng to Everyone: (7:29 PM)
drive 的是 conversion。。不是说脑补需求。。这个有区别的。。
From Peng Su to Everyone: (7:29 PM)
Drive的意思不是我就按照我自己的路线开车
From jun to Everyone: (7:29 PM)
1024/50
From Xinyu Zhang to Everyone: (7:29 PM)
（requirement7分钟 + capacity6分钟）
From Ken to Everyone: (7:30 PM)
Time: 7:16-8:01Meeting notes and QR codehttps://docs.google.com/document/d/19WtV88EbH8_t5J8bxZ6tAbMJz0iO6EtsKetzun55UC8/edit#
From jun to Everyone: (7:31 PM)
permission is at the front
From 老黄瓜 to Everyone: (7:31 PM)
CRUD 每个功能一个Service?
From Kun Zhang to Everyone: (7:32 PM)
As a serverless?
From jun to Everyone: (7:32 PM)
no cache?
From Eric Haung to Everyone: (7:32 PM)
Auth service 不是应该在Upload, download, delete, list files service etc的前面吗？
From 老黄瓜 to Everyone: (7:33 PM)
感觉我已经confuse了
From Xinyu Zhang to Everyone: (7:33 PM)
DB
From Tony Y to Everyone: (7:33 PM)
冷静。。可能只是general api
From James to Everyone: (7:33 PM)
不理hint。不太好吧
From 应Jianghong to Everyone: (7:33 PM)
Eric说得对
From Xinyu Zhang to Everyone: (7:33 PM)
连DB都没有
From Yu Zheng to Everyone: (7:33 PM)
的确是。。忽视 hint 貌似是个常见错误
From 应Jianghong to Everyone: (7:33 PM)
Auth service应该是一个截面
From jun to Everyone: (7:34 PM)
Google drive shall use single-signon, right?
From Li to Everyone: (7:34 PM)
“连DB都没有” +1
From Kai to Everyone: (7:34 PM)
Is it overkill to build once service  for each upload/download/delete action?
From Eric Haung to Everyone: (7:35 PM)
could file storage应该是他想表达的DBcloud*
From Tong Liu to Everyone: (7:35 PM)
Cache可以先不设计吗，到scale那步再加？
From Li to Everyone: (7:35 PM)
DB需要存metadata
From Kai to Everyone: (7:35 PM)
Metadata +1
From jun to Everyone: (7:35 PM)
metadata+1
From Vivian huai to Everyone: (7:35 PM)
interviewee好像不需要interviewer讲话就好的感觉 >_<
From Li to Everyone: (7:35 PM)
file的metadata, user的metadata， 各个device的metadata， etc
From 应Jianghong to Everyone: (7:35 PM)
理论上的话one service per each crud operation可以horizonal scaling
From 老黄瓜 to Everyone: (7:35 PM)
感觉可以稍微更high-level，比如 user -> API gateway -> CRUD service -> DB. etc 然后每个部分细节再展开，可能让观众更容易跟上
From 应Jianghong to Everyone: (7:36 PM)
实际上嘛我估计代码会很bloated
From Xinyu Zhang to Everyone: (7:36 PM)
DB要存的东西挺多的 那个FS和DB完全是不一样的东西
From Qiqi to Everyone: (7:36 PM)
存metadata为啥不能直接用cloud?
From jun to Everyone: (7:36 PM)
It does not have a big picture
From Eric Haung to Everyone: (7:36 PM)
感觉走太快了 应该把各个component 说一次，make sure 和面试官是same page
From Tony Y to Everyone: (7:36 PM)
先写一个mvp然后再考虑scale呢？
From Xinyu Zhang to Everyone: (7:36 PM)
而且这个API很不rest
From James to Everyone: (7:36 PM)
感觉沟通有点脱节
From Eric Haung to Everyone: (7:36 PM)
现在面试官可能很多问题，但是已经开始写API了。。
From Cory Wang to Everyone: (7:36 PM)
+1 沟通脱节
From lw to Everyone: (7:37 PM)
api需要写这么细嘛（是一个问题）？能不能不写这么多args。
From 应Jianghong to Everyone: (7:37 PM)
其实中间的service应该整合成一个application layer
From Kd to Everyone: (7:37 PM)
觉得是面试不太够，没有把一个深度的东西画出来。 那个图可能没有任何信息量
From Eric Haung to Everyone: (7:37 PM)
+1
From jun to Everyone: (7:37 PM)
+1
From Yu Zheng to Everyone: (7:37 PM)
do you know that....very bad wording lol
From Vivian huai to Everyone: (7:37 PM)
+1 沟通脱节
From xiaonan to Everyone: (7:37 PM)
面试者过会应该会更新他的图吧
From Tony Y to Everyone: (7:38 PM)
offline sync 是个functional requiremnet
From Xinyu Zhang to Everyone: (7:38 PM)
这个API要写的话可以 file/  GET/PUT/DELETE
From jun to Everyone: (7:38 PM)
drive to details too fast
From Eric Haung to Everyone: (7:38 PM)
POST,GET,Delete
From jun to Everyone: (7:38 PM)
HEAD?
From Tony Y to Everyone: (7:38 PM)
api我经常不写。。。
From jun to Everyone: (7:39 PM)
no user db?
From 应Jianghong to Everyone: (7:39 PM)
我突然意识到一个问题，这个里头是不是缺了个front end?
From jun to Everyone: (7:39 PM)
web/clientapp
From 应Jianghong to Everyone: (7:39 PM)
要不然你怎么sync呢？
From iPhone to Everyone: (7:39 PM)
是不是他画的User？被挡住了…
From Eric Haung to Everyone: (7:40 PM)
好像面试的哥哥想一次性把自己的想法写出来 再慢慢讲解 这个缺点是 如果一开始就是错了 那么就浪费非常多时间
From Qi Wang to Everyone: (7:40 PM)
service的划分好像不太对，应该是读写service分离，然后用cache解决读的问题。
From jun to Everyone: (7:40 PM)
It looks like user call rest api directly
From James to Everyone: (7:40 PM)
我觉得可以在functional requirement的时候就写APIs
From Li to Everyone: (7:41 PM)
table存在了file system里面了，这明显错误了
From Xinyu Zhang to Everyone: (7:41 PM)
这个为啥要transaction啊
From Yu Zheng to Everyone: (7:42 PM)
哈哈太自信了
From emma to Everyone: (7:42 PM)
seems like the candidate doesn't know the difference between db and file storage
From Li to Everyone: (7:42 PM)
“seems like the candidate doesn't know the difference between db and file storage” +1
From Yu Zheng to Everyone: (7:42 PM)
indeed
From I to Everyone: (7:42 PM)
+1
From 老黄瓜 to Everyone: (7:42 PM)
感觉这个设计比较难collect signals
From jun to Everyone: (7:42 PM)
The question is too big to answer in 1 hour
From xiaonan to Everyone: (7:43 PM)
+1
From Vivian huai to Everyone: (7:43 PM)
感觉有点像背答案，不是真的理解
From Ming to Everyone: (7:43 PM)
Meta data 一般存db，对吧？
From Phia to Everyone: (7:43 PM)
不是应该follow interviewer direction吗？ 感觉小哥就是想把自己想讲的讲了
From jun to Everyone: (7:43 PM)
NOSQL DB
From Yu Zheng to Everyone: (7:43 PM)
恩，从 feature 开始就是很明显背答案了。。所以都在往自己准备过的上面去套。。。
From iPhone to Everyone: (7:43 PM)
先设计出个MVP感觉比较安全
From Jinmin’s iPad to Everyone: (7:43 PM)
I’d like to see the relationship between the permission service and the other services. I’d like to see a workflow about how permission service, upload service and db work together.
From 老黄瓜 to Everyone: (7:43 PM)
File blob 和 metadata 是存在一起吗？transaction 保证 metadata 就行了吧？
From Huimin Yang to Everyone: (7:44 PM)
怎么突然跳到chunk了。。
From Shihao Zhong to Everyone: (7:44 PM)
有个问题，这个题目的核心应该是做一个分布式文件系统还是说product的功能？
From Xinyu Zhang to Everyone: (7:44 PM)
而且直接就nosql了，每个api的qps全没用到。之前说了6分钟的capacity几乎没用到
From First Last to Everyone: (7:44 PM)
背答案 + 1
From Kd to Everyone: (7:44 PM)
感觉就是没有什么节奏可言
From Ming to Everyone: (7:44 PM)
因为要讲multi part upload
From 老黄瓜 to Everyone: (7:44 PM)
感觉pick too many small things
From Kd to Everyone: (7:44 PM)
整个flow没走通就已经开始detail了
From Ming to Everyone: (7:44 PM)
的确是太跳跃了
From Eric Haung to Everyone: (7:44 PM)
背答案 + 1
From First Last to Everyone: (7:44 PM)
感觉面试者不理解这个东西
From Li to Everyone: (7:44 PM)
这个设计没法做到多个device之间sync file change。需要大改。
From Yu Zheng to Everyone: (7:45 PM)
因为找到的答案就是这样。。。
From Jay to Everyone: (7:45 PM)
lol
From 老黄瓜 to Everyone: (7:45 PM)
Compression/chunk + zip 一句话可以带过的
From lw to Everyone: (7:45 PM)
可是答案也不是这样的。。
From iPhone to Everyone: (7:45 PM)
Chunks是哪里来的？
From First Last to Everyone: (7:45 PM)
图的component，完全不是这样的，答案也不是这样的。。
From jun to Everyone: (7:45 PM)
+1
From Kd to Everyone: (7:45 PM)
有好答案链接推荐吗
From Xinyu Zhang to Everyone: (7:45 PM)
“可是答案也不是这样的。” + 1
From Tony Y to Everyone: (7:45 PM)
感觉讲回来点了，先说chunk然后就可以推到为什么需要metadata了
From 应Jianghong to Everyone: (7:45 PM)
我是觉得最好还是top down drive的
From Kd to Everyone: (7:45 PM)
YouTube link? Web link?
From Ming to Everyone: (7:45 PM)
s3就是有用chunk。
From Yu Zheng to Everyone: (7:45 PM)
一会可以看看面试官准备的参考答案
From Ken to Everyone: (7:45 PM)
meeting notes and QR codehttps://docs.google.com/document/d/19WtV88EbH8_t5J8bxZ6tAbMJz0iO6EtsKetzun55UC8/edit#Time: 7:16-8:01Ming Dao School event calendar:https://www.designclub.mingdaoschool.com/calendar.html Vote for popular questionshttps://www.designclub.mingdaoschool.com/popular-interview.html
From 应Jianghong to Everyone: (7:45 PM)
面试官可能不care chunk
From Catherine zhang to Everyone: (7:45 PM)
大家不要做太多假设 再说本来来这里面试就是会提前做准备的啊
From Esther to Everyone: (7:46 PM)
面试官可能不care chunk +1
From Li to Everyone: (7:46 PM)
+1
From lw to Everyone: (7:46 PM)
背答案ok的呀。
From Zhengguan Li to Everyone: (7:46 PM)
(为啥我感觉说的还行...)
From First Last to Everyone: (7:46 PM)
参考答案：https://www.youtube.com/watch?v=PE4gwstWhmc Dropbox Senior Engineer design at Stanford University.
From lw to Everyone: (7:46 PM)
一样的题当然是背咯。
From Xinyu Zhang to Everyone: (7:46 PM)
30min了  7min requirement+6min capacity + 17min后来的这些 基本上真正面试已经结束了
From 老黄瓜 to Everyone: (7:46 PM)
Data estimation 也没说大文件，chunks 可能也不需要
From kk to Everyone: (7:46 PM)
那咱们讲点建设性的 怎样才能减少背答案的印象啊，有啥方法技巧需要注意吗
From jun to Everyone: (7:46 PM)
suppose u r a user
From Yu Zheng to Everyone: (7:47 PM)
碰到准备过的题目不要高兴得太早。。去挖掘对方感兴趣的考点。。而不是自己硬套
From lw to Everyone: (7:47 PM)
我觉得就是和面试官聊出来。
From Li to Everyone: (7:47 PM)
同一用户多个客户端/设备之间保持local的文件一致，这点完全没有谈
From lw to Everyone: (7:47 PM)
来出来就是自然的。聊
From iPhone to Everyone: (7:47 PM)
理解原理的话背答案没问题啊，面试官的问题都能处理好就行
From Yu Zheng to Everyone: (7:47 PM)
比如 feature 你可以说自己准备的，然后问他想要看到什么。。
From 应Jianghong to Everyone: (7:47 PM)
这个sync还是有一定技术难度的
From lw to Everyone: (7:47 PM)
而且面试官也知道你学过。。
From Vivian huai to Everyone: (7:47 PM)
和面试官聊和沟通很重要
From Kd to Everyone: (7:47 PM)
感觉主要是得刚开始先跟着面试官的要求走，然后再往自己的去套
From Yu Zheng to Everyone: (7:47 PM)
不要去 challenge interviewer。。去 合作解决问题。。
From Ming to Everyone: (7:48 PM)
+1
From DEFA WANG to Everyone: (7:48 PM)
开5个services。。。。
From Kd to Everyone: (7:48 PM)
就是一开始太根据自己的想法走，不理面试官的feeback，就会有种背答案的感觉了。
From Yu Zheng to Everyone: (7:48 PM)
比如面试官刚才说 meta db 和 file db 是不是要分开？你回答分开就是了。。
From Vivian huai to Everyone: (7:48 PM)
而且确实没听面试官在说啥
From lw to Everyone: (7:49 PM)
感觉这些service之后可以分开。一开始这么多，而且很相似，没啥必要。
From Cory Wang to Everyone: (7:49 PM)
high level design完成了吗？  这个图不是high level design吧
From 老黄瓜 to Everyone: (7:49 PM)
我没理解 他的数据是咋存的 NoSQL 是metadata 还是both
From jun to Everyone: (7:49 PM)
Sometimes interviewers work like that
From Cory Wang to Everyone: (7:49 PM)
Not workable solution
From Vivian huai to Everyone: (7:49 PM)
很多该说清楚的都没说清楚
From Xinyu Zhang to Everyone: (7:49 PM)
其实不带chunker也没问题 就是效率低呗  关键的file-detecter和每个file的version怎么查搞个minimal working solution 然后再说怎么不同用户sync， resolve conflict
From iPhone to Everyone: (7:49 PM)
为啥引入MQ？好像没做太多tradeoff的讨论和原因解释
From kk to Everyone: (7:49 PM)
感谢各位大佬的举例！
From First Last to Everyone: (7:49 PM)
Not workable solution + 1
From 老黄瓜 to Everyone: (7:50 PM)
还有一些 fail case 能讨论估计能更清晰，比如用户传到一半失败了，后面是重新来还是有checkpoint？用户 pull 最新的文件允许lag吗？用户2个手机，一个删文件一个加文件咋handle？
From Catherine zhang to Everyone: (7:50 PM)
MQ 在这里干什么 没听明白
From Huimin Yang to Everyone: (7:51 PM)
面试的时候跟面试官在same page很重要吧。。感觉不能各说各的
From tom to Everyone: (7:51 PM)
MQ在实时共享编辑的时候是需要的
From jun to Everyone: (7:51 PM)
If you delete a file, the operation will go to a mq
From Catherine zhang to Everyone: (7:52 PM)
这个chat里的tom是interviewee?
From Kevin to Everyone: (7:52 PM)
@Catherine，是的
From iPhone to Everyone: (7:52 PM)
不是说MQ在这里用的完全不对，是觉得应该解释下，否则就可能有点跳跃了
From Vivian huai to Everyone: (7:52 PM)
感觉interviewee就是把各种东西七拼八凑在一起，撞到关键词就算
From Kd to Everyone: (7:52 PM)
一般MQ是什么时候需要？解耦？
From Anony to Everyone: (7:52 PM)
Tom & Jerry
From Cory Wang to Everyone: (7:52 PM)
😂
From Catherine zhang to Everyone: (7:52 PM)
太厉害了 mutli-tasking
From jun to Everyone: (7:53 PM)
multi operation on a single resource
From 应Jianghong to Everyone: (7:53 PM)
MQ肯定是少不了的，但是没有讲service 和storage architecture就显得很跳脱
From Jerry to Everyone: (7:53 PM)
怎么解决version conflict
From Anony to Everyone: (7:53 PM)
还真有jerry呀
From Huimin Yang to Everyone: (7:54 PM)
...
From Vivian huai to Everyone: (7:54 PM)
没有给面试官深入探讨的机会。面试官一问就转移话题
From Shihao Zhong to Everyone: (7:54 PM)
别搞这个啊老哥
From christie Yu to Everyone: (7:54 PM)
为什么要讨论client connection option?
From emma to Everyone: (7:54 PM)
a walk through of a single flow is necessary
From christie Yu to Everyone: (7:54 PM)
这是在讨论 同时编辑一个文件嘛？
From Kd to Everyone: (7:54 PM)
https://www.youtube.com/watch?v=PE4gwstWhmc 这个就没有MQ呀？
From 老黄瓜 to Everyone: (7:54 PM)
感觉讨论都很细节 high-level不是很多
From Tony Y to Everyone: (7:54 PM)
就很真实。。。我第一次面亚麻就这样没准备好亚麻直接给了一年半冷冻期
From emma to Everyone: (7:55 PM)
感觉讨论都很细节 high-level不是很多 +1
From Dingwen Chen to Everyone: (7:55 PM)
好像用不上MQ， 至少没解释清楚
From 应Jianghong to Everyone: (7:55 PM)
network latency呢
From Hao Wu to Everyone: (7:55 PM)
感觉真实面试不需要这么多细节吧
From Li to Everyone: (7:55 PM)
“没有给面试官深入探讨的机会。面试官一问就转移话题” +1
From Xinyu Zhang to Everyone: (7:55 PM)
fifo? 放queue里的先后也有网络延时
From Qi Wang to Everyone: (7:55 PM)
为啥要用websocket，有啥场景需要么，
From jun to Everyone: (7:56 PM)
The interview do watch chats!
From Ming to Everyone: (7:56 PM)
我觉得细节也不够。很多东西刚开始讲，就没很detail就下一个了
From Tony Y to Everyone: (7:56 PM)
面试的人别看chats哈 会被影响的
From iPhone to Everyone: (7:56 PM)
权限还没检查呢，User有没有资格上传？
From ZZB to Everyone: (7:56 PM)
I think the permission/auth layer will not work in this design. It can not be parallel with other ops
From Catherine zhang to Everyone: (7:57 PM)
这个问题就是很大 都讲时间肯定不够 要先抓住一个feature 讲清楚 再说别的
From Ping Lu to Everyone: (7:57 PM)
系统是不是太大太复杂了，该怎么取舍才能把问题讲清楚，觉的遇到这样的系统设计题，挺难面的。
From Kd to Everyone: (7:57 PM)
是个好策略
From Xinyu Zhang to Everyone: (7:57 PM)
这个题要fanout么？
From Yu Zheng to Everyone: (7:57 PM)
一定要挖掘大概也可以。。但是没必要吧
From xiaonan to Everyone: (7:57 PM)
chunk check在client端做是不是更容易一些？
From Huimin Yang to Everyone: (7:57 PM)
问面试官想dive deep哪里，然后讲那一块就好了
From Xinyu Zhang to Everyone: (7:57 PM)
pull mode不行么？
From ZZB to Everyone: (7:57 PM)
MQ 为啥做的？
From Ming to Everyone: (7:58 PM)
面试官会guide的，跟着面试官就好。大部分时候说完high level，抓一两个深入讨论。
From Vivian huai to Everyone: (7:58 PM)
铺的很大，但是都没讲清楚
From jun to Everyone: (7:58 PM)
+1
From First Last to Everyone: (7:58 PM)
逻辑很混乱
From Ping Lu to Everyone: (7:59 PM)
也有面试官让你自己决定，
From Yu Zheng to Everyone: (7:59 PM)
先有个 mvp 比较好。。。hint 一直在给
From 老黄瓜 to Everyone: (7:59 PM)
感觉应该把核心数据流走完 能满足用户需求 再去加cache或者别的
From Kevin Li to Everyone: (7:59 PM)
面试官第三次说run a use case了 。。
From Cory Wang to Everyone: (7:59 PM)
+1
From 老黄瓜 to Everyone: (7:59 PM)
先满足 functional 再去想 non-functional
From jun to Everyone: (7:59 PM)
面试官已经尽力了
From Cory Wang to Everyone: (7:59 PM)
同意老黄瓜讲的
From Jerry to Everyone: (7:59 PM)
MQ应该是上传的时候时间太长的情况用吧，其它操作感觉都要很强很及时的consistency
From Ken to Everyone: (8:00 PM)
meeting notes and QR codehttps://docs.google.com/document/d/19WtV88EbH8_t5J8bxZ6tAbMJz0iO6EtsKetzun55UC8/edit#Time: 7:16-8:01
From Vivian huai to Everyone: (8:00 PM)
interviewer可以控制下时间了
From First Last to Everyone: (8:00 PM)
时间到了！
From Neng Wang to Everyone: (8:00 PM)
时间到了吧
From iPad to Everyone: (8:00 PM)
面试官说了一万次放下手下的..跑跑usecase
From iPhone to Everyone: (8:00 PM)
名词有点多，有俩MQ吗？不好意思，有点跟不上了
From Ken to Everyone: (8:00 PM)
30 seconds..
From jun to Everyone: (8:00 PM)
上传的时候mq里面的命令已经执行了
From Li to Everyone: (8:00 PM)
彻底的混乱了。。。。。。
From Yu Zheng to Everyone: (8:00 PM)
杯具。。时间到了，还没 workable
From Huimin Yang to Everyone: (8:01 PM)
。。。
From Li to Everyone: (8:01 PM)
因为最简单的case都跑不通，不停的加/改，设计已经失控了
From ZZB to Everyone: (8:01 PM)
为啥还有dedupe?
From lw to Everyone: (8:01 PM)
dedup也没那么重要。。
From iPhone to Everyone: (8:01 PM)
待会儿能让面试官和面试者领着大家过一遍这个解法吗？
From 老黄瓜 to Everyone: (8:01 PM)
感觉Tom同学还是有知识储备的 如果能自己给自己mock训练一下面试技巧会更好
From jun to Everyone: (8:01 PM)
感觉背了一堆细节
From Tony Y to Everyone: (8:01 PM)
dedupe is optmization
From Vivian huai to Everyone: (8:01 PM)
把背过的知识点都拿出来说一下。。。
From Kd to Everyone: (8:01 PM)
感觉Tom是看过DDIA的
From Vivian huai to Everyone: (8:02 PM)
ddia是啥
From Xinyu Zhang to Everyone: (8:02 PM)
dup是啥dup啊？ 同样的file么？ （存储便宜）
From lw to Everyone: (8:02 PM)
细节是ok。但是要面试官问吧。面试官都不想听这个。
From Ping Lu to Everyone: (8:02 PM)
看过能用上，很牛
From kk to Everyone: (8:02 PM)
待会能让面试官带着讲讲合理的时间分配吗
From iphone to Everyone: (8:02 PM)
Dedupe： remove duplicate
From Jerry to Everyone: (8:02 PM)
merle hash tree可以用来dedup吗merkle *
From Vivian huai to Everyone: (8:03 PM)
请问DDIA是什么
From Selena to Everyone: (8:03 PM)
野猪头
From Catherine zhang to Everyone: (8:03 PM)
dedup可以有很多很多方式 要based on use case
From Jerry to Everyone: (8:03 PM)
Designing Data-Intensive Applications (DDIA)书
From Vivian huai to Everyone: (8:03 PM)
谢谢
From jun to Everyone: (8:04 PM)
能给下slides？谢谢
From Neng Wang to Everyone: (8:04 PM)
没收到问卷
From Vivian huai to Everyone: (8:04 PM)
没收到问卷
From 老黄瓜 to Everyone: (8:04 PM)
同没收到
From 老黄瓜 to Everyone: (8:06 PM)
Hard skill 问卷收到了。。
From Vivian huai to Everyone: (8:07 PM)
+1
From Qi Wang to Everyone: (8:07 PM)
没收到问卷的是因为zoom client 版本没有升级到最新
From iPad to Everyone: (8:07 PM)
ipad的话貌似打开了chat poll就不popup*
From Neng Wang to Everyone: (8:07 PM)
我也是这次收到了
From 应Jianghong to Everyone: (8:07 PM)
下次应该让设计一个zoom poll system
From iPhone to Everyone: (8:07 PM)
能让面试官和面试者领着大家过一遍这个解法吗？谢谢
From 老黄瓜 to Everyone: (8:08 PM)
哈哈 下次来个 design zoom poll feature
From Ken to Everyone: (8:08 PM)
https://www.designclub.mingdaoschool.com/popular-interview.html
From Xinyu Zhang to Everyone: (8:08 PM)
是个fanout lol
From tom to Everyone: (8:08 PM)
tinder system design
From 老黄瓜 to Everyone: (8:09 PM)
@tom tinder 系统设计不简单的 😅
From HW to Everyone: (8:09 PM)
会有machine learning system design吗？
From ZZB to Everyone: (8:10 PM)
I can not see the screenNow I can
From ggg to Everyone: (8:11 PM)
问题是有点复杂太general了我印象中tinder要做geofencing还有一个分发机制吧YouTube 上有个简单的 mock tindertinder的主要的问题，是不是推荐系统？看面试官具体问什么。。。Tiner 没用过， 能假名要注册一么？面试官比较nice
From Catherine zhang to Everyone: (8:14 PM)
都要
From Xinyu Zhang to Everyone: (8:14 PM)
都要
From Esther to Everyone: (8:15 PM)
不打断 +1
From Yu Zheng to Everyone: (8:15 PM)
给 hint 不接受也没辙。。。
From Kd to Everyone: (8:15 PM)
我是面试者我肯定想让面试官打断我的，但我遇到的面试官都是很nice的，让我在错误的道路上越来越远
From Cory Wang to Everyone: (8:15 PM)
不打断然后最后给个no hire?
From Huimin Yang to Everyone: (8:15 PM)
我也觉得适当打断比较好
From jun to Everyone: (8:15 PM)
这个是捧杀
From Huimin Yang to Everyone: (8:16 PM)
不然最后都讲飞了
From jun to Everyone: (8:16 PM)
打断是棒杀
From 老黄瓜 to Everyone: (8:16 PM)
Tinder1. 发现附近的人2. 每个用户有自己的主页，能上传照片3. 匹配上的人能发起(实时)对话4. 能解除匹配
From Cory Wang to Everyone: (8:16 PM)
不打断是捧杀，打断是棒杀😂
From 应Jianghong to Everyone: (8:16 PM)
？？？
From emma to Everyone: (8:16 PM)
omg
From Qi Wang to Everyone: (8:16 PM)
这一点面试官很对，
From Xinyu Zhang to Everyone: (8:16 PM)
画图最好还是分着画
From iPhone to Everyone: (8:17 PM)
面试官应该是说画在两个框框里就比较清楚
From Ming to Everyone: (8:17 PM)
都是分开放的
From Yu Zheng to Everyone: (8:17 PM)
这个所有 mock interview 都是分开画得吧。。
From 1705081 Shimingyi Chen to Everyone: (8:17 PM)
db和file storage 都是分开的吧
From Qi Wang to Everyone: (8:17 PM)
逻辑上的cloud file storage 和db不是一回事
From Cory Wang to Everyone: (8:18 PM)
不打断但是design的不是我想要的，那给hire还是 no hire
From iphone to Everyone: (8:18 PM)
The purpose of interview is to please the interviewer(boss)
From Cory Wang to Everyone: (8:18 PM)
+1
From Yu Zheng to Everyone: (8:18 PM)
and provide enough signal for hire..
From emma to Everyone: (8:19 PM)
逻辑上的cloud file storage 和db不是一回事 +1
From Cory Wang to Everyone: (8:19 PM)
😂
From NL to Everyone: (8:19 PM)
愿意打断的都是好心的
From Vivian huai to Everyone: (8:19 PM)
面试官这个没法控制，每个人都有不同风格
From Catherine zhang to Everyone: (8:19 PM)
是的
From Jenny Xu to Everyone: (8:19 PM)
面试官逐渐安静不是啥好信号
From Cory Wang to Everyone: (8:20 PM)
面试官默默打开自己的电脑开始干自己的活
From Tony Y to Everyone: (8:20 PM)
嗯 可以马上聊sync
From 老黄瓜 to Everyone: (8:20 PM)
赞同 @panfeng 说的 很对
From 应Jianghong to Everyone: (8:20 PM)
在service level可能是同一个service你能够拿到metadata和文件，但是绝对不意味db和filestorage是一个logical  component
From Cory Wang to Everyone: (8:21 PM)
哈哈哈
From Xinyu Zhang to Everyone: (8:21 PM)
不clear要问面试官 +1
From Jenny Xu to Everyone: (8:22 PM)
+1
From iPhone to Everyone: (8:22 PM)
问面试官+1
From Yu Zheng to Everyone: (8:22 PM)
不要去 challenge 面试官。。。
From NL to Everyone: (8:22 PM)
面试者超级自信👍
From Vivian huai to Everyone: (8:22 PM)
不要去 challenge 面试官+1
From Ming to Everyone: (8:22 PM)
都是分开放的面试者不太尊重面试官。我们原来碰到过一次，直接pass。
From Cory Wang to Everyone: (8:22 PM)
不要去challenge面试官+1
From yao yao to Everyone: (8:22 PM)
那你可能是超级面试者。。。
From Qi Wang to Everyone: (8:22 PM)
面试官问sync是很正常的，也是很好心。
From lw to Everyone: (8:23 PM)
考点是sync。。怎么能不问呢。
From Li to Everyone: (8:23 PM)
“面试者不太尊重面试官。我们原来碰到过一次，直接pass。” +1
From Qi Wang to Everyone: (8:23 PM)
sync或者写锁是这个题的重要考点之一
From lw to Everyone: (8:23 PM)
deupe这种都是小事。
From First Last to Everyone: (8:24 PM)
面试者不太尊重面试官。我们原来碰到过一次，直接pass。” +1
From emma to Everyone: (8:24 PM)
面试者不太尊重面试官。我们原来碰到过一次，直接pass。” +1
From Esther to Everyone: (8:24 PM)
面试者不太尊重面试官。我们原来碰到过一次，直接pass。” +1
From Xinyu Zhang to Everyone: (8:24 PM)
pass是给过了？
From Catherine zhang to Everyone: (8:24 PM)
不能follow hint的 在我们这里 属于not a team player lol
From lw to Everyone: (8:24 PM)
pass是下一个。
From Cory Wang to Everyone: (8:24 PM)
pass是给挂了
From jun to Everyone: (8:24 PM)
太幽默了
From Yu Zheng to Everyone: (8:24 PM)
pass candidate, next one lol
From Xinyu Zhang to Everyone: (8:24 PM)
lol
From Ming to Everyone: (8:24 PM)
当然是fail，连feedback都省了
From First Last to Everyone: (8:24 PM)
pass掉，就是不理会，挂掉！
From Xinyu Zhang to Everyone: (8:25 PM)
一亩三分地耍久了 面经pass是过。。
From lw to Everyone: (8:25 PM)
提醒了吧。。
From Esther to Everyone: (8:26 PM)
提醒了吧。。 +1
From Jenny Xu to Everyone: (8:26 PM)
提醒挺多次了😂
From Cory Wang to Everyone: (8:26 PM)
提醒了 +1
From lw to Everyone: (8:26 PM)
主要没时间。只能看面试官想考啥。
From Xinyu Zhang to Everyone: (8:26 PM)
而且qps说了半天也没说到一个具体的数量级 比如多少k，而且最后api和DB选择也没用到
From Jenny Xu to Everyone: (8:27 PM)
面试者像极了几年前找工作的我。。。自嗨得不行
From iPhone to Everyone: (8:27 PM)
MVP先设计出来会比较安全
From Kd to Everyone: (8:27 PM)
MVP是什么？
From Xinyu Zhang to Everyone: (8:27 PM)
求面试官带着讲一个答案
From Jenny Xu to Everyone: (8:27 PM)
Minimun Variable Product
From Vivian huai to Everyone: (8:27 PM)
可以讨论下这个题目该怎么解答吗
From lw to Everyone: (8:28 PM)
告诉我9999我也没办法量化。
From jun to Everyone: (8:28 PM)
这个问题单讲可能需要100个小时
From lw to Everyone: (8:28 PM)
对的。
From Shihao Zhong to Everyone: (8:28 PM)
同样有这种怡文疑问
From emma to Everyone: (8:29 PM)
我觉得面试官可以更自信一些
From Vivian huai to Everyone: (8:29 PM)
+1
From Huimin Yang to Everyone: (8:29 PM)
这里说的L4是Google的L4嘛
From Cory Wang to Everyone: (8:29 PM)
当面试官也不容易
From 老黄瓜 to Everyone: (8:29 PM)
L4不是不考系统设计吗😂
From lw to Everyone: (8:30 PM)
avaiability有不需要的么。主要看面试官考不考。
From Panfeng Xue to Everyone: (8:30 PM)
L4 也会考
From ZZ to Everyone: (8:30 PM)
要考的
From Catherine zhang to Everyone: (8:30 PM)
会比较简单
From 201703005 Di Ha to Everyone: (8:30 PM)
只有Google的L4不考sd，其它公司的mid senior都考sd
From Yue to Everyone: (8:31 PM)
MLE的话考sd吗
From Yu Zheng to Everyone: (8:32 PM)
考的。。只是侧重点不太一样
From Catherine zhang to Everyone: (8:32 PM)
考ml sd
From lw to Everyone: (8:33 PM)
kafka consumer group？
From Tony Y to Everyone: (8:33 PM)
书似乎实在有kafka之前写的
From HW to Everyone: (8:33 PM)
其实有点想了解ml sd的interview，不知道铭道会有这方面的计划没有
From Shihao Zhong to Everyone: (8:33 PM)
感觉这个有一致性的问题把
From jun to Everyone: (8:33 PM)
这个是没办法的
From Tony Y to Everyone: (8:34 PM)
这里说的是每个user要有一个queue, 我看的时候觉得不一定是最优的
From Xinyu Zhang to Everyone: (8:34 PM)
不同client同时写 怎么handel啊？
From emma to Everyone: (8:34 PM)
dropbox好像用的是long polling
From Yu Zheng to Everyone: (8:34 PM)
google drive 其实本身还真有不少一致性问题没解决
From Tony Y to Everyone: (8:34 PM)
一致性书里也问题没讲我的想法是有conflict就算一个新的branch 算一个文件的副本
From Jerry to Everyone: (8:37 PM)
应该是先建metadata再传文件chunk最后完成在更新metadata吗
From 老黄瓜 to Everyone: (8:37 PM)
感觉需要 checksum，metadata 每个文件有状态，保证chunk 不完整的时候 metadata 状态不是 complete
From Jerry to Everyone: (8:38 PM)
drive属于object storage吗
From 老黄瓜 to Everyone: (8:38 PM)
比如 file_123, in-progress, 5/8, 同时维护 file_123 -> {chunk_123_0, …} 的映射
From Li to Everyone: (8:38 PM)
“应该是先建metadata再传文件chunk最后完成在更新metadata吗” +1
From Tekken to Everyone: (8:38 PM)
https://whimsical.com/google-drive-BBsXFU8DQX7tp9CMyTXASs
From Jerry to Everyone: (8:39 PM)
一次传不完，还要断点续传
From Kd to Everyone: (8:39 PM)
上传失败了可
From jun to Everyone: (8:40 PM)
你在硬盘上是连续存的？本地要存metadata, 然后比较哪个chuck被改了chunker切的。client里有chunker感觉就是性能好 但不是最基本所需要的对，应该不是 mvp 的图中的 workspace 是啥意思？赞同 可以假设用户只能上传 <10MB 的文件 然后完成MVP@Ming, 说得对在前端就block了
From iPhone to Everyone: (8:42 PM)
肯定是先访问Metadata Service啊，权限不得先检查才能忘File Server里写
From Qi Wang to Everyone: (8:43 PM)
我觉得这个图的基本思路没啥问题。
From Tony Y to Everyone: (8:44 PM)
查重可能是个花时间的活 所以先存cloud storage再去重？
From Xinyu Zhang to Everyone: (8:44 PM)
好奇 这个从cloud往本地sync的过程，假设download很慢，本地就是先存file, 然后改本地DB,是吧
From jun to Everyone: (8:45 PM)
我觉得太抠细节了
From Shihao Zhong to Everyone: (8:45 PM)
我觉得太抠细节了
+1
From Randy to Everyone: (8:45 PM)
“应该是先建metadata再传文件chunk最后完成在更新metadata吗” +1
client 先post https://FQDN/files/upload
拿到短期的storage URI， 然后client对内容进行分块，往storage URI里一块一块upload 
结束了，再把metadata改成 upload finished
From iPhone to Everyone: (8:45 PM)
安浏览器插件吧
From Shihao Zhong to Everyone: (8:45 PM)
@Randy甚至还是幂等操作
From Catherine zhang to Everyone: (8:46 PM)
同意 @randy solution
From lw to Everyone: (8:46 PM)
这个属于同步编辑了。
From Qi Wang to Everyone: (8:46 PM)
一开始提需求的时候不要提merge冲突的问题，这是个坑
From 老黄瓜 to Everyone: (8:46 PM)
感觉可以稍微优化，支持乱序并行批量上传
From iphone to Everyone: (8:46 PM)
Google drive and google doc is a different topic
From Qi Wang to Everyone: (8:46 PM)
加个写锁就完了
From emma to Everyone: (8:47 PM)
workspace是啥
From Qi Wang to Everyone: (8:47 PM)
保持文件原子性
From iPhone to Everyone: (8:47 PM)
不让改就完了
From Xinyu Zhang to Everyone: (8:47 PM)
那用户假设存了一个很重要的文件，但是另一个用户存了个同名的没用的file, 更新的时候被lock了
From tom to Everyone: (8:47 PM)
加写锁太慢了
From Cory Wang to Everyone: (8:47 PM)
😂
From Qi Wang to Everyone: (8:47 PM)
加写锁为啥慢，又不是全局写锁这里的写锁是逻辑上的，不是数据库里的写锁。
From HW to Everyone: (8:48 PM)
其实可以在这里讨论tradeoff
From jun to Everyone: (8:49 PM)
+1
From Shihao Zhong to Everyone: (8:49 PM)
直接传变成一个新version然后让用户自己决定哪个version有效呗
From Tony Y to Everyone: (8:49 PM)
offline sync 有锁也没用吧
From jun to Everyone: (8:49 PM)
感觉是技术大牛讨论方案
From tom to Everyone: (8:49 PM)
version diff conflict resolution+eventual consistency, conflict解决不了的时候roll back
From Kun Zhang to Everyone: (8:49 PM)
What is the data model for the per line lock?
From Jerry to Everyone: (8:49 PM)
那不就是github 了哈哈
From 应Jianghong to Everyone: (8:50 PM)
这么看的话metadata放前面其实更好解决锁的问题或者把锁的metadata单独拿出来放前面
From iPhone to Everyone: (8:51 PM)
HDFS就是客户端先chunk，然后访问Namenode的metadata，然后再去到Namenode给定的data nodes上传下载
From Amity to Everyone: (8:51 PM)
😂， Merge， solve conflict before allow update。
From Xinyu Zhang to Everyone: (8:51 PM)
好奇，本地从cloud下载的过程当中，本地文件被人为修改了。本地最好怎么改啊？
From Tony Y to Everyone: (8:51 PM)
start a new branch will sovle conflict
From Panfeng Xue to Everyone: (8:52 PM)
merge conflict
From ZZB to Everyone: (8:54 PM)
Is Long polling ok for pulling newest meta data (and download …)
From Qi Wang to Everyone: (8:54 PM)
不是的，是你一个客户端添加了一个new file，这个file 的信息要sync带其他连接的客户端上。
From 老黄瓜 to Everyone: (8:55 PM)
同感觉没必要作push 这样可能引入race condition，同时pull一样的文件需要做 version isolation
From Ken to Everyone: (8:58 PM)
Thanks everyone for coming.  If you want to join our WeChat group, the QRCode is here on top of the doc: https://docs.google.com/document/d/19WtV88EbH8_t5J8bxZ6tAbMJz0iO6EtsKetzun55UC8/edit#
From Xinyu Zhang to Everyone: (8:58 PM)
那不还是上传完之后更新么
From 老黄瓜 to Everyone: (9:01 PM)
这样会有一致性问题 保证所有chunk更新或者不更新
From iPhone to Everyone: (9:01 PM)
Metadata里面有权限信息，你可能根本没有权限去写Cloud Storage里面的某个Url目录
From 应Jianghong to Everyone: (9:02 PM)
这么弄怎么解决multi client sync的问题
From 老黄瓜 to Everyone: (9:02 PM)
没必要每个chunk去更新 等metadata完成 无非E2E latency慢一点
From Qi to Everyone: (9:02 PM)
文件上传是不可能走lb的一般都是系统分配一个url，直接传到s3或者对等的obj storage里
From Ken to Everyone: (9:04 PM)
Thanks everyone for coming.  If you want to join our WeChat group, the QRCode is here on top of the doc: https://docs.google.com/document/d/19WtV88EbH8_t5J8bxZ6tAbMJz0iO6EtsKetzun55UC8/edit#
From Xinyu Zhang to Everyone: (9:04 PM)
md没有就重新来呗，反正存储不贵
From 老黄瓜 to Everyone: (9:05 PM)
只要metadata DB能保证transaction 保证ALO message delivery没有问题
From Jingru Li to Everyone: (9:05 PM)
十分感谢主持这次session！:) :) :)
From Ken to Everyone: (9:06 PM)
谢谢大家！
From ZZB to Everyone: (9:07 PM)
Metadata 包括 chunk 信息？
From Vivian huai to Everyone: (9:08 PM)
notification server主要是为了不同用户的sync吧
From Kai Sun to Everyone: (9:09 PM)
十分感谢主持这次 session🙏🏼请问之后会公佈 recording 吗？
From 老黄瓜 to Everyone: (9:09 PM)
感觉不加notification也没啥问题
From Ken to Everyone: (9:10 PM)
目前没有计划发布 recording，@KS, 笔记都可以在 designclub.mingdaoschool.com 查到。
From Vivian huai to Everyone: (9:10 PM)
Notification server主要什么作用呢
From Kai Sun to Everyone: (9:10 PM)
谢谢 Ken 🙏🏼
From 老黄瓜 to Everyone: (9:12 PM)
可以做 lazy update
From Zhengguan Li to Everyone: (9:13 PM)
最新的不就是做一个full scan嘛？
From 老黄瓜 to Everyone: (9:13 PM)
没必要每次pull full scan
From Zhengguan Li to Everyone: (9:13 PM)
notification只有改动的
From Tiger to Everyone: (9:15 PM)
既然能常链接，为什么不能client每隔几秒pull一下status？是不是类似的？
From lw to Everyone: (9:15 PM)
pull就要respnose。long polling不用一直response。
From Zhengguan Li to Everyone: (9:16 PM)
SSE 使用 HTTP 协议，现有的服务器软件都支持。WebSocket 是一个独立协议。-https://www.bookstack.cn/read/webapi-tutorial/spilt.2.docs-server-sent-events.md
From Ken to Everyone: (9:16 PM)
https://mint-lillipilli-1b9.notion.site/Comparison-for-4-methods-for-asynchronous-request-and-response-over-HTTP-a372edb5b8614d63ba115fa7156187af
From Guest to Everyone: (9:17 PM)
Push is implemented by long polling?
From emma to Everyone: (9:17 PM)
np
From Zhengguan Li to Everyone: (9:20 PM)
虚拟机可以扩展port嘛？
From Eric Haung to Everyone: (9:21 PM)
请问那个LP是什么？
From Guest to Everyone: (9:25 PM)
Push is also http
From emma to Everyone: (9:30 PM)
strong hire
From Ming to Everyone: (9:30 PM)
L5也够了
From SmallCracker to Everyone: (9:58 PM)
这个画图得工具是什么啊？
From Tekken to Everyone: (9:58 PM)
whimsical
From SmallCracker to Everyone: (9:59 PM)
谢谢哈
From Peace to Everyone: (10:05 PM)
请问过去的mock interview recording在哪里能看到？
From Yao Xiao to Everyone: (10:07 PM)
老师应该下线了
From Ping Lu to Everyone: (10:07 PM)
谢谢！
From SmallCracker to Everyone: (10:07 PM)
谢谢！ 晚安
From Yao Xiao to Everyone: (10:07 PM)
自动解散吧
From Yang Bai to Everyone: (10:07 PM)
👍
From Yao Xiao to Everyone: (10:07 PM)
晚安
From Tekken to Everyone: (10:07 PM)
https://whimsical.com/google-drive-BBsXFU8DQX7tp9CMyTXASs