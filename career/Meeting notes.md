- [Behavior](#behavior)
- [Cassandra](#cassandra)
  - [Multi-User chat room](#multi-user-chat-room)
  - [Ticket master](#ticket-master)
  - [Calendar](#calendar)
  - [TopK](#topk)
  - [Youtube](#youtube)

# Behavior
 请问亚麻recruiter嘉宾，能给个内推吗？感谢  running away what?  要慎用“running away from bad things” 吧  听众能不能mute一下  哪个老铁 mute一下啊😯  Rong Yao  弹琴的可以mu一下吗  🙏  就是可以你来我往的感觉吧  Would you please give an example of a bar raising answer example?  主持人可以先全部过一遍slice吗？而不是自己的follow up，这样子大家可以有个全局观，谢谢  money incentive 可以搞笑的方式带过 diversity and inclusion 的话题怎么回答  我就怕我吹大劲儿  我也是  一定要提team work project 嗎？還是個人開發的也可以  Frank, thank you very much! I appreciate it!  吹牛就没输过  个人开发可以，看你怎么讲了。比如说你参加solo hackathon，赢了，就很牛逼了  所以做政府项目的要怎么描述  謝謝  Q: 对于刚毕业的学生，这些话题大部分都没有经历，HR或者recruiter如果面试NG， BQ questions他们应该会怎么问呢？ 还是按部就班的念稿吗，对于NG怎么准备BQ？因为没有工作经验，这些问题大部分都没有体会  project或者side project的经验，都可以讲  NG 可以说school project，里面也涉及到collaboration、communication 哇  这种学校的东西和在工作中，完全不一样 非要硬往上帖，只能编了  实在不行参加一些hackathon吧，很真实的团队合作了，大部分hackathon 36-48小时。构思，分工，熬夜，meet deadline，pitching。等于是压缩了一个季度的工作  能举个例子吗 什么叫complex  嗯嗯 同问  Paul 说到点子上了！  Paul刚才说啥来着？ 错过了  谢谢 Paul 补充！  16 Leadership now!  Shopify的特有吧  Paul说尽量选一个大的project，能说明你的实力的，方便评级更高  Project dive deep 是主要考察技术能力吧  感谢ganxie  可以讲完slide再一起提问吗  今天有录像吗  搞过sev0 算不算 厉害厉害  可以说 没有mistake吗？说了 会很尴尬吗 ：）  如果sev0 不是你写出来的话  会  没有mistake不太真实  blame game is on  就说会尴尬么。。。这么答就挂了？ 有人这样回答我的，我选择没挂他这一题。。。  这个故事好，背下来 这个故事要强调tech context吗？  这样来说， 总体 来说， 感觉我们就会招进来 差不多的人 突然想 艺术类 面试 ，越奇葩，越个性 越好  晚了，已经背下来了。  能讲一下例子吗 fail的例子  晚了，背完了已经  不能背啥啊  没人能crash我， 算法可以。  不怕 找人多mock！ hoho  戳中了说的就是我  晚了，已经背了。  上司的话需要analyze 但是你的customer 不管说什么必须要听  例子太好了，深深印入了脑海里  只有魔法才能打败魔法  晚了，已经背下来了。  这样会不会显得manager很笨  你可以编一下，说是隔壁组的manager  同问 感觉manager听到能挣钱很难会block你  跟直接manager有conflict不太合适  正常人早就同意了 你还和我争  对啊  有conflict一般都是 clarity不够，有个人没有拿到所有信息  话说面试官是中国人的话是好事吗  解释一下就行了，但是这样没scope啊。还是之前的故事好。 都背下来背下来  "有conflict一般都是 clarity不够，有个人没有拿到所有信息"
👍  Wendy 牛！同样一件事情说到了manager level  有没有5分钟中场休息，喝点水上个洗手间的?  现在下半场都要结束了😃  😂  确实new grad太难了，很难讲leadership的  Hr会给你发公司value  bq搞定亚麻 = 搞定一切公司  对NG要求不会很高  说实话，准备好了amazon的bq，其他公司的bq都稳了 哈哈哈，Gigas也是也么想的  上面说的可能有误导，我一会补充一下  请问有过往其他行业工作经历的转专业学生，面试讲故事可以用以前的工作经历吗？还是必须要讲和tech有关的项目或经历？  lol  爱因斯坦是谁  lol  🤣  太棒了  好像是个政客  爱泼斯坦他哥？  夸父是个好同志  大公司不需要招 那个level 的聪明人  wendy好牛啊  wendy可以考虑开个小红书账号分享职场面试信息吗  有时间的可以看看wiki和公司官网blog和公司的linkedin，然后我觉得每个glassdoor的positive评价也可以很好地看出一个公司地value  be myself 我怕没人给offer  爱因斯坦克  be best myself 不用谢～  👍 Paul Wendy 在理  大牛👍  哪家对人大于业务？  麦霸  leadership ownership ng可当作主人翁精神 肯定可以聊  大家可以读一下Ben Horowitz写的一本书叫The hard thing about hard things，里面有一章节叫Take care of the people, the products and the profits - in that order  职场俱乐部volunteer 很锻炼leadership 😀  这是回应之前的问题“哪家对人大于业务？”  又背过一个： 主动帮oncall engineer去解决livesite的incident  其实每家公司都是“人大于业务”，也是“业务大于人”，取决于您的观察点 对牛人，就是人大于业务， 如果您没有贡献，就是业务大于人，呵呵  你就说系统本身的限制，不说别人的问题 old system就是很菜  觉得ownership跟之前的第二个category “tell me a project you are proud of.” 和第五个category “leadership”都有点overlap？ 老师们的例子听起来好像是有点overlap。请问怎样解决一个例子好像可以对应很多问题的情况呢？ 谢谢大家  你还有第七页？  earn trust?  我参加的比较晚 请问能把ppt的几点title列一下吗？  +1  ppt没啥，没有老师讲的故事精彩 反客为主 6666  这就是handle ambiguity把  可以问回去  amazon的interview feedback要写的很详细的， 面试官主要是collect datapoint  但是最后也不给feedback  面试官会不会光听不写，然后回去编？😂  不可能 我从来没有见过这样的，那样问题就大了  Amazon onsite behavior question  https://www.1point3acres.com/bbs/thread-307462-1-1.html  earn trust是啥故事来着？  比如我最近面试一个SDM， 光feedback都写了近2000 words  请问我们有recording吗？来晚了，miss掉了前半部分😭  如果来面Amazon， 最好真的好好读读LP，一定要 不然很难过的  What is LP?  Leadership Principles  每个问题对应的lp在这里找：Amazon onsite behavior question  https://www.1point3acres.com/bbs/thread-307462-1-1.html 
From zhaozhonghao to Everyone: (8:59 PM)
 想问一下，如果问到tell me about your weakness, 应该怎么说？ 
From Paul Lou to Everyone: (8:59 PM)
 另一家重视BQ的公司是Apple，级别高一些的有一半或超过一半的面试是BQ 
From Hobite Sun to Everyone: (8:59 PM)
 https://www.kraftshala.com/blog/amazon-interview-questions/ 
From Hobite Sun to Everyone: (9:00 PM)
 上面的链接是amzn问题对应的lp 
From Betty Ho to Everyone: (9:00 PM)
 謝謝 
From Changzheng Rao to Everyone: (9:01 PM)
 十分感谢🙏 
From Kevin Wen to Everyone: (9:01 PM)
 我们几乎不问“weakness” 了吧，如果被问了，就举一个真实的故事， 还有您怎么自己提高的， 或者如果是manager的话，也可以讲如何利用下属或者同事来check balance 终归没有完美的人嘛 
From Andrew to Everyone: (9:01 PM)
 问的。我就被问到有什么hash feedback, harsh* 
From Kevin Wen to Everyone: (9:02 PM)
 Harsh feedback那个是“earn trust”的问题 收到feedback，如果是又问题，怎么解决，最后earn the trust back 
From Verity Chu to Everyone: (9:03 PM)
 谢谢 Ken 老师 
From lei chen to Everyone: (9:03 PM)
 earn trust 是承认错误吗 
From Tao Mao to Everyone: (9:03 PM)
 很好的回答！ 
From Paul Lou to Everyone: (9:03 PM)
 Kevin Wen很有经验 
From Kevin Wen to Everyone: (9:03 PM)
 如果是有问题，为嘛不承认？承认了改了就好 
From W D to Everyone: (9:03 PM)
 想问一下对于转码选手，讲的故事和cs不相关可以么？ 
From Anna to Everyone: (9:03 PM)
 想问对于 handle tight deadline，Couldn’t finish tasks before deadline这类问题，有没有比较好的回答 
From lei chen to Everyone: (9:04 PM)
 就主要是不懂earn trust 
From Kevin Wen to Everyone: (9:04 PM)
 如果没有问题， 就是要怎么提供资料，对事不对人的解决 
From lei chen to Everyone: (9:04 PM)
 有点lost 
From Kevin Wen to Everyone: (9:04 PM)
 谢谢Paul 
From Andrew to Everyone: (9:04 PM)
 有错误也不能承认啊 
From YL to Everyone: (9:04 PM)
 help peers也是earn trust 
From Kevin Wen to Everyone: (9:04 PM)
 learn Learn  from your scar 
From Becky to Everyone: (9:05 PM)
 想请问一下bq和system design对定level哪个权重比较高呢？ 
From Verity Chu to Everyone: (9:05 PM)
 谢谢老师们～ 
From Andrew to Everyone: (9:06 PM)
 可不可以试试就知道。。 
From Kevin Wen to Everyone: (9:06 PM)
 如果您要到L6+， 一定要“Learn from your scar” 
From Pencil to Everyone: (9:06 PM)
 无关的意思不大 
From Kevin Wen to Everyone: (9:06 PM)
 不能是大错误哈 
From Tony to Everyone: (9:07 PM)
 亚麻最近coding题不算难 
From Kevin Wen to Everyone: (9:07 PM)
 您不能说我把AWS或者Azure搞down了 
From Gigas to Everyone: (9:07 PM)
 amazon 的大量是指多少呢 
From Kevin Wen to Everyone: (9:07 PM)
 那就麻烦了 
From Yang to Everyone: (9:07 PM)
 好像没有回答刚才的问题 
From YL to Everyone: (9:07 PM)
 500+吧 
From May Zhu to Everyone: (9:07 PM)
 earn trust 就是和团队契合，团队觉得你可以完成工作内容，还有会和团队相处融洽。基本上各种BQ都是考研你值不值得trust吧 
From Andrew to Everyone: (9:07 PM)
 500够吗 
From Yang to Everyone: (9:07 PM)
 问题问的是已经通过了coding test拿到面试的前提下，bq能不能用其他行业的工作经历来回答 
From Kevin Wen to Everyone: (9:08 PM)
 亚麻肯定可以 
From jackson to Everyone: (9:08 PM)
 可以的，就想有个人说组织meeting 
From Tony to Everyone: (9:09 PM)
 最近面亚麻每一轮都问behavior 
From Yang to Everyone: (9:10 PM)
 谢谢大家！ 
From Kevin Wen to Everyone: (9:10 PM)
 亚麻BR对SDE一般还是要问一个简单的技术问题的 
From Tao Mao to Everyone: (9:10 PM)
 说没有！ 
From keira to Everyone: (9:10 PM)
 谢谢老师们分享！ 辛苦！ 
From Betty Ho to Everyone: (9:11 PM)
 謝謝今天的分享～受益良多 
From Lu Li to Everyone: (9:11 PM)
 谢谢老师们分享！两小时干货满满 收获很多！ 
From Isabella to Everyone: (9:11 PM)
 谢谢大家 
From Yang Bai to Everyone: (9:11 PM)
 非常感谢老师！ 
From Kevin Wen to Everyone: (9:11 PM)
 谢谢大家，我也很受教！！ 
From Sophie Chen to Everyone: (9:11 PM)
 謝謝各位老师今天的分享～!!! 
From May Zhu to Everyone: (9:12 PM)
 非常感谢各位老师的分享，非常受用！ 
From Selena to Everyone: (9:12 PM)
 对对 earn trust求解 
From wz to Everyone: (9:12 PM)
 Thank you Ken, Wendy & Frank! Thank you host! 




# Cassandra
 C* consistency levels: one -> quorum -> all  more options to choose  Typical, and more added later… CAP theorem: C vs. A tradeoff  not all company swap cassandra , some big company are still use it but if you use it in critical data/transaction business, then cassandra is NOT the right choice  请问 wide-column store 是啥意思  这样讲听不懂  没听到Cassandra的应用场景，请问早来的朋友点我几句  Quote - “if you use it in critical data/transaction business, then cassandra is NOT the right choice”  那cassandra有啥用。。  Typical use cases: chat messages, logs, IoT data, etc.  @Sh.W For cases that you need fast write: chat, event logs, etc  用zookeeper保存的  Service discovery  感谢解答。那这个Cassandra跟mongodb啥的有啥区别，为啥要用cassandra存log  没讨论过，后面可以问～  行，谢谢。那我就认为存log吧。。。  dns?  你把前面放个reverse proxy？ lb  https://teddyma.gitbooks.io/learncassandra/content/client/which_node_to_connect.html  最近在看firebase，就是document db，和MongoDB一样可以很好地用来快速储存聊天信息  active-passive lb  active-passive lb +10086  这些东西应该都是cloud provider管吧？  是的 是ring0 infra管的 跟application layer木啥关系  https://www.ibm.com/docs/en/b2b-integrator/5.2?topic=system-installing-apache-cassandra-apache-zookeeper  我觉得是问peer2peer怎么propogate 数据变化的 query node 1， node1怎么决定找下一个node，如果本node无此数据。  复制只是复制到replicator  vector 是啥  version vector  quorum-based还能是LWW吗  Cassandra没啥关系 看一下service discovery/service mesh一般怎么做  能不能share 一下ppt  我最后会发一个总结，slides会包含在里面，还有一些扩展阅读  为什么update不行？？  Cassandra released in 2008; dynamo in 2012 为啥说cassandra 抄dynamo？！  Dynamo 不是dynamoDB  请问可以share一下具体table长什么样吗？0基础有点看不懂啊。  Dynamo在SOSP‘07发表的：https://www.allthingsdistributed.com/files/amazon-dynamo-sosp2007.pdf  越删越多跟aws redshift有点像啊  Ok cool 抄挺快！  对 vacuum  HBase 和RocksDB也都有类似合并机制，两者都是LSM tree  log structure merge tree吗  是的  Log Structured Merge Tree  primary key = partition key + clustering key. clustering key is ordered  DynamoDB   是single leader的 不是P2P  根据CAP定理，只能同时满足两个，而由网络分区带来的分区错误风险是必然存在的，因此只能在CA中间选一个，Cassandra选择了AP  那cassandra有什么应用场景，跟mongodb 应用场景有什么不同  请问MySQL是什么呢？AC吗？  @Neal yes  CP  Cassandr一般用在高可用性的场景，即使整个集群就剩下一台机器了，也要能工作，用Raft等协议就保证不了这一点  RAFT是啥  猴子哥  跟 MySQL 集群具体的高可用和一致性方案有关吧？  Raft一致性协议，比Paxos简化  谢谢！系统设计如果遇到银行相关的得保证AC，也需要scalable，那他就没有partition tolerance吗？  https://donggeitnote.com/2021/10/16/raft/ 我们前几周讲的raft  这个讲raft讲的特别好
http://thesecretlivesofdata.com/raft/ 确切说，是newbie friendly  newbie dota best dota  +1  就是来听db选型的  类似于sortkey的一个东西？ denormalized吧  column可以存json 吗  column family, all columns within the same column family are stored sequentically on disk  Wikipedia has this  column family, all columns within the same column family are stored sequentically on disk 牛逼牛逼  A wide-column store is a type of NoSQL database. It uses tables, rows, and columns, but unlike a relational database, the names and format of the columns can vary from row to row in the same table  query within the same column family is faster than row-based db.  relational DB对column number有限制，Oracle是1000  那为什么还要定义schema  那这个wide-column和存json是一个意思吗？就是可以nest column吗？  value can be json blob  主要是在disk上的存储方式不同  
[file: (null)]
  Column family使用cluster key来定义的吗  妈同事介绍 暴露了 哈哈哈哈  我也给你介绍一个好不好 答主  大家还是注意保护好自己信息 不要乱开玩笑哈  列式存储 同一个column family的data存成一个文件  (key: value (key: value)) ?  这个稳步不搞明白 不知道往里面存啥 这个问题不搞明白 不知道往里面存啥  NoSQL有好几种，也不一样  parquet是可以的 col-oriented columnar storage format  大佬最后可以讲一下为啥淘汰了  😂  找对人，
Hire and develop the best
哈哈  
[file: (null)]
  要deprecated 掉啊？  业务导向  Cassandra严重依赖gossip，这个有性能问题  gossip好 我们的Cassandra就10几个node。。  超大规模的话 consistency就是bottleneck了  答主负责。不用不乱说  今天有个群里发过https://blog.softwaremill.com/what-is-wrong-with-apache-cassandra-materialized-views-a7a25431dad  👍  感谢感谢 respect  感谢 分享  cogs？还是cost？  Rebalancing 要花很久 我们也遇到过  写烂了是个好的总结  好几个小时  印度人😂  🤣  现代分布式系统 不考虑consistency就是原罪  那大部分nosql 都躺枪了  spanner  我上学的时候的proj就是simulate gossip。。。  兰伯特论文写的烂  我也想当个科学家 现在还来得及吗 最近的道路是不是转数据科学家  schemaless  FB在MySQL上面包了一层: TAO  为什么说Cassandra schemaless但是我们这里又定义了schema: Cassandra旧版本用的binary protocol Thrift-based API所以还支持schemaless。但是在新的实现里CQL+storage engine已经需要定义schema了。 https://stackoverflow.com/questions/63380973/in-cassandra-how-is-it-possible-to-save-data-in-a-column-name-while-leaving-the/63381339#63381339  所以最好用的是不是mysql + shard？  Tao这个名字很酷，难怪改名叫Meta  Thank you for praising my name.  Tao， 66666666666666  哈哈哈哈哈哈哈  技术互喷  transactional 是啥意思  事务嘛  事务是啥  要么都成功，要么都失败，ACID  甩锅失败 哈哈哈  啊，ACID啊  2pc  懂了  你的麦克风好像有点杂音  transaction是指的DB的ACID  有背景音  风很大  哈哈  我挺赞同东哥的观点  +1  +1 
From 非洲黑猴子 to Everyone: (7:58 PM)
 +2 
From weibo wang to Everyone: (7:58 PM)
 同意 
From Sh.W to Everyone: (7:59 PM)
 东哥6666 
From Zooey to Everyone: (7:59 PM)
 教练：这叫意识流选手 
From Sh.W to Everyone: (7:59 PM)
 意识流是啥 
From Mark Liu to Everyone: (7:59 PM)
 头脑风暴，如果有怀疑的位置，提出来，然后暂停，继续后面。 
From Michael Qiu to Everyone: (7:59 PM)
 对 我们就互相默认大家都是菜鸡。 
From YL to Everyone: (8:00 PM)
 同意，都是学习的 
From Sh.W to Everyone: (8:00 PM)
 同意，都是学习的 
From Zooey to Everyone: (8:00 PM)
 我联动的活动我自己都不懂 
From Sh.W to Everyone: (8:00 PM)
 respect 
From Zooey to Everyone: (8:00 PM)
 🤣 
From Sh.W to Everyone: (8:00 PM)
 666666666 999999999999 
From 非洲黑猴子 to Everyone: (8:00 PM)
 讨论才是大头，主要就从讨论中学习 
From Cheng Jing to Everyone: (8:00 PM)
 名词全都不知道的有✋ 
From Sh.W to Everyone: (8:00 PM)
 6翻了 
From Bei Z to Everyone: (8:00 PM)
 respect+1 
From Sh.W to Everyone: (8:00 PM)
 讨论才是大头，主要就从讨论中学习 
From Kevin Hu to Everyone: (8:01 PM)
 谢谢！ 
From Sh.W to Everyone: (8:01 PM)
 我也讨论时候学的东西好多 append only妈？ 
From Becky to Everyone: (8:02 PM)
 和mysql mvcc比较一下 
From 非洲黑猴子 to Everyone: (8:03 PM)
 SQL是从单机进化来的，不是原生分布式的，所以容易支持事务 
From 木仓馆长 to Everyone: (8:04 PM)
 transaction里面那个isolation也非常重要 


## Multi-User chat room
From Michael to Everyone: (7:12 PM)
 没声音？ 
From 2002079 Xi Zhou to Everyone: (7:12 PM)
 有呀 
From Qian Teng to Everyone: (7:12 PM)
 +1 
From lining to Everyone: (7:12 PM)
 有 
From yyh Ace to Everyone: (7:12 PM)
 你要join audio吧 
From A to Everyone: (7:13 PM)
 啥时候开始。。 
From Michael to Everyone: (7:13 PM)
 OK 不小心点错了 
From Ken to Everyone: (7:15 PM)
 meeting notes: https://docs.google.com/document/d/1Hfnhg09v9ISJ20151u7PDTpvzqjpp5ajiNI5h-KevY0/edit# 
From YL to Everyone: (7:16 PM)
 这字体这么fancy吗 
From Jian Zhu to Everyone: (7:17 PM)
 看着好难受 - - 
From Becky to Everyone: (7:19 PM)
 @group member 
From Jiabei Luo to Everyone: (7:19 PM)
 开始时候介绍用的slide能share一下嘛？具体就是l4 l5 criteria的那一页。谢谢~ 
From A to Everyone: (7:20 PM)
 又要开始算算术了吗 
From Weida to Everyone: (7:20 PM)
 感觉面试官不大兴奋？ 
From 2002079 Xi Zhou to Everyone: (7:21 PM)
 lol 
From Jiabei Luo to Everyone: (7:21 PM)
 can you add additional people to a 1:1 chat? And turn it into group chat 
From xinz to Everyone: (7:21 PM)
 面试官是senior 吗？ 
From A to Everyone: (7:21 PM)
 这是在reverse engineering 微信吗 
From xing wang to Everyone: (7:22 PM)
 大多数面试官都这样，把舞台交给应试者 
From A to Everyone: (7:22 PM)
 请向答主要个表情包feature，谢谢。 
From 2002079 Xi Zhou to Everyone: (7:23 PM)
 Edit  已经发送的信息 feature 
From tomdi to Everyone: (7:23 PM)
 2min内可以recall message 
From Becky to Everyone: (7:23 PM)
 Online status 要不要 
From v to Everyone: (7:24 PM)
 感觉这边很多聊天软件设计理念和wechat区别挺大。。。总以wechat作为出发点 可能非国人会confuse 
From Eddie菜 to Everyone: (7:24 PM)
 正在偷人...... 
From ningdi to Everyone: (7:24 PM)
 要需求的这个聊天方式好棒 
From Ken to Everyone: (7:24 PM)
 Soft Skills: 1: requirements gathering 2: make decisions and justify tradeoffs 3: describe the solution using clear presentation, concise language and accurate technical terms  Hard Skills: 1: design quality; scalability, reliability, efficiency etc (L4, L5) 2: basic facts about existing software solutions and hardware capabilities (L4 - partly, L5) 3: project lifecycle awareness, e.g. how a project is developed and maintained (L5) 
From Jiabei Luo to Everyone: (7:24 PM)
 Edit or delete message (like discord) ? 
From A to Everyone: (7:24 PM)
 谁在偷人 
From Will to Everyone: (7:24 PM)
 请问record在哪里能看到？ 
From A to Everyone: (7:24 PM)
 这么刺激 
From AAA to Everyone: (7:25 PM)
 exciting 
From Anita Chen to Everyone: (7:25 PM)
 一般來說和面試官confirim req應該佔多長時間呀？ 
From Robin to Everyone: (7:25 PM)
 会不会聊太多requirement了，这些很难能在45min内聊清楚吧，我觉得能深入说清楚1:1chat和group chat就很不错了 
From Weilong Ding to Everyone: (7:26 PM)
 少于十分钟吧 
From Becky to Everyone: (7:26 PM)
 Retrieve chat history from local storage or remote? 
From A to Everyone: (7:26 PM)
 同感，我觉得requirement时间有点长 
From Feng Gao to Everyone: (7:26 PM)
 有以往的mock recording吗 
From xing wang to Everyone: (7:26 PM)
 计算存储量了吗？ 
From Richard Tu to Everyone: (7:26 PM)
 确实感觉，如果45分钟总时间的话，更符合真实interview 
From A to Everyone: (7:26 PM)
 计算存储量为啥？硬盘很贵吗 
From yingzhu to Everyone: (7:26 PM)
 没见过这么久clarify的。。。 
From kk to Everyone: (7:27 PM)
 同意。感觉确认mvp就好了。
比如图片功能等等在最后optimization/extension的时候再聊吧 
From 非洲黑猴子 to Everyone: (7:27 PM)
 原来不就是个Multi user的聊天室吗？现在微信快设计全了，除了朋友圈 
From YL to Everyone: (7:27 PM)
 1+1， group，notification就可以讲很久了 
From 2002079 Xi Zhou to Everyone: (7:27 PM)
 一般clarification  大概多少分钟 
From Feng Gao to Everyone: (7:27 PM)
 NR 一般还会有个HA吧 
From xinz to Everyone: (7:27 PM)
 聊天信息的顺序要保证吧 
From A to Everyone: (7:27 PM)
 NR是啥，HA是啥 
From Michael to Everyone: (7:27 PM)
 real-time message应该是要的吧 
From xinz to Everyone: (7:27 PM)
 还有availability 也要保证吧 
From Feng Gao to Everyone: (7:28 PM)
 HA: High availability
NR: Non-functional requirement 
From A to Everyone: (7:28 PM)
 non-functional requirement直接跳过吧 
From v to Everyone: (7:28 PM)
 顺序只能保证每个人看到的order一样吧。。没办法global 保证顺序？ 
From A to Everyone: (7:28 PM)
 说不说的吧 
From YL to Everyone: (7:29 PM)
 这咋能跳过呢.. 
From Jiabei Luo to Everyone: (7:29 PM)
 顺序在毫秒级别差别应该没那么重要吧 
From Michael to Everyone: (7:29 PM)
 +1 
From v to Everyone: (7:29 PM)
 顺序不对会有些逻辑上的错误 
From Sean Gao to Everyone: (7:29 PM)
 顺序是不是在 server 端加 ts 作为 truth ？ 
From A to Everyone: (7:30 PM)
 为啥不能跳过呢？ NR 有啥用呢？ 
From ningdi to Everyone: (7:30 PM)
 server端加上也没办法保证ordering 
From v to Everyone: (7:30 PM)
 比如三个人群聊。。在c看来 a对b的回答比b对a的提问先到 
From xing wang to Everyone: (7:30 PM)
 45分钟吗？ 
From Sean Gao to Everyone: (7:31 PM)
 @v 有道理 
From Michael to Everyone: (7:31 PM)
 感觉snowflake的id就已经满足大部分要求了。happend-before relation和绝对的顺序感觉还挺难的。 
From YL to Everyone: (7:31 PM)
 顺序只能在自己端看到的是一样的 
From Fei to Everyone: (7:31 PM)
 顺序无所谓的 
From v to Everyone: (7:31 PM)
 顺序能不能用vector clock来解决？ 
From A to Everyone: (7:31 PM)
 顺序感觉不重要 大差不差就得 
From Fei to Everyone: (7:32 PM)
 只要保证partial order就可以了 
From A to Everyone: (7:32 PM)
 partial ordre是啥意思 
From AAA to Everyone: (7:32 PM)
 以前微信好像也会出现信息错位的问题 
From A to Everyone: (7:32 PM)
 微信现在也有 
From AAA to Everyone: (7:32 PM)
 所以不用确保完全正确吧 
From ningdi to Everyone: (7:32 PM)
 顺序都保证不了。。 就不是聊天了。。 
From Fei to Everyone: (7:32 PM)
 就是A说了话，引起B说话，显示的时候A在B前面，因果关系 
From Sean Gao to Everyone: (7:33 PM)
 @v 想了想， b对a 的response，一定晚于 a 的问题。如果server 端排序，那不可能b比a先到。 
From A to Everyone: (7:33 PM)
 有时候信息都丢了 
From ray to Everyone: (7:33 PM)
 it's the hardest problem to ensure the high consistency 
From Michael to Everyone: (7:33 PM)
 @v 我觉得不用vector clock,简单的lamport clock就行。但是要是面试会不会太复杂。 
From ningdi to Everyone: (7:33 PM)
 我们以前做过测试，一个群里看到的消息确实可能不是一个顺序 
From Ken to Everyone: (7:33 PM)
 Meeting notes with QR code to join WeChat group (if you have not joined yet): https://docs.google.com/document/d/1Hfnhg09v9ISJ20151u7PDTpvzqjpp5ajiNI5h-KevY0/edit# 
From zepengzhao to Everyone: (7:33 PM)
 duplicate 那些是不是reliability的问题呢 
From ray to Everyone: (7:33 PM)
 duplicate is still consistency issue I think 
From A to Everyone: (7:33 PM)
 b的response一定晚于a的问题啊。如果a的问题没有deliver，b问题都没看到，怎么会发response？ 
From xing wang to Everyone: (7:33 PM)
 用什么db讲了吗？ 
From A to Everyone: (7:33 PM)
 这和设计没关系。。基本法啊 
From Ender to Everyone: (7:34 PM)
 但是deliver到c的顺序可能是b在a前面 
From ray to Everyone: (7:34 PM)
 I didn't see any multi thread topic popped yet 
From v to Everyone: (7:34 PM)
 对于a和b是没问题。。。对于c来说。。。 顺序是乱的 
From YL to Everyone: (7:35 PM)
 a和b同时向对方发送信息，两个人的顺序就是不一样的 
From Sean Gao to Everyone: (7:35 PM)
 @v c如果严格读取 group 的 ts，就不会 
From v to Everyone: (7:35 PM)
 顺序可能是乱的。。比如a的问题发给c的时候延时特别大 
From Sean Gao to Everyone: (7:35 PM)
 @yl 他说的是 b回复a 的信息，是有关联的。 
From v to Everyone: (7:35 PM)
 服务器的时间是不准确的 可能会有回调 
From Weilong Ding to Everyone: (7:35 PM)
 timestamp是不可靠的 
From v to Everyone: (7:35 PM)
 除非用version id 类似lambo clock 
From Sean Gao to Everyone: (7:35 PM)
 回调确实会 
From A to Everyone: (7:36 PM)
 这个delivery顺序没法保证，除非牺牲latency。 上一条msg没收到，你就不发下一条信息。这样太扯了。 
From ningdi to Everyone: (7:36 PM)
 那种延迟也是对于client端来说的，但是对于server端，不应该存在servers有不同的 ordering。 
From A to Everyone: (7:36 PM)
 而且delivery 有quality of service要求的。要求不高的直接qos=0 发了就不管了，drop and go 
From 姚剣楠 to Everyone: (7:37 PM)
 用什么协议 需不需要提一下？websocket？ 
From Jerry to Everyone: (7:37 PM)
 服务器时间一般什么情况会回调？ 
From Feng Gao to Everyone: (7:37 PM)
 message应该就是用服务器端的timestamp 吧，存DB 
From zepengzhao to Everyone: (7:37 PM)
 我觉得要踩到点吧 
From ray to Everyone: (7:37 PM)
 I remember the wechat use the multi paxos 
From zepengzhao to Everyone: (7:37 PM)
 real time messaging 
From A to Everyone: (7:37 PM)
 这种message topic/queue都append only的吧，server发了， 爱收到不收到。。 
From zepengzhao to Everyone: (7:37 PM)
 paxos 是一致性的 
From Weilong Ding to Everyone: (7:37 PM)
 建议去看ddia有讲 
From Feng Gao to Everyone: (7:37 PM)
 message也要存数据库的吧 
From ningdi to Everyone: (7:37 PM)
 不同server的ts可能不一致。。感觉总是要把group放在一个server上 才能真的rely on ts 
From Jackie G to Everyone: (7:38 PM)
 Do we need to expand on UserMeta? What kind of metadata is stored? 
From bernini to Everyone: (7:38 PM)
 Ddia是书还是视频？ 
From Lucas Li to Everyone: (7:38 PM)
 要对比一下通信方式，HTTP Polling, Long Polling, WebSocket之间的区别么 
From zepengzhao to Everyone: (7:38 PM)
 要很多machine maintain tcp connection （websocket） 
From Weilong Ding to Everyone: (7:38 PM)
 书 
From Lucas Li to Everyone: (7:38 PM)
 这种面试，是不是每道题目都要提前准备一下啊 
From xing wang to Everyone: (7:38 PM)
 用什么协议 需不需要提一下？websocket？这是重点，应该开始就讲 
From zepengzhao to Everyone: (7:38 PM)
 trade off 比较 
From bernini to Everyone: (7:38 PM)
 我感觉要来不及了。。。 
From zepengzhao to Everyone: (7:39 PM)
 基本上requirement 讲太久了 
From Ken to Everyone: (7:39 PM)
 presentation: https://docs.google.com/presentation/d/1pWuOkQrxk_Eib3oBwEGccXnqSZXisuoeSfdfToYLK7w/edit?usp=sharing 
From Michael to Everyone: (7:39 PM)
 websocket scale也得注意 
From lining to Everyone: (7:39 PM)
 用NTP同步Server时间不就行了 
From 姚剣楠 to Everyone: (7:39 PM)
 Nginx + WebSocket这里 有个坑 就是6w 端口限制 这块设计好了 会是加分 
From zepengzhao to Everyone: (7:39 PM)
 像fb 45分钟 
From xing wang to Everyone: (7:39 PM)
 要对比一下通信方式，HTTP Polling, Long Polling, WebSocket之间的区别么，，，，同意！ 
From v to Everyone: (7:39 PM)
 websocket scale有啥问题？ 
From 姚剣楠 to Everyone: (7:39 PM)
 nginx能支撑的websocket连接数最大只有 65535吧 
From Sean Gao to Everyone: (7:40 PM)
 对，如果server time 同步了， 还有 ts 的问题么 ？ 
From 姚剣楠 to Everyone: (7:40 PM)
 》 websocket scale有啥问题？ 
From zepengzhao to Everyone: (7:40 PM)
 10分钟 requirement 10分钟high level，然后剩下20分钟deep dive 
From 姚剣楠 to Everyone: (7:40 PM)
 Websocket 文件描述符数量的调整下吧 
From Sean Gao to Everyone: (7:40 PM)
 tcp 链接是 5元组 判定唯一， 65535好像不是平静。 
From ningdi to Everyone: (7:40 PM)
 Server ts咋同步。。 每时每刻都syc吗。。 多个server如何确定谁是master的ts 
From 姚剣楠 to Everyone: (7:40 PM)
 每打开一个tcp链接 占用一个文件描述符 
From Sean Gao to Everyone: (7:40 PM)
 瓶颈 
From Michael to Everyone: (7:41 PM)
 @v 你得记录下哪个client在哪个websocket server上或者，client连上websocket server之后得subscript一个topic 
From Sean Gao to Everyone: (7:41 PM)
 @ningdi 好像有专门的协议 
From lining to Everyone: (7:41 PM)
 NTP 
From v to Everyone: (7:41 PM)
 我之前看好像一个server可以hold up to 1million websocket connection？ 
From zepengzhao to Everyone: (7:41 PM)
 要先有个server discover 吧 
From Sean Gao to Everyone: (7:41 PM)
 @v 那个是 WhatsApp 的 erlang 
From v to Everyone: (7:41 PM)
 对。。需要存下connection的信息 
From ningdi to Everyone: (7:42 PM)
 @Sean, 那time draft的情况也会发生吧 难道还能修复已经persist 到db的records？ 
From Sean Gao to Everyone: (7:42 PM)
 @ningdi 细节我不懂。。。 
From v to Everyone: (7:42 PM)
 erlang是个类似websocket的协议么 
From 姚剣楠 to Everyone: (7:42 PM)
 Websocket的连接量不是瓶颈 百万应该也没问题 但是前面要是有nginx 那nginx的6w5端口数 就是瓶颈了 
From ray to Everyone: (7:42 PM)
 right 
From Yijie Shen’s iPhone to Everyone: (7:42 PM)
 Scale 可以弄多个websocket handler 吗？ 
From Sean Gao to Everyone: (7:43 PM)
 thanks Jiannan 
From ningdi to Everyone: (7:43 PM)
 Nginx会成为battleneck？ 是因为websocket 这个协议导致的吗？ 
From Lucas Li to Everyone: (7:43 PM)
 websocket连的是http 服务器 一个机器大概50K个连接左右？ 
From 姚剣楠 to Everyone: (7:44 PM)
 https://blog.51cto.com/u_15300443/3091841 这里有人也处理过这个坑 
From ray to Everyone: (7:44 PM)
 how many tcp connections are available for one server? theoretically? 
From Lucas Li to Everyone: (7:44 PM)
 有个著名的10K问题 
From ningdi to Everyone: (7:44 PM)
 666 感谢 
From YL to Everyone: (7:44 PM)
 这不应该发送到group然后再发送给每个user吗 
From Lucas Li to Everyone: (7:45 PM)
 后来有100K,1M 50K应该没有问题 
From zepengzhao to Everyone: (7:45 PM)
 好像后端可以做成pub/sub 
From 姚剣楠 to Everyone: (7:45 PM)
 how many tcp connections are available for one server? theoretically? 如果你内存 cpu够大 几百万是完全没问题的 
From A to Everyone: (7:45 PM)
 group chat 必然是push啊 
From Sean Gao to Everyone: (7:45 PM)
 NTP server time sync
NTP is intended to synchronize all participating computers to within a few milliseconds of Coordinated Universal Time (UTC). It uses the intersection algorithm, a modified version of Marzullo's algorithm, to select accurate time servers and is designed to mitigate the effects of variable network latency. 
From zepengzhao to Everyone: (7:45 PM)
 group chat里面的参与者都subscribe到某个conversation 
From A to Everyone: (7:45 PM)
 肯定是pub、sub，一个groupchat就是一个topic 
From 姚剣楠 to Everyone: (7:45 PM)
 我试过 把websocket服务器的文件描述符改成200w 一台也能处理 
From ray to Everyone: (7:46 PM)
 google global database use time stamp for strong consistency 
From tomdi to Everyone: (7:46 PM)
 whatspp 一个server可以 5M connection 
From zepengzhao to Everyone: (7:46 PM)
 要clarify 不会有很多人 
From 姚剣楠 to Everyone: (7:46 PM)
 2. 文件描述符数量     可能需要调整内核参数，文件描述符的数量其实也是和内存相关的，因为每打开一个tcp连接，就得占用一个文件描述符。     内核参数：fs.file-max     这是和系统资源相关的，也不会是瓶颈 
From zepengzhao to Everyone: (7:46 PM)
 fanout太多人的话就会有performance问题 
From ray to Everyone: (7:46 PM)
 NTP is quite an important module for server 
From 姚剣楠 to Everyone: (7:46 PM)
 搬运工 供参考 
From zepengzhao to Everyone: (7:46 PM)
 这其实跟new feed的道理差不多 fanout 
From v to Everyone: (7:47 PM)
 感觉最开始提需求挖坑太多了 
From Robin to Everyone: (7:47 PM)
 +1 挖坑太多了 
From Lucas Li to Everyone: (7:47 PM)
 需求跟面试官都确认过的吧 
From zzb to Everyone: (7:47 PM)
 是的 应该就简单的 use case 开始做 
From ningdi to Everyone: (7:47 PM)
 nginx最多只能维持(65535*后端服务器IP个数)条websocket的长连接-> 意思是 我加很多台机器 其实也不算是啥瓶颈咯。 正常来说希望一个机器处理多少connection比较合适呢？ 
From A to Everyone: (7:47 PM)
 IBM 的chatting broker可以handle 最多100万个session 
From Lucas Li to Everyone: (7:48 PM)
 是不是面试官点头的，都要讨论啊 
From A to Everyone: (7:48 PM)
 为啥用websocket？ websocket协议有啥优势吗？ 
From xing wang to Everyone: (7:48 PM)
 超时了吗？有人记录时间吗？ 
From Lucas Li to Everyone: (7:48 PM)
 双向通信 
From Andrew Hou to Everyone: (7:48 PM)
 好奇的问下 multi user chat的系统设计 不应该是focus on 系统设计上吗，感觉现在是在说多人聊天的功能逻辑 
From zepengzhao to Everyone: (7:48 PM)
 而且很重要一点，好像把面试官当coworker会比较好， 而不是给他找个solution 
From A to Everyone: (7:48 PM)
 这里面没必要双向通信 
From ningdi to Everyone: (7:48 PM)
 说实话 面试官给的这个requirement 我都觉得没必要用websocket了  long pulling貌似都能处理的了 
From zepengzhao to Everyone: (7:48 PM)
 T5 
From kk to Everyone: (7:49 PM)
 聊天为什么没必要双向。。 
From zepengzhao to Everyone: (7:49 PM)
 要求drive， 还有如何应对feedback 
From v to Everyone: (7:49 PM)
 long pulling和web socket之前的pro con分部是啥？ 
From A to Everyone: (7:49 PM)
 因为你是在和server 双向通信，不是sender和receiver 
From zepengzhao to Everyone: (7:49 PM)
 都是lantency 
From Yijie Shen’s iPhone to Everyone: (7:49 PM)
 Rea time 是不是用web socket 比较好 
From bernini to Everyone: (7:49 PM)
 开销太大？ 
From A to Everyone: (7:49 PM)
 可以decouple sender and receiver 
From v to Everyone: (7:49 PM)
 啥情况下long pulling比较好？ 
From zepengzhao to Everyone: (7:49 PM)
 websocket latency最短 
From Lucas Li to Everyone: (7:49 PM)
 间隔短了服务器吃不消，间隔长了体验差 
From kk to Everyone: (7:49 PM)
 并没有long pulling比较好情况。 long pulling直接http，比较容易实现。 懒人专用。、 
From bernini to Everyone: (7:50 PM)
 发太多request了 
From ningdi to Everyone: (7:50 PM)
 这个requirement 感觉有conflict 一方面real time 一方面又 不login 不收消息。。。 
From Robin to Everyone: (7:50 PM)
 wesocket开销小，不需要每条message都要新开connection 
From 姚剣楠 to Everyone: (7:50 PM)
 作chat room ，websocket 或者SSE都可以，long pulling 没有优势吧 
From v to Everyone: (7:50 PM)
 是啊。。感觉websocket总是比long pulling好 
From Feng Gao to Everyone: (7:50 PM)
 感觉没时间设计storage了 
From zepengzhao to Everyone: (7:50 PM)
 不login不收notification 
From 姚剣楠 to Everyone: (7:50 PM)
 Websocket 熟悉框架的话 其实开发也很快 
From Andrew Hou to Everyone: (7:50 PM)
 多人聊天肯定 不是real time 第一 设计 notification 第二 异步推送 
From ray to Everyone: (7:50 PM)
 why not real time for group chat 
From A to Everyone: (7:50 PM)
 你看她的设计，明显这个backend是作为一个broker出现的 
From kk to Everyone: (7:50 PM)
 Long pulling ne 
From A to Everyone: (7:50 PM)
 没必要双向通信 
From Lucas Li to Everyone: (7:50 PM)
 这里的login应该和online两码事 
From kk to Everyone: (7:50 PM)
 能做到的，ws都能做到。 
From Jerry to Everyone: (7:51 PM)
 group chat是不是要分在线的和离线的两波用户讨论 
From Michael to Everyone: (7:51 PM)
 发送给在线user和离线user应该不一样吧 
From v to Everyone: (7:51 PM)
 为啥要用message queue Kafka能支持这么多topic么 
From Lucas Li to Everyone: (7:51 PM)
 解耦 
From lining to Everyone: (7:51 PM)
 1 to 1要用pub、sub吗？ 
From kk to Everyone: (7:51 PM)
 没必要。 
From 姚剣楠 to Everyone: (7:51 PM)
 Kafka能支持这么多topic么 同样的疑问 
From A to Everyone: (7:51 PM)
 解耦，高端词汇 
From v to Everyone: (7:52 PM)
 这为啥需要decouple 
From Feng Gao to Everyone: (7:52 PM)
 我也觉得有点奇怪，message为啥不放DB 
From zzb to Everyone: (7:52 PM)
 这里用不用socket 是具体实现问题 面试candidate 应该把重心放在模块上面 有哪些 数据类型 哪些数据DB 怎么跟系统交互 这里讲清楚 
From v to Everyone: (7:52 PM)
 感觉这么多topic kafka性能会有影响 
From 姚剣楠 to Everyone: (7:52 PM)
 每个聊天室 或者 1 对1的聊天 都抽象成websocket里面的一个channel 
From ray to Everyone: (7:52 PM)
 maybe use redis for the in-cache mem 
From lining to Everyone: (7:52 PM)
 如果 1 to 1 用pub, sub,那得多少topic 
From Lucas Li to Everyone: (7:52 PM)
 MQ先进先出，吞吐量大 
From zepengzhao to Everyone: (7:52 PM)
 可以做一个conversation 啊， 聊天参与者是conversation subscribers 
From kk to Everyone: (7:52 PM)
 chat server用mq，是没必要的。 
From Yijie Shen’s iPhone to Everyone: (7:53 PM)
 对方离线的时候 可以把msg 放到kafka, 在线的时候用websocket 
From zepengzhao to Everyone: (7:53 PM)
 101， group chat都可以吧 
From ningdi to Everyone: (7:53 PM)
 1:1也topic n^2 topic 
From v to Everyone: (7:53 PM)
 没必要啊。。。直接query一下就行了 
From A to Everyone: (7:53 PM)
 chat server 用mq很常见 
From ray to Everyone: (7:53 PM)
 too many topics 
From yingzhu to Everyone: (7:53 PM)
 有websocket了是不是没必要mq了？ 
From v to Everyone: (7:53 PM)
 没必要缓存啊 
From ningdi to Everyone: (7:53 PM)
 ws跟mq不冲突吧 
From ray to Everyone: (7:53 PM)
 message queues looks like used for service to service delivery not for the user to user chat :) 
From zepengzhao to Everyone: (7:53 PM)
 too many topics可以infra解决吧 
From kk to Everyone: (7:53 PM)
 mq用在聊天很常见？ 
From Lucas Li to Everyone: (7:54 PM)
 service来不及怎么处理啊 
From bernini to Everyone: (7:54 PM)
 实测wechat也丢消息 
From kk to Everyone: (7:54 PM)
 我表示怀疑。 
From sherry的 iPhone to Everyone: (7:54 PM)
 如果mq jam了怎么办 
From zepengzhao to Everyone: (7:54 PM)
 too many topic有问题吗 
From A to Everyone: (7:54 PM)
 mq可以1. 吧那些没有及时处理的请求存在mq里 2. 把messagelog存进queue里，用来做之后的分析 和存储，作为一个append only log存在 
From bernini to Everyone: (7:54 PM)
 离线sync写出过问题 
From Lucas Li to Everyone: (7:54 PM)
 水平切分 
From A to Everyone: (7:54 PM)
 水平切分是啥意思 
From xing wang to Everyone: (7:55 PM)
 今天的面试官太nice了 
From ray to Everyone: (7:55 PM)
 maybe the user pull the messages directly from the in-mem cache,? 
From zepengzhao to Everyone: (7:55 PM)
 而且没有问消息要存云端不 
From A to Everyone: (7:55 PM)
 horizontal sharding? 
From zepengzhao to Everyone: (7:55 PM)
 都没说好 
From A to Everyone: (7:55 PM)
 要cache干啥？ 
From 姚剣楠 to Everyone: (7:55 PM)
 直接存用户的聊天记录 不会有法律问题吗 
From v to Everyone: (7:55 PM)
 这设计write fanout amplification也太大了。。。五百人的群 每个消息都写500份到kafka 
From Yijie Shen’s iPhone to Everyone: (7:55 PM)
 面试官输出的比较少，大多数面试都是这样吗？ 
From ningdi to Everyone: (7:56 PM)
 1:1要是用了mq， 想象一下2个人加了好友，然后发消息，现场创建topic？ 如果是pre set topic， 那么你有5m的用户，他们每个人都有1000个好友，你要创建5km的topic 
From zepengzhao to Everyone: (7:56 PM)
 500 fanout不算大吧 主要怕millions 比较川普粉丝mllions 
From A to Everyone: (7:56 PM)
 用户信息必须存啊 
From YL to Everyone: (7:56 PM)
 我觉得要看你怎么和面试官交流吧 
From zepengzhao to Everyone: (7:56 PM)
 500 用多个worker就可以了 
From kk to Everyone: (7:56 PM)
 这样topic一多。。 
From sherry的 iPhone to Everyone: (7:56 PM)
 不需要创造500个topic吧 500个user subscribe一个topix不结了 
From 姚剣楠 to Everyone: (7:56 PM)
 1:1要是用了mq， 想象一下2个人加了好友，然后发消息，现场创建topic？ 如果是pre set topic， 那么你有5m的用户，他们每个人都有1000个好友，你要创建5km的topic。 同意 kafka大部分时间在忙着建立和删除topic 
From Lucas Li to Everyone: (7:56 PM)
 一个用户一个topic就可以了吧 
From kk to Everyone: (7:56 PM)
 kafka顶得住？ 我表示怀疑。 
From A to Everyone: (7:56 PM)
 你要不放心就hash一下，想看的时候偷偷看 
From ray to Everyone: (7:57 PM)
 kafka ding bu zhu 
From A to Everyone: (7:57 PM)
 一个用户就是一个topic 
From ray to Everyone: (7:57 PM)
 how many memory it would be for only creating one new topic? 
From ningdi to Everyone: (7:57 PM)
 一个用户一个topic 也不现实。 
From A to Everyone: (7:57 PM)
 topic是tree 结构的，/a/b/c/d/e/牛逼闪闪 
From Jiayue(Hubert) Wu to Everyone: (7:57 PM)
 不需要每个消息存500份吧 ，存一份再push到所有人 
From YL to Everyone: (7:57 PM)
 面试官好像笑出来了 
From zepengzhao to Everyone: (7:57 PM)
 topic多infra解决， 时间到了 
From Lucas Li to Everyone: (7:58 PM)
 想象1000台机器，每台机器10000个topic 
From 姚剣楠 to Everyone: (7:58 PM)
 Web socket 应该很容易解决上面kafka那些问题。。 直接用socket不好吗 
From ningdi to Everyone: (7:58 PM)
 kafka broker已经可以处理这么多topic了？ 
From Jerry to Everyone: (7:58 PM)
 但是用户离线的情况怎么办 
From Lucas Li to Everyone: (7:58 PM)
 http server来不及处理怎么办啊 
From YL to Everyone: (7:58 PM)
 离线通过notification sever解决吧 
From Michael to Everyone: (7:59 PM)
 browser没法用socket 
From Lucas Li to Everyone: (7:59 PM)
 notification server来不及处理怎么办啊 
From Feng Gao to Everyone: (7:59 PM)
 image/video应该是没法放进DB的。需要blob storage 
From xing wang to Everyone: (7:59 PM)
 讲了为啥不用kv吗 
From Richard Tu to Everyone: (7:59 PM)
 不好意思，我可能错过了什么documentDB干嘛的？ 
From bernini to Everyone: (7:59 PM)
 一般都是socket，法request太昂贵了 
From 姚剣楠 to Everyone: (7:59 PM)
 是不是预估一下网络traffic比较好 10m用户同时在线 你发一条消息，会广播给同组的人 最多扩大500倍 其实用户多起来 traffic也是蛮大 
From bernini to Everyone: (7:59 PM)
 kv range search太贵 
From 非洲黑猴子 to Everyone: (7:59 PM)
 附件用对象存储就好，还能压缩转码 
From 姚剣楠 to Everyone: (7:59 PM)
 我们组的服务上周刚被 4gps每秒的攻击 挤爆网络 
From Lucas Li to Everyone: (8:00 PM)
 多媒体发送给S3，消息里面放个链接就可以了 
From ningdi to Everyone: (8:00 PM)
 你们组没有黑名单吗。。 
From Yijie Shen’s iPhone to Everyone: (8:00 PM)
 Attachment 比如image 或者video 是不是用S3比较好啊 
From Enze to Everyone: (8:00 PM)
 mqtt 可以支持大量topics 
From Feng Gao to Everyone: (8:00 PM)
 这个backend就一个service。。。。 
From A to Everyone: (8:00 PM)
 终于有人提mqtt了 我就知道lol 
From ray to Everyone: (8:01 PM)
 how does the message delivered cross the region, like one person send at asia, and another one received at northamerica, is there any replication in this case? 
From A to Everyone: (8:01 PM)
 附件用对象存储就好，还能压缩转码 ，猴子哥，对象存储是什么？ 
From A to Everyone: (8:01 PM)
 object storage吗？ s3？ 
From Lao luo to Everyone: (8:01 PM)
 是group chat的话，说大概要多少个group吗？10 M user 不说明group就多吧 
From 非洲黑猴子 to Everyone: (8:02 PM)
 存文件的，kv，拿着key就能找到云上的文件 
From ningdi to Everyone: (8:02 PM)
 上来就aws全家桶 基本啥也不是瓶颈 啥也不是问题了 哈哈哈 
From Lao luo to Everyone: (8:02 PM)
 Kafka确实有topic 
From Sean Gao to Everyone: (8:03 PM)
 @ray 存到 global NoSQL， reqplicate 到 其region ？ 
From xing wang to Everyone: (8:03 PM)
 上来就aws全家桶 基本啥也不是瓶颈 啥也不是问题了 哈哈哈，，，有取巧之嫌 
From YL to Everyone: (8:03 PM)
 45分钟的话时间已经到了…实际应该只有不到半小时吧 
From Lucas Li to Everyone: (8:03 PM)
 S3设计又是一道面试题 
From ningdi to Everyone: (8:03 PM)
 gfs 
From ray to Everyone: (8:04 PM)
 there should be some sycronize process to ensure the consistency of messages for group chat 
From A to Everyone: (8:04 PM)
 s3好啊，背过了，多谢非洲黑猴子哥 
From Lucas Li to Everyone: (8:04 PM)
 实在不行把多媒体的需求略过 
From Richard Tu to Everyone: (8:04 PM)
 我觉得上面也是我想问的问题。架设你作为candidate，对aws全家桶特别熟，但是从面试官角度来说，是面试官想要的吗 
From A to Everyone: (8:04 PM)
 附件dedup也要做一下 
From bernini to Everyone: (8:04 PM)
 实测所媒体跨区域sync延迟极大 
From Zhengguan Li to Everyone: (8:04 PM)
 receiver不就是MQ consumer? 
From ray to Everyone: (8:04 PM)
 there is tunnel for cross region network 
From A to Everyone: (8:05 PM)
 tunnel是啥 
From ray to Everyone: (8:05 PM)
 it would be supper high speed 
From Lucas Li to Everyone: (8:05 PM)
 In the first test, we set up a Kafka cluster with 5 brokers on different racks. In that cluster, we created 25,000 topics, each with a single partition and 2 replicas, for a total of 50,000 partitions. So, each broker has 10,000 partitions. We then measured the time to do a controlled shutdown of a broker. The results are shown in the table below. https://blogs.apache.org/kafka/entry/apache-kafka-supports-more-partitions 
From Becky to Everyone: (8:05 PM)
 Receiver service 是单机流吗 
From A to Everyone: (8:06 PM)
 那是个aws asg，里面有1万个vm 
From ray to Everyone: (8:06 PM)
 we are using the chat function of zoom LOL 
From v to Everyone: (8:06 PM)
 lol 
From ray to Everyone: (8:06 PM)
 and we can deliver real time video audio globally 
From A to Everyone: (8:06 PM)
 zoom牛逼 
From zepengzhao to Everyone: (8:06 PM)
 message -> using conversation_id to get chat participants -> get participant’s web socket sessions -> push message to participant’s machine -> ws -> user device 
From Lucas Li to Everyone: (8:07 PM)
 ZOOM同时在线的用户有限 
From ray to Everyone: (8:08 PM)
 there is roles and permissions setup for the chat especially for the group 
From A to Everyone: (8:08 PM)
 IAM 答主没说 
From ray to Everyone: (8:08 PM)
 the admin to create group, delete group 
From A to Everyone: (8:08 PM)
 要了那么多requirement，来不及说了 
From Jerry to Everyone: (8:09 PM)
 consume那一步都做什么操作 
From lining to Everyone: (8:09 PM)
 requirement数量要自己控制吗？ 
From Zhengguan Li to Everyone: (8:09 PM)
 请问一下 面试的时候也有条件画图嘛 
From ray to Everyone: (8:09 PM)
 if it is message queue,  it is pulling message from the topic 
From YL to Everyone: (8:09 PM)
 画的 
From Jerry to Everyone: (8:09 PM)
 group的聊天记录接受者要用什么DB存取 
From ray to Everyone: (8:10 PM)
 so it is quite a drawback of pulling, since it is hard to balance the produce and consume 
From Sean Gao to Everyone: (8:10 PM)
 lsm 应该是 
From Lucas Li to Everyone: (8:10 PM)
 pub/sub是不是可以给服务发消息啊 
From ray to Everyone: (8:10 PM)
 there is push style 
From YL to Everyone: (8:10 PM)
 我感觉还是应该用wide column DB 
From Jerry to Everyone: (8:10 PM)
 message queue会有容量上限吗? 到了上限这么办 
From aaa to Everyone: (8:11 PM)
 时间差不多到了 
From ray to Everyone: (8:11 PM)
 If everyone likes to use aws, we can have interview like how to design aws servce like s3, documentDB 
From YL to Everyone: (8:11 PM)
 时间都超了 
From Andrew Hou to Everyone: (8:11 PM)
 我觉得这个是一个设计流程图 而不是设计系统 
From Ken to Everyone: (8:11 PM)
 Started 7:13 
From shawnzech to Everyone: (8:11 PM)
 .... 
From Zhengguan Li to Everyone: (8:12 PM)
 有上限 queue depth(# of messages), 或者size(e.g. 5M)啥的 
From Jiabei Luo to Everyone: (8:12 PM)
 Why not use noSQL DB? 
From ray to Everyone: (8:12 PM)
 good point 
From Sean Gao to Everyone: (8:12 PM)
 一定是吧，写多，少edit，时间排序 
From ray to Everyone: (8:12 PM)
 and there is also blacklist friend functionality of wechat, LOL 
From Richard Tu to Everyone: (8:13 PM)
 1小时正好 
From lining to Everyone: (8:13 PM)
 到点了 
From Jerry to Everyone: (8:13 PM)
 这好像还没说message存储是怎么partition的吧? 
From Peng Wang to Everyone: (8:14 PM)
 meta data是什么？ 
From YL to Everyone: (8:14 PM)
 除了by UserId还有什么partition的方法？ 
From A to Everyone: (8:14 PM)
 就是facebook的data 
From ray to Everyone: (8:14 PM)
 metadata is like some control plane data, like configuration of the message queue 
From lining to Everyone: (8:15 PM)
 LOL 
From ray to Everyone: (8:15 PM)
 facebook has no face anymore, lol 
From 非洲黑猴子 to Everyone: (8:15 PM)
 为了元宇宙，脸都不要了 
From A to Everyone: (8:15 PM)
 给大包也得去啊 
From AAA to Everyone: (8:16 PM)
 😂 
From A to Everyone: (8:16 PM)
 求原宇宙公司内推 
From Pencil to Everyone: (8:16 PM)
 That’s a good question 
From AAA to Everyone: (8:16 PM)
 求抱大腿 
From bernini to Everyone: (8:16 PM)
 上次E5，fb挂我sd，加面过了都不给去， 结果几个月后大放水 
From A to Everyone: (8:16 PM)
 e5大佬 wow 
From AAA to Everyone: (8:17 PM)
 大佬什么背景 
From Jiabei Luo to Everyone: (8:17 PM)
 放水怎么说？ 
From A to Everyone: (8:17 PM)
 放水都不要我😭  要求降下来了  现在还放水吗  在吧？  E5 要求变E4?  我疫情刚爆发时候面的  312  要求变E4，被拒绝，找你就是要在找E5  132  132  132  132  132  132  321  132  132  132  132  132  132  132  132  132  132  132  123  123  How much time should we spend in gathering requirements?  123  123  123  123  123  123  213 
From Peijin to Everyone: (8:19 PM)
 213 
From 非洲黑猴子 to Everyone: (8:19 PM)
 213 
From Kevin to Everyone: (8:19 PM)
 123 
From 2002079 cici to Everyone: (8:19 PM)
 213 
From lining to Everyone: (8:19 PM)
 213 
From A to Everyone: (8:19 PM)
 1 
From johnc to Everyone: (8:19 PM)
 213 
From Patrick to Everyone: (8:19 PM)
 21 
From Spin to Everyone: (8:20 PM)
 21 
From ningdi to Everyone: (8:20 PM)
 2 
From Qianwen Huang to Everyone: (8:20 PM)
 21 
From ray to Everyone: (8:20 PM)
 hard skill 213 
From yinghuaguan to Everyone: (8:20 PM)
 2 
From ningdi to Everyone: (8:21 PM)
 拿需求那步真的秀。 学到了 
From A to Everyone: (8:21 PM)
 面试官应该scope down到chat service only 
From lining to Everyone: (8:22 PM)
 可能对wechat太熟了 
From A to Everyone: (8:22 PM)
 答主问了太多requirement clarification，面试官需要控制一下 
From Jian Zhu to Everyone: (8:22 PM)
 这些应该是面试者自己控制啊 
From bernini to Everyone: (8:22 PM)
 需求花了太多时间了 
From ningdi to Everyone: (8:22 PM)
 可能要我 我就focus在消息送达 上面了。。 其他的根本不care 😂 
From 非洲黑猴子 to Everyone: (8:22 PM)
 面试官控场一下 
From AAA to Everyone: (8:23 PM)
 抱歉，发错了 
From A to Everyone: (8:23 PM)
 需要面试官控场 
From AAA to Everyone: (8:23 PM)
 有回放么  其实我觉得真实面试情况下，还是不能依靠面试官。如果面试官本身就不postive，candidate还是得尽量自己控制时间  面试官，自信点。是！  如果自己不问需求，会不会失分  我觉得ballpark 计算dau之类的感觉没啥用 面试时能跳过吗？  我面一次amazon 真的2场遇见风格完全不一样的面试官。。一个小白哥贼积极，一个亚裔就根本不鸟我，全是我在bb  要是我以后当面试官 我觉得ballpark计算可以直接跳过了  大多数面试官都这样，把舞台交给应试者  ballpark计算最没用 直接跳过 浪费时间  我也觉得那些计算没啥用  可以直接问面试官 我不说会不会扣分吗？  貌似完全没有api design 😂  除非对硬件的capability有数。要不然就是白费劲  如果后面设计db的时候要用到，可以再提一下  对哦 
From Zhengguan Li to Everyone: (8:26 PM)
 protocol是Http long polling, websocket啥的嘛 
From Jiabei Luo to Everyone: (8:26 PM)
 没有api design只有object design hh 
From Michael to Everyone: (8:26 PM)
 应该是receiver端 real time chat 
From Michael Qiu to Everyone: (8:26 PM)
 chat needs bi-directional communication 
From bernini to Everyone: (8:27 PM)
 websocket不用一直握手，开销小 
From 非洲黑猴子 to Everyone: (8:27 PM)
 底层netty可以考虑 
From Sean Gao to Everyone: (8:28 PM)
 java 估计性能还是差了点， cpp or c or erlang ？ 
From A to Everyone: (8:28 PM)
 netty是啥 猴子哥 
From bernini to Everyone: (8:28 PM)
 nosql？ 
From v to Everyone: (8:28 PM)
 面试官讲一下kafka在这里用可以么？ 这么多的topic可以支持么 
From bernini to Everyone: (8:28 PM)
 那重新登录的时候拉history会不会很慢？ 
From Michael to Everyone: (8:28 PM)
 是那个java 的 nio 模型framework ? 
From 非洲黑猴子 to Everyone: (8:29 PM)
 底层用来做RPC层的框架，spark、flink、dubbo都在用 
From Sean Gao to Everyone: (8:29 PM)
 是的 nio framework 
From 非洲黑猴子 to Everyone: (8:29 PM)
 Kafka高冷，人家自己写的网络层 
From A to Everyone: (8:29 PM)
 rpc的protocol 很轻量吗 ？ 
From zepengzhao to Everyone: (8:30 PM)
 group chat fanout 到底在哪里好 可以讨论下吗 
From Jiabei Luo to Everyone: (8:30 PM)
 history 不能in memory cache 一段 吗 
From zepengzhao to Everyone: (8:31 PM)
 是不是有五十个人的chat room， 我们把一条message fanout成50条，然后每条conv_id一样，然后reciepient不一样呢 
From A to Everyone: (8:31 PM)
 history为什么要放在cache 
From 非洲黑猴子 to Everyone: (8:31 PM)
 可以，在client端都可以缓存historymessage 
From ningdi to Everyone: (8:31 PM)
 Fanout说白了就是 有n个消失 for each 写给m个人 消息* 
From A to Everyone: (8:31 PM)
 你的chat history都是存在本地的吧 
From Jiabei Luo to Everyone: (8:31 PM)
 我是说clientside localstorage 这种 对 
From Zhengguan Li to Everyone: (8:31 PM)
 protocol和kafka这两个点怎么关联起来? 
From zepengzhao to Everyone: (8:32 PM)
 chat history要requirement的时候讲清楚 
From Lao luo to Everyone: (8:32 PM)
 有谁解决过动态创建topic太多的问题了吗？我们现在就碰到类似的问题 
From xinz to Everyone: (8:33 PM)
 内部service 之间的通信是用RPC 吗？ 
From A to Everyone: (8:33 PM)
 非洲黑猴子 大神，大家挺好 大家听好  请问 token 是存在哪里的  token存在本地  没错  所以还是用的topic吗？ 每个group chat一个topic？  SQL too expensive  我觉得要pub/sub + message handler + message queue比较robust  我还是持反对意见  what? SQL?  user必须用sql啊  Message 用sql 写太慢了吧  message queue只有一个subscriber  我第一次听说NoSQL 存uder table  message可以存noSQL  message用sql不好吧  user group可以存SQL  分开呀  message写sql，能慢到哪去呢，我不觉得会特别慢  Message 用nonsql  db又不贵，多来几个db nosql用bigtable 稳得很，widecolume  message 用 NoSQL, 图片用s3， 其它用sql  sql主要是scalability不太好, message太多，分片存NoSQL挺好的  对 存历史消息 可以用非同期处理  why the user table has to use sql? sorry I didn't get it  Is an object store a database?  Cuz user -group needs relational association  There is no point for SQL unless you need to join multiple tables  当你需要查看你的微信群里都有谁的时候，叫啥，长得好看不好看，你需要sql  我不同意任何说SQL分片或者scalability的意见，因为在现在，任何nosql能做的分片/scalebility，sql都能做  Every time send/receive a message need to read/write a row from the SQL database  @Richard 那 写的效率呢 ？？  为什么要从db读？ 只是写而已。  msg要放在append only的nosql db里，user，auth这些必然sql。  我一般都用一个例子，就是dynamoDB用的是mySQL作为单点storage node  是， uber内部的也是 MySQL 改的，存实时地理数据  Normalization  不过这样的话，那区别是什么呢 ？  user-group table needs secondary key? 
From ray to Everyone: (8:40 PM)
 dynamodb use the mysql database engine 
From Enze to Everyone: (8:41 PM)
 数据量大时怎么join？ 
From A to Everyone: (8:41 PM)
 data normalization 是有好处的 
From Jackson to Everyone: (8:41 PM)
 firebase的firestore，太少人用了，说实话firebase虽然是NoSQL，但是他能做到sql的对应关系。用firebase做例子不好 
From bernini to Everyone: (8:41 PM)
 会溢出？ 
From YL to Everyone: (8:41 PM)
 那这样讲的话所有的都可以用sql+good partition吗 
From Yue Liang to Everyone: (8:42 PM)
 ^我也这么理解。 Tao 后台也是用mysql 
From Richard Tu to Everyone: (8:43 PM)
 对，虽然现在engine改成myrocket了 
From ningdi to Everyone: (8:43 PM)
 也就是说 如果没有partition limit是10k 
From A to Everyone: (8:43 PM)
 什么是topic partition？ 
From ningdi to Everyone: (8:43 PM)
 那么最多也就10k个topic 
From Lucas Li to Everyone: (8:43 PM)
 分片联合查询？ 
From Lucas Li to Everyone: (8:44 PM)
 In the first test, we set up a Kafka cluster with 5 brokers on different racks. In that cluster, we created 25,000 topics, each with a single partition and 2 replicas, for a total of 50,000 partitions. So, each broker has 10,000 partitions. We then measured the time to do a controlled shutdown of a broker. The results are shown in the table below.

https://blogs.apache.org/kafka/entry/apache-kafka-supports-more-partitions 
From Lao luo to Everyone: (8:44 PM)
 Kafka topic太多性能就下来了 
From ningdi to Everyone: (8:44 PM)
 3秒的delay也太大了 
From Lao luo to Everyone: (8:44 PM)
 和partion有关 
From bernini to Everyone: (8:46 PM)
 我们都拿firebase当cache用 

## Ticket master
From Jun to Everyone: (7:17 PM)
 这个画图是什么网站？ 
From Laoluo to Everyone: (7:17 PM)
 肯定是秒杀相关 但是是不是象12306一样有不同的站 
From Ken to Everyone: (7:17 PM)
 We have notes for previous meeting.  Please scan the QR code on top of the notes doc in this doc: https://docs.google.com/document/d/11hsGVxwAzfBPR6coFB-RiXmokUgqKbnFQ1R7urE6m_s/edit# to join our WeChat group 
From Laoluo to Everyone: (7:17 PM)
 有不同的站会复杂不少 
From Skit to Everyone: (7:19 PM)
 什么是p0, p3 
From Qiang Lu to Everyone: (7:20 PM)
 is it priority? 
From Christie to Everyone: (7:20 PM)
 Priority 0? 
From 非洲黑猴子 to Everyone: (7:20 PM)
 需求是不是最急需的 
From Eric Che to Everyone: (7:20 PM)
 这题的考点应该是类秒杀设计，怎么保证在大并发下能保证票不会超卖，并且能handle大并发量。 
From 非洲黑猴子 to Everyone: (7:20 PM)
 P0基本就是MVP了 
From Skit to Everyone: (7:23 PM)
 callback function? 
From Tekken to Everyone: (7:23 PM)
 直接开始接口设计了吗 
From Alan to Everyone: (7:23 PM)
 can customer send out and order and payment info, or customer can make an order first and then pay within an hour and payment info, assume payment system 
From Laoluo to Everyone: (7:25 PM)
 比较好奇最近几次好象都是男的出题，女的做题 
From Erwin to Everyone: (7:25 PM)
 is non functional requirements skipped for this one? 
From Sun Anna to Everyone: (7:25 PM)
 +1 where is the non functional requires? 
From Jackie G to Everyone: (7:27 PM)
 Sorry, What is “item”? 
From Skit to Everyone: (7:28 PM)
 i think she meant ticket "name" or description 
From YL to Everyone: (7:28 PM)
 每一次order都要更新所有的ticket吗？ 
From Yanbin Li to Everyone: (7:28 PM)
 请问这个ticket系统是卖什么票的，这个聊了吗 
From Richard Tu to Everyone: (7:29 PM)
 同问，这个不用考虑什么座位号之类的吗 
From bill.wang to Everyone: (7:29 PM)
 TicketMaster--mostly they are concert tickets 
From YL to Everyone: (7:29 PM)
 所有ticket都是一样的 
From Erwin to Everyone: (7:29 PM)
 seat num should be needed in tickermaster so that user could select the seat they want 
From Richard Tu to Everyone: (7:30 PM)
 从我角度，应该有个event之类的 event table mapping multiple tickets 
From anna to Everyone: (7:30 PM)
 求问个题外话，这是什么画图软件？ 感觉好好用，拖拉拽超级方便 
From YL to Everyone: (7:30 PM)
 感觉可以考虑成音乐节的门票之类的，都站票 
From Tekken to Everyone: (7:30 PM)
 需求分析后 直接跳到接口设计 这时候面试官是不是控场一下更好 资源预估 Data Flow, Service讨论 系统设计图 这些都是要先于接口设计做吧 
From Richard Tu to Everyone: (7:31 PM)
 可以是可以，但是requirement有说过吗，是我miss了什么需求吗 针对 > 感觉可以考虑成音乐节的门票之类的，都站票 
From YL to Everyone: (7:32 PM)
 所有ticket没有分级，都是一样的 
From iPad to Everyone: (7:32 PM)
 Requirement说all tickets are the same，是不是就是说的是没有座位的分别啊 
From Erwin to Everyone: (7:32 PM)
 I bought tickets from Ticketmaster and there are different types of tickets for one event 
From Yanbin Li to Everyone: (7:32 PM)
 需求分析后做API design没啥问题吧，我理解首先通过API design明确你的service提供什么服务，后面才好设计为了提供这些服务怎么设计数据模型和系统架构 
From Jilong Chen to Everyone: (7:32 PM)
 User table should include credit card or other payment methods 
From Christie to Everyone: (7:32 PM)
 Ticket service, order service 分别在哪呀 
From Yufei Qian to Everyone: (7:33 PM)
 QPS, TPS分析了吗 
From 非洲黑猴子 to Everyone: (7:33 PM)
 应该是后面的数据库表不一样，一个是ticket另一个是order 
From Christie to Everyone: (7:34 PM)
 謝謝！ 
From Yufei Qian to Everyone: (7:34 PM)
 这么多User直接hit relational会击穿吧 
From YL to Everyone: (7:34 PM)
 ACID优先 
From Erwin to Everyone: (7:35 PM)
 where do we store payment related info? 
From 非洲黑猴子 to Everyone: (7:35 PM)
 看怎么设计了，设计好了不会击穿，多拿流量都能给你扛下来 
From Yufei Qian to Everyone: (7:35 PM)
 目前的设计没有cache 
From renyuming to Everyone: (7:37 PM)
 cache一个需要考虑的是时效性 
From YL to Everyone: (7:38 PM)
 这里加cache的话应该存啥， available ticket？ 
From ds awsome to Everyone: (7:38 PM)
 10m user ～ 100 qps，这样一台server就够了吧 
From Richard Tu to Everyone: (7:38 PM)
 所以这个设计，是更偏向秒杀？ 10m qps? 
From ds awsome to Everyone: (7:38 PM)
 是不是根本不用分布式系统啊 
From 非洲黑猴子 to Everyone: (7:38 PM)
 先看面试官咋说 
From Shihao Zhong to Everyone: (7:38 PM)
 10m user应该是都在一个时间左右买票，所以可能qps要10m 吧 
From H.B. to Everyone: (7:38 PM)
 一定要保证用户看到的ticket 数量是最新的？ 
From Erwin to Everyone: (7:38 PM)
 have we discussed peak qps before? 
From Shihao Zhong to Everyone: (7:38 PM)
 10m 不是平均的啊， 
From 非洲黑猴子 to Everyone: (7:39 PM)
 用户一定看到最新的，但是下单的时候一定是检查最新的 
From leo zhang to Everyone: (7:39 PM)
 10m user如何推出 100 qps的? 这是秒杀10m 不是dau 
From 非洲黑猴子 to Everyone: (7:39 PM)
 用户不一定看到最新的，但是下单的时候一定是检查最新的 
From Laoluo to Everyone: (7:40 PM)
 SQL应该可以的，Cache或读写分离 
From Erwin to Everyone: (7:40 PM)
 also each ticket should have a uuid? so that we could refer to the payment/user related info? 
From v to Everyone: (7:40 PM)
 就算用了redis 还是有100k的qps？ 
From 非洲黑猴子 to Everyone: (7:40 PM)
 Reds支持秒级10万并发 
From v to Everyone: (7:41 PM)
 就算用了redis 还是会有100k的qps到mysql 因为有100k的票 
From ds awsome to Everyone: (7:41 PM)
 要是10m秒杀，那至少要10个 redis？ 
From Shihao Zhong to Everyone: (7:41 PM)
 你可以一次发n张票到redis啊 redis不是有原子操作么 
From iPad to Everyone: (7:41 PM)
 一开始都没想到这是个flash sale的题目 
From renyuming to Everyone: (7:41 PM)
 redis有cluster也是可以scale up的 到mysql的100K需要shard ticket了 
From 非洲黑猴子 to Everyone: (7:42 PM)
 看了不一定买，写请求怎么处理且看面试者怎么设计 
From renyuming to Everyone: (7:42 PM)
 每一张ticket都应该是一个record 
From ds awsome to Everyone: (7:42 PM)
 那这个scale mysql撑不住吧 
From iPad to Everyone: (7:42 PM)
 现在的设计怎么防止超卖呢？ 
From renyuming to Everyone: (7:42 PM)
 redis在前面挡着 每张ticket也会有write lock吧 
From 非洲黑猴子 to Everyone: (7:42 PM)
 锁定库存 
From Richard Tu to Everyone: (7:43 PM)
 至少得加个mq，异步吧 
From v to Everyone: (7:43 PM)
 Total quantity可以分开维护么。。这样就不用每次query 数据库来算count了？ 
From Richard Tu to Everyone: (7:43 PM)
 削峰限流 
From Jerry to Everyone: (7:43 PM)
 要到最后成功支付成功或者取消才算确认吧 
From Alan to Everyone: (7:43 PM)
 双十一抢购 
From renyuming to Everyone: (7:44 PM)
 支付和book感觉可以分成两部分，book之后有一定时间去pay 
From Christie to Everyone: (7:44 PM)
 是不是可以先放 MQ ，payment 成功才更新 available_quantity? 
From renyuming to Everyone: (7:44 PM)
 因为pay一般做不到ms级 
From ds awsome to Everyone: (7:44 PM)
 Redis的性能是每秒10万 还是1M啊？ 
From 非洲黑猴子 to Everyone: (7:44 PM)
 阿里自己二开了MySQL，增加了写请求排队功能 
From renyuming to Everyone: (7:44 PM)
 available_quantity 我感觉可以直接用redis + lua，只跟book相关，不跟pay相关 
From v to Everyone: (7:45 PM)
 这个设计会有thundering herd吧 
From renyuming to Everyone: (7:45 PM)
 也只保存quantity 不是加了mq？ 
From Jerry to Everyone: (7:46 PM)
 pay的过程中要锁定这部分库存吧但是要加个time out 
From Yufei Qian to Everyone: (7:46 PM)
 thundering herd无法避免，traffic pattern就是这样，需要设计去handle 
From Shihao Zhong to Everyone: (7:47 PM)
 为什么会有thundering herd，没有理解 
From Laoluo to Everyone: (7:47 PM)
 掉坑里了 
From YL to Everyone: (7:47 PM)
 1s就抢没了 
From Christie to Everyone: (7:47 PM)
 10 sec 東西賣光了 
From leo zhang to Everyone: (7:47 PM)
 抢票不刷新就没了啊 
From Alan to Everyone: (7:47 PM)
 request进来是不是要先做order request 然后排队 
From Alan to Everyone: (7:48 PM)
 如果拿到ticket，那就create order 
From 非洲黑猴子 to Everyone: (7:48 PM)
 读的时候读不到最新的没关系，pay的时候不出错就好 
From Yufei Qian to Everyone: (7:48 PM)
 那样用户体验会比较差 
From Jerry to Everyone: (7:49 PM)
 限购的需求是不是还没加 
From Yufei Qian to Everyone: (7:49 PM)
 是的，限购需求没有讨论 
From Alan to Everyone: (7:49 PM)
 每个ticket system 有个cache， ticket先产生，买个ticket 系统分配一定数量ticket在 cache 从cache拿ticket需要synchronous 如果需求不多，应该很快，需求多就要排队，因为synchrounous 
From tomdi to Everyone: (7:51 PM)
 payment 和 ticket count update 做一个 transaction 事务， cache write upate只在每个transcation commit之后 
From Laoluo to Everyone: (7:51 PM)
 cache里保证所有的ticket不断地减少，不能不变 
From Alan to Everyone: (7:51 PM)
 ticket是事先产生的啊 肯定不能做ticket count啊，笑死人 
From Erwin to Everyone: (7:53 PM)
 如果这里ticket service的一个server挂了，有没有什么办法保证这个service对应的tickets可以被其他server利用？ 
From Alan to Everyone: (7:53 PM)
 data race是难点 
From Skit to Everyone: (7:53 PM)
 ticket先create row,然后book把 
From tomdi to Everyone: (7:53 PM)
 cache只读，db transaction update后再 update cache, cache读data可以有滞后 
From Skit to Everyone: (7:54 PM)
 这样不需要count 
From v to Everyone: (7:54 PM)
 用redis的话 如果不写到disk 会有data loss的风险。。。如果写到disk的话 write throughput会很不好吧？ 
From Ken to Everyone: (7:54 PM)
 5 more minutes 
From H.B. to Everyone: (7:54 PM)
 他好像没说啥时候create session ? 
From Alan to Everyone: (7:54 PM)
 有，每个ticket，保存分配到哪个server信息 
From H.B. to Everyone: (7:54 PM)
 session 不是一个小时吗 
From Tekken to Everyone: (7:54 PM)
 这是道老题目了 如果事前稍微准备下 油管上能找到很多很成熟的设计方案 
From Alan to Everyone: (7:54 PM)
 如果那个server crash， 他的那些没被order的ticket从新被放回去 
From leo zhang to Everyone: (7:55 PM)
 可能事先不知道题目 
From YL to Everyone: (7:55 PM)
 知道吧 
From Laoluo to Everyone: (7:56 PM)
 cache自行减少，不用更新数据库，蛤payment的service来更新ticket 的数量就行了 payment量少很多，cache来读数据做同步 
From H.B. to Everyone: (7:56 PM)
 为了读的块 
From Kasey to Everyone: (7:57 PM)
 为啥不能直接用redis？ 
From 非洲黑猴子 to Everyone: (7:57 PM)
 好几张表要join 
From H.B. to Everyone: (7:57 PM)
 卡住了 
From Christie to Everyone: (7:57 PM)
 所以不用 ticket mysql 的表了？ 
From Kasey to Everyone: (7:57 PM)
 就把所有ticket 存redis里面不行么 
From Laoluo to Everyone: (7:58 PM)
 关键点没有讨论，特别是怎样削峰 
From Hao to Everyone: (7:58 PM)
 问个问题，应该什么时候减库存呢？是pay成功才减库存？ 
From Kasey to Everyone: (7:58 PM)
 肯定吧 
From Laoluo to Everyone: (7:59 PM)
 +1 
From H.B. to Everyone: (7:59 PM)
 肯定pay 成功后 
From Ender Li to Everyone: (7:59 PM)
 Pay成功才减库存不会超卖吗 
From wantong jiang to Everyone: (7:59 PM)
 这怎么避免超卖呢？ 
From leo zhang to Everyone: (7:59 PM)
 不会啊 
From tomdi to Everyone: (7:59 PM)
 pay成功和减库存是一个  transaction 
From Kasey to Everyone: (7:59 PM)
 pay成功和失败是两个情况 
From H.B. to Everyone: (7:59 PM)
 你说的库存是mysql里的？ 
From Zhengguan Li to Everyone: (7:59 PM)
 pay成功前也可以呃减啊 事先锁定嘛 
From H.B. to Everyone: (7:59 PM)
 还是他说redis里的 
From leo zhang to Everyone: (7:59 PM)
 pay的时候检查库存, 放一个tracnsaction 
From renyuming to Everyone: (8:00 PM)
 应该是book就lock库存，之后pay失败了就恢复，pay成功了就减掉了 
From leo zhang to Everyone: (8:00 PM)
 order成功没付款还有有风险不能proceeed 
From Kasey to Everyone: (8:00 PM)
 嗯 
From renyuming to Everyone: (8:00 PM)
 transaction感觉很慢？ 尤其带3 party的api的？ 
From lily liu to Everyone: (8:01 PM)
 order service的时候要减db库存并更新cache了吧，然后payment 成功的时候再调整一次 
From H.B. to Everyone: (8:01 PM)
 312 
From Zhengguan Li to Everyone: (8:01 PM)
 321 
From Zidong to Everyone: (8:01 PM)
 321 
From 非洲黑猴子 to Everyone: (8:01 PM)
 跟钱相关的不得不transaction 
From Spin to Everyone: (8:01 PM)
 132 
From leo zhang to Everyone: (8:01 PM)
 付款 lantency不是最紧急的需求吧? 
From Yufei Qian to Everyone: (8:01 PM)
 132 
From Simon Z to Everyone: (8:01 PM)
 321 
From david to Everyone: (8:01 PM)
 132 
From Christie to Everyone: (8:01 PM)
 312 
From Jackie G to Everyone: (8:01 PM)
 132 
From leo zhang to Everyone: (8:01 PM)
 accuracy更重要 
From Kj to Everyone: (8:01 PM)
 312 
From x to Everyone: (8:01 PM)
 312 
From xinz to Everyone: (8:02 PM)
 132 
From 非洲黑猴子 to Everyone: (8:02 PM)
 312 
From johnc to Everyone: (8:02 PM)
 312 
From Xiaoqin Fu to Everyone: (8:02 PM)
 312 
From Julie Long to Everyone: (8:02 PM)
 312 
From anna to Everyone: (8:02 PM)
 132 
From YL to Everyone: (8:02 PM)
 312 
From Qiang Lu to Everyone: (8:02 PM)
 312 
From HW to Everyone: (8:02 PM)
 321 
From Peiwen Tian to Everyone: (8:02 PM)
 312 
From Xuexin Chen to Everyone: (8:02 PM)
 312 
From Richard Cao to Everyone: (8:02 PM)
 312 
From TBL to Everyone: (8:02 PM)
 132 
From christie Yu to Everyone: (8:02 PM)
 312 
From lily liu to Everyone: (8:02 PM)
 132 
From Ken to Everyone: (8:03 PM)
 Hard skill 
From Zhengguan Li to Everyone: (8:03 PM)
 312 
From YL to Everyone: (8:03 PM)
 21 
From christie Yu to Everyone: (8:03 PM)
 213 
From H.B. to Everyone: (8:03 PM)
 21 
From Jackie G to Everyone: (8:03 PM)
 213 
From lining to Everyone: (8:03 PM)
 21 
From Spin to Everyone: (8:03 PM)
 21 
From Mark Liu to Everyone: (8:03 PM)
 213 
From johnc to Everyone: (8:03 PM)
 21 
From Christie to Everyone: (8:03 PM)
 21 
From xinz to Everyone: (8:03 PM)
 21 
From Xiaoqin Fu to Everyone: (8:03 PM)
 21 
From david to Everyone: (8:03 PM)
 21 
From HW to Everyone: (8:03 PM)
 21 
From Shihao Zhong to Everyone: (8:04 PM)
 这个3还是没有了解是什么 
From TBL to Everyone: (8:04 PM)
 21 
From Charlie to Everyone: (8:04 PM)
 只发数字顺序看不出那个好坏程度，还是每个指标打分更合适 
From H.B. to Everyone: (8:05 PM)
 嗯嗯我也觉得 每个都给1-5打分 
From leo zhang to Everyone: (8:07 PM)
 +1. 
From H.B. to Everyone: (8:08 PM)
 考官？ 
From Kasey to Everyone: (8:08 PM)
 考官哈哈哈 
From lining to Everyone: (8:08 PM)
 😀 
From Zhengguan Li to Everyone: (8:09 PM)
 面试者: 考官竟是我自己..哈哈 
From Jackie G to Everyone: (8:14 PM)
 弱问一下： 如果所有票都一样，为什么还要一张ticket一行呢？直接一行ticket和count不行吗？ 
From x to Everyone: (8:14 PM)
 对，其实面试者说的意思就是只需要一行 
From Peijin Sun to Everyone: (8:15 PM)
 Ticket 是不是其实是event 
From Richard Tu to Everyone: (8:15 PM)
 他这个表就是event 稍微有点儿confusing 
From lining to Everyone: (8:17 PM)
 对 
From Zidong to Everyone: (8:18 PM)
 5000个是可以refilll吗 
From leo zhang to Everyone: (8:19 PM)
 限流没问题啊 
From Zidong to Everyone: (8:19 PM)
 like 一个buket？ 
From iPad to Everyone: (8:19 PM)
 P0: buy ticket, cap = 2 tickets / user, all tickets the same 
From leo zhang to Everyone: (8:19 PM)
 多放点到后面就是 但是限流可能是需要的，因为商品只有这么多 
From Lu to Everyone: (8:20 PM)
 想问下大家知道newSQL吗 听说了这个concept，好像又可以ACID，又可以horizontally scale 
From leo zhang to Everyone: (8:20 PM)
 放10M的流量到后面去没有意义 
From Shihao Zhong to Everyone: (8:20 PM)
 面试用newsql好么。 
From Lu to Everyone: (8:21 PM)
 没试过 😄 
From Jerry to Everyone: (8:21 PM)
 限流的话就是假设每个request只能买一张票 
From Brave to Everyone: (8:21 PM)
 搞个message queue存requests，然后异步处理，异步处理可以merge requests（比如同一类的票可以合并）避免访问太多次数据库 
From Jerry to Everyone: (8:21 PM)
 多张票要多个requests吗 
From Richard Tu to Everyone: (8:21 PM)
 用呗，别说什么newSQL，提具体的db名字 
From Lu to Everyone: (8:21 PM)
 Google Spanner？ 
From leo zhang to Everyone: (8:22 PM)
 10 M限制成 120k的流量, 这就是huge win 
From Jerry to Everyone: (8:22 PM)
 那面试者的POST url就不能有number_items了吧 
From Shihao Zhong to Everyone: (8:22 PM)
 贵啊 
From Jackie G to Everyone: (8:23 PM)
 Redis 的replication支持strong consistency吗？还是eventual consistency？ 
From Jerry to Everyone: (8:23 PM)
 Redis 有AOF log恢复 不过这题redis好像不需要恢复 数据库重读更新就可以吧 
From Lu to Everyone: (8:25 PM)
 同意 直接cache在API GATEWAY… 
From leo zhang to Everyone: (8:27 PM)
 写120K /hour 只要前面filter了, 后面写不是问题 
From Zhengguan Li to Everyone: (8:30 PM)
 redis可以设置一个timeout? 
From leo zhang to Everyone: (8:30 PM)
 MQ有 exact-once sementic 跟consumer配合 
From Ender Li to Everyone: (8:32 PM)
 那对redis的-1操作必须要锁redis是吗？那每次只有一个人可以update redis，这个不会成为性能瓶颈吗 
From Shihao Zhong to Everyone: (8:32 PM)
 redis有cas 原子操作 不会成为瓶颈这里 
From Sean Gao to Everyone: (8:33 PM)
 CAS 能包括 send to Kafka 么 ？ 
From Sean Gao to Everyone: (8:33 PM)
 或者 CAS 包括 persistence 步骤 ？ 
From Brave to Everyone: (8:35 PM)
 春节买火车票就是在那排队 
From Ender Li to Everyone: (8:36 PM)
 CAS不就是说大量并发去更新redis只有一个会成功，其他都需要重试吗？因为别的old value都不对，更新会失败，是吗？ 
From Sean Gao to Everyone: (8:36 PM)
 @ender  redis全内存操作， 性能损失不大 。 也不是完全lock，是 CAS。 
From hobite to Everyone: (8:36 PM)
 输入完信用卡信息，商家完成与银行间的认证，用户点submit 的瞬间，update 
From Mark Liu to Everyone: (8:37 PM)
 不对吧，Payment按照12306会给你20分钟左右的操作 Payment在20分钟内不成功，才会失败吧 
From hobite to Everyone: (8:38 PM)
 comit redis， 同时send payment and clean up task to queue. 这也涉及到如果放到queue里的期望，就是我们expect 送到queue里的task会99.9%的可能性成功，除非系统崩溃。 
From Sean Gao to Everyone: (8:39 PM)
 “comit redis， 同时send payment and clean up task to queue.” -- 这个是能全部放入 transacation么 ？ 
From hobite to Everyone: (8:40 PM)
 另外一个solution, 是用户在query的时候锁住一个seat，submit的时候update或者release lock 另外一个用户在lock的期间默认这个seat不available 
From Jerry to Everyone: (8:43 PM)
 redis的数据定期去db里同步更新可以吗 

## Calendar
 https://docs.google.com/document/d/1Zod6Cz0-KGJ-BDeZJoqEFrMQ4sdsSiJ-qT8Jkegwd-I/edit# Meeting notes: https://docs.google.com/document/d/1Zod6Cz0-KGJ-BDeZJoqEFrMQ4sdsSiJ-qT8Jkegwd-I/edit# For friends who just joined zoom: Meeting notes: https://docs.google.com/document/d/1Zod6Cz0-KGJ-BDeZJoqEFrMQ4sdsSiJ-qT8Jkegwd-I/edit#  For friends who just joined zoom: Meeting notes: https://docs.google.com/document/d/1Zod6Cz0-KGJ-BDeZJoqEFrMQ4sdsSiJ-qT8Jkegwd-I/edit#  For friends who just joined zoom:  Interview will take 45 minutes: 6:15-7pm PST Meeting notes: https://docs.google.com/document/d/1Zod6Cz0-KGJ-BDeZJoqEFrMQ4sdsSiJ-qT8Jkegwd-I/edit#  已经30min了  For friends who just joined zoom:  Interview will take 45 minutes: 6:15-7pm PST Meeting notes: https://docs.google.com/document/d/1Zod6Cz0-KGJ-BDeZJoqEFrMQ4sdsSiJ-qT8Jkegwd-I/edit#  Are User and Calendar in one-to-one relationship?  based on the requirement, no ^  Thanks  Does the event attendance status need to be consistent? Eg user A updates yes, is it OK to sometimes get the incorrect status?  eventual consistency should be ok  Stupid idea, please critique:  When the read requests exceeds the number that a SQL server can handle. Can we split out read request to read -pnly replica instead of using a cache? Read-only*  of course yes  这是mvcc的问题，用sql的话是可以保证strongconsistency, read要么读到 A update之前的status，要么读到 A update之后的status, 取决于read的time stamp是在write commit之前还是write commit之后  客户端到现在还闲着呢，最近的事件可以缓存在客户端，可以有效减小服务端的读取压力  这里是不是会涉及 跨shard join 的问题？ 有没有什么指导原则 ？  我是觉得这个东西对一致性要求没那么高，应该问题不大 话说是不是到时间了，今天是45分钟来着？  有道理  Yes. Time is up now.  Consistency guarantees depend on which part of calendar - updating attendance might be OK with eventual but event privacy (public vs private) likely needs strong consistency.  Score比之前只打相对分的靠谱多了  稍等，投票再哪？  Where is the poll?  弹出来的  没看到  where is the poll?  没有  有看到pop up  No pop-up for me  Could you send one more time?  zoom这里是不是用的 eventual consistency....  ...  strong consistency是sql免费送的，NoSQL要考虑consistency的问题  用量的大的时候 这个就得考虑k/v storage，像 aggregation event的 join costing 就大太多了，所以SQL最好就别用了，Shard的时候我觉得Shard key 也是应该根据 event time 来做indexing  👍  1. Requirement gathering - meets  1 requirement gathering: exceed, meet, needs  1m  1 meet  2.needs  2n  n  2 n  2n  看见poll应该要升级zoom 客户端  3m  3m  3n  3m  3m  3 n  3n  看不到，被盖住了  看不到  看不到  ok  可以  ok  要不用一個poll master 做？ https://doodle.com/poll-maker  好像hard skill target L5  不一定吧  热数据相当于cache？  冷库不是还要做sharding  interview summary google doc的链接能再分享一下嘛  应该还是要做sharding  冷库也可以不走cache  Meeting notes: https://docs.google.com/document/d/1Zod6Cz0-KGJ-BDeZJoqEFrMQ4sdsSiJ-qT8Jkegwd-I/edit# 
From Eric Che to Everyone: (7:25 PM)
 引用kafka？ 
From Xinyu Zhang to Everyone: (7:26 PM)
 shedule紧急meeting前几秒钟 延迟几秒 嗯。。 
From Sean Gao to Everyone: (7:26 PM)
 小概率 
From Richard Tu to Everyone: (7:27 PM)
 ^sheculde meeting 几秒，这概率也太小了。那就完全可以用另一套workflow了 
From Xinyu Zhang to Everyone: (7:28 PM)
 假如发一个invent, 邀请了全公司所有人，每个人点accept或者propose to new time都算修改么？ sorry, invite* 
From Cheng Jing to Everyone: (7:29 PM)
 算修改吧 
From Xinyu Zhang to Everyone: (7:29 PM)
 那CEO给全公司发一个，那修改量不小啊 
From Sean Gao to Everyone: (7:29 PM)
 batch 处理 write request，应该还好吧 
From Xinyu Zhang to Everyone: (7:30 PM)
 恩恩 
From Quan to Everyone: (7:30 PM)
 为什么1million的用户，有10 million的calendar 
From Cheng Jing to Everyone: (7:30 PM)
 倒是不用都在同一个时间发invite，我觉得 
From Sean Gao to Everyone: (7:30 PM)
 不过我也不懂， 1个 sql 改1000行， 和 1000个 sql 每个改一行，性能差别多大？ 
From Richard Tu to Everyone: (7:31 PM)
 accept我觉得不能算update吧？至少不会update event本身 
From Xinyu Zhang to Everyone: (7:31 PM)
 对了 好奇 有必要设计calendar table么？ 直接用event table可以么？ 
From Sean Gao to Everyone: (7:31 PM)
 accept算吧，因为你以后还能读出来。  应该是 update 的 是  user 和 event 的 relation。 
From Richard Tu to Everyone: (7:32 PM)
 那relation的表，key肯定不一样，所以应该不会造成hot key 
From Sean Gao to Everyone: (7:33 PM)
 对的，不是只对着一个 event写。 如果 NoSQL 可能就不一样了。  也许用 redis ？ 
From Zoom user to Everyone: (7:34 PM)
 1000 sql written in 1 batch vs 1000 sql transactions are the same from consistency guarantee perspective. But I wonder if there's perf overhead due to locks 
From Xinyu Zhang to Everyone: (7:36 PM)
 感觉sql这里有很多优点， 但这个 data structure 是一定是定死的么 
From Eric Che to Everyone: (7:42 PM)
 不能当做是一个event来看待吗？ 

## TopK
 https://docs.google.com/document/d/1YYrcTZ5Spbz2gauu-U8PgZLv7bsQuH69kml8cY3hO38/edit  high availability是指时间上的概念（i.e. 24/7 available）还是multiplatform？  24/7  high availability是service  availability吧？ 500ms 不算low latency了吧？  这里的low latency是啥意思。。。 一个trending service关心的不都是real time的trending吗。。 不懂这个low latency在这干嘛的。。。 模版吗。。  好像所有的面试回答第一条都是high availability,然后就没有下文了，就是保证server 24/7运行 + backup server in case primary server failure?  感觉从头到尾都是模板  这几个term好像都是模板  感觉design还没开始  打开YouTube之后需要加载五秒钟还是挺影响体验的  是0.5s吧？  开始了喊我  0.5s  我的意思是这道题low latency还是很有必要的  Target是l4 大牛们稍安勿躁  请教一下： high available和fault-tolerent是不是重复的概念？  estimation这里是不是时间花太多了  我觉得是 保证server不断电  low latency是 get topk的时候 快速返回 还是说 我的last 24 hrs trending是 real time的 还是 有1-2 小时延迟。。  后者是consistency吧  这种情况design一个function in existing system，我们是不是需要先问问什么已经有了  low latency是 get top的时候 快速返回  fault tolerance  = high availability + proper failure handling  谢谢  同意 我觉得是不是能assume已经有了一个counting system  感觉模板不好用了 连观众都不买帐了  我感觉，last 24hrs trending，经常可以延迟产生的
我觉得没人会关心这个trending是不是实时刷新  500ms不是low latency了吧  500ms 也能算low吧。  500ms对这个top k应该够了，个人意见吧  I see  感觉500ms有点卡  对于YouTube是有点卡，哈哈  100m的 dau 然后每个人的点击都会影响trending。。 如何收集 咋收集。。都是个问题 😂  无所谓了，你说500ms，100ms最后design出来不都是一套系统。。  24hours trending看你用batch还是streaming processing, 一般5到10分钟的delay是可以实现的  其实没有必要在意这些细节，毕竟这只是设计，不是实现  trending不用实时更改，观众不会那么关心试试更改Trending  其实我一直没搞懂traffic estimate的意义在哪里  这个时间安排45分钟不够了吧  traffic estimation是grokking模版里，我个人认为性价比最低的part，聊胜于无  没有意义，只要问一下每秒有多少个view就行了  traffic estimation是grokking模版里，我个人认为性价比最低的part，聊胜于无

同意  大家面试会跳过么？  还行dau开始算，没必要 traffic estimation是grokking模版里，我个人认为性价比最低的part，聊胜于无

同意‘  所以面试的时候 如果考官没有提，是不是可以直接略过这个traffic estimation? 不会成为扣分点吧？  有大神指导我说，traffix estimation的一个意义是：
选择哪种数据库，是选择sql或nosql  我个人，一般traffic大概估算一下，还有storage那块，主要是数据库怎么设计  .... 爲啥traffic 和 db選擇 有關係啊  estimation 3-5分钟快速搞完？  完全沒有關聯啊  求细节，怎么个估算法？怎么选数据库？  不会真的有面试官期望你设计一个not distributed system吧  数据多就NoSQL？  看 read 和 write的啊 怎麽可能是看traffic  视频基数到底多大 才是top k的基本问题吧。。 nlogk 你好歹要知道n是多少呀。。 难不成30亿的n吗。。  事务多就SQL?  对，是Read write  怎么看read write选数据库？  这里的count min是count min sketch吗  簡直了  嗯，事物也是  誰亂講的  哪种数据库不能read  write呢？  重点是ratio  去看ddia  streaming system 都有log的，如果从existing system开始讲， 可能会容易点  lol ddia万能啊 面试时候我也这么说  哈哈，我瞎讲，多半我就记得不对  我感覺top video 可以用 url在 redis 存 你想想disk IO，write heavy 做sql 刺激嗎？ schema on write 的時候用sql 是什麽體驗  payment service: ??  所以如果用NoSQL会比较好嘛  不用sql用啥  如果能用的话  选择NoSQL的原因是什么？  如果做olap，用 NoSQL 的 join 是不是很刺激  因为别人都说用nosql，所以大部分人选了nosql  讲道理，你distributed sql的join和nosql join有什么区别。。  Count-min 中间插个storage是干什么用的。  data collection phase -》 data calculation phase -〉 result read phase。。 这道题是想考的到底是哪几个？  NoSQL写的快，但是无法join，事务性也比较难

sql最容易，最好用，但不能handle那么多write  “这里的count min是count min sketch吗” - count min sketch一般用来统计频率，unique topk一般用hyperloglog  👍  只要sql shard出现，就跟joint没啥关系了  东欧大哥的topK用的count min sketch  东欧大哥，哈哈哈  东欧大哥是？ LOL  youtuber  东欧大哥是？  请教Chanel名字 谢谢  Shouldn’t we have a data schema, then API?  這個 fast processors Count-Min 我看不懂  https://www.youtube.com/watch?v=kx-XDoPjoHw  那是top K的频率 而不是view count,否则数学上推到不了  谢谢楼上 有点像伯恩 LOL  我最近看了一篇google napa data warehousing的paper。主要就是根据不同时间片段来分级aggregate。感觉能用在topK的case。http://vldb.org/pvldb/vol14/p2986-sankaranarayanan.pdf  fast processors是什么，是个service吗？  data warehouse，就不是实时了吧  5 mins is a good choice  我看里面ingestion是可以streaming进来的，应该可以保证个near-realtime  Napa supports database freshness of near-real time to a few hours 小时级别是不是不大够用  这5 mins的 设计不cover很多corner case吧  Schema感觉又问题，应该存frequency吧  trending这种那就1 min 更新一次？ micro batch应该也行  我感觉B站大概30s一次？  real time那更好了 这个distributed MQ 是干啥的  Update latest view  奥，makes sense  这个也需要设计吗？existing system不就有吗  这在东欧大哥视频里是重点  只有fast的吗？  求个东欧大哥的link 我去听听正确答案睡觉去了  https://www.youtube.com/watch?v=kx-XDoPjoHw  感谢女神  我一直以为他是毛子  帮我留个feedback，感谢。1. load estimate 时间太长了 后面也没用到 2.面试加油。  這也是我的問題 到底這是count 啥，爲啥是 min  Count-min要花点时间解释的  面试环境和自己想还是不一样的。林老师已经给了很多hint了，但面试的人太紧张就会get不到。也许换一个环境他就会说的很好，但面试的时候难说了。  L4不用考system design，能做到这样，我觉得可以了  只是谷歌不考而已  只是google不考吧，别的公司还考呢  时间真快，45min了  amazon L4 SD不考  Amzn L4 = Google L5 Sry  反了哥  這個接近L5 水品了  Amzn L4 = Google L3  這已經很厲害了  一般这个群备注的等级，都是按google来的  我觉得这题应该有个MR的solution吧？？  我面某家公司，就给的是spark solution  MR是啥  map reduce  地图-降低  这题跟collect metric的区别在哪里  Which one is for senior level, L4 or L5 ?  https://docs.google.com/document/d/1YYrcTZ5Spbz2gauu-U8PgZLv7bsQuH69kml8cY3hO38/edit#  如何体现project lifecycle awareness?  加monitoring?  metrics? 怎么evaluate project lifecycle awareness?  如何iterate project ?  系统不可能3个月就做完了是吧  alarm metrics  agree  @Ken林老师待会儿可以讲讲project awareness 吗  能给个例子么？怎么计算机器数量？  怎么根据qps估算机器数量？有公式吗？  dau --> qps --> 根据一台机器的qps处理能力，来估计需要几台机器  1000 qps你就算一台 一般都没问题  所以要背mongoDB, Cassandra的throughput？ 太卷了吧  这个只是算的web server吧  nosql 10k tps ， sql 1k tps ， memory 100k 不需额外背吧  1000 QPS is a lambda, 一台机大概50000  ‘1000 qps你就算一台’多大RAM?几个Core?  Kafka, SQS?  你真的要把时间放在 machine几个core上吗。。。  我刚才没听见fast processor是run在什么东西上的  直接说 10k qps 我给30台机器你觉得行吗；； 没人会纠结这个的。。 没人会卡你这个 给你ram 给你core然后让你算他的capability。。  我没听明白他这个是怎么calculate top K viewed videos  这个design真的是workable的吗。。  cross team dependency 
handling unusal spike of traffic 
scale up  read flow没有讲过  这种情况下面试官会期望答案跟组里一致么  能走一个case吗。。 就是他如何读topk的？ YouTube有30亿个video 假设1个亿的video 在过去5分钟被人view过。。 nlogk的n是1个亿啊。咋store 咋sort。 
From Liang Tan to Everyone: (8:24 PM)
 请教一下如果用了MR了, 这里的 MQ 还是需要的嘛， 是不是放在GFS上就行了。 
From Zhao to Everyone: (8:28 PM)
 Batch处理可以直接读log file，结果会比较准确。实时的request话不需要每一个都处理，可以做一下sampling，比如从1亿个request降到1M，然后接一个queue来处理 
From Weizhe Liang to Everyone: (8:29 PM)
 也有道理 多一個queue 去reduce queue 的話會好做點 
From bambloo to Everyone: (8:33 PM)
 LRU 
From Zhao to Everyone: (8:34 PM)
 redes 
From bambloo to Everyone: (8:34 PM)
 可不可以用LFU？ 
From ningdi to Everyone: (8:36 PM)
 终于有人问这个了。。 
From Spin to Everyone: (8:36 PM)
 这样没法防刷单吧？ 
From ningdi to Everyone: (8:36 PM)
 Sampling 确实是个解决办法。。。 能大量减少unique id 只有一个1 view 
From james to Everyone: (8:41 PM)
 Mongo DB seems a good choice 
From Zhao to Everyone: (8:42 PM)
 我觉得DB里可以分级存，比如daily数据可以留365的，一天，hourly的留24*30的，5min的留一周的，这样无论你要什么granularity 都能满足 
From admin to Everyone: (8:42 PM)
 离线+实时计算  hive+flink 
From james to Everyone: (8:42 PM)
 Each video has its own document 
From jao to Everyone: (8:42 PM)
 要求多长时间刷新排行榜？每五分钟吗 
From Spin to Everyone: (8:43 PM)
 怎样保证一个unique user的count只计算一回？ 
From ningdi to Everyone: (8:44 PM)
 unique user的count 值计算一次 可以在client 端做去重比较简单 会不准确 但是我觉得most case应该是work的 
From admin to Everyone: (8:46 PM)
 Click事件可以异步发送kafka 然后保存数仓里面 
From Zhao to Everyone: (8:46 PM)
 同意tomdi说的 系统设计不是唯一解，没必要争论，我看scott shi的mock里面只要能讲的通好像就可以 
From Lixuan Zhu to Everyone: (8:54 PM)
 https://www.youtube.com/watch?v=kx-XDoPjoHw 
From Ender to Everyone: (9:00 PM)
 请教一下topK这个问题有什么点或者follow up是俄罗斯大哥的视频没cover到的吗？ 

## Youtube

From Ken to Everyone: (6:16 PM)
 Starting around: 6:15 Approximate completion time: 7:00 Welcome to our event.  I am taking notes in https://docs.google.com/document/d/1XLpTHQxYaZBDJzRTyVgfHFfxBHD9G79hNFDvNlRwty4/edit# 
From Ken to Everyone: (6:20 PM)
 If you have not joined Ming Dao WeChat group, you can join using the QR code on the top of the document Approximate Start time: 6:15. End time: 7:00 
From Ken to Everyone: (6:24 PM)
 Welcome to our event.  I am taking notes in https://docs.google.com/document/d/1XLpTHQxYaZBDJzRTyVgfHFfxBHD9G79hNFDvNlRwty4/edit# If you have not joined Ming Dao WeChat group, you can join using the QR code on the top of the document Approximate Start time: 6:15. End time: 7:00 
From Jackie G to Everyone: (6:27 PM)
 Does “width” mean “throughput”? 
From Bam to Everyone: (6:27 PM)
 Bandwidth I guess 
From Jackie G to Everyone: (6:27 PM)
 thanks 
From A to Everyone: (6:28 PM)
 感谢楼主算算术，来晚了，设计还没开始 
From ningdi to Everyone: (6:28 PM)
 Hahah :) 
From Yue's iPad to Everyone: (6:30 PM)
 7G per second for video upload 是怎么来的? 
From Ken to Everyone: (6:30 PM)
 Welcome to our event.  I am taking notes in https://docs.google.com/document/d/1XLpTHQxYaZBDJzRTyVgfHFfxBHD9G79hNFDvNlRwty4/edit# If you have not joined Ming Dao WeChat group, you can join using the QR code on the top of the document Approximate Start time: 6:15. End time: 7:00 
From Bot to Everyone: (6:30 PM)
 这estimate那个1:200让我想到了educative.io那个 
From Jackie G to Everyone: (6:31 PM)
 Why does uploadVideo accept a videoId? I thought video id is generated by the system when the video is uploaded. Does he mean “video title” instead? Thanks 
From ningdi to Everyone: (6:31 PM)
 压根没有downloading的 req 但是模版背多了直接就来了个ratio。。 
From Charlie to Everyone: (6:31 PM)
 storage 683T/day 是根据什么算的 
From 非洲黑猴子 to Everyone: (6:32 PM)
 传offset可能不太行。一旦传了offset给服务端，那不就意味着文件上传下载就需要经过服务端server？其网关或者LB的网卡可能成为瓶颈 
From Charles  to Everyone: (6:33 PM)
 Upload rate 
From A to Everyone: (6:33 PM)
 这是在背诵educative的solution吗 
From Spin to Everyone: (6:33 PM)
 这是指对client的，last viewed position? 
From A to Everyone: (6:33 PM)
 我去看看答案 
From ningdi to Everyone: (6:33 PM)
 一个3小时长的视频，后段可能是cut成很多小的chunks,然后offset可以快速定位到具体哪个chunk你要去load 
From ningdi to Everyone: (6:33 PM)
 我觉得是这样吧。。 
From A to Everyone: (6:35 PM)
 啥av 
From Sean Gao to Everyone: (6:35 PM)
 @猴子哥   offset 我感觉 GFS 类似系统可以提供吧？  或者 server 先把 offset 在 metadata 里面 解析成 GFS 的chunk 地址，再从 GFS 传。 
From Bot to Everyone: (6:35 PM)
 avi 
From Sean Gao to Everyone: (6:36 PM)
 这里 aws 提供 api 来fetch 一部分的 file： https://stackoverflow.com/questions/36436057/s3-how-to-do-a-partial-read-seek-without-downloading-the-complete-file 
From Ken to Everyone: (6:36 PM)
 Welcome to our event.  I am taking notes in https://docs.google.com/document/d/1XLpTHQxYaZBDJzRTyVgfHFfxBHD9G79hNFDvNlRwty4/edit# If you have not joined Ming Dao WeChat group, you can join using the QR code on the top of the document Approximate Start time: 6:15. End time: 7:00 
From Robin to Everyone: (6:37 PM)
 是的，我觉得(videoID + offset) 对应到一小段视频，这个offset是某种预设的granularity，比如可能后端只支持按分钟分块 
From Yi to Everyone: (6:37 PM)
 不需要offset, client 直接chunk 成小块，upload这些data blob，服务器对每一个blob返回一个hash id，然后client把这些id 拼接起来commit到metadata service 
From Erwin to Everyone: (6:38 PM)
 client不是用presigned url upload到s3吗？ 
From ningdi to Everyone: (6:38 PM)
 这里只是再说 play video需要offset 
From Sean Gao to Everyone: (6:38 PM)
 那你 getVideo 要从中间chunk读起来， server 怎么知道从哪个 blob 开始传给你 ？ 
From Yi to Everyone: (6:38 PM)
 看错了，我以为是upload。。 
From Sean Gao to Everyone: (6:38 PM)
 ack 
From ningdi to Everyone: (6:40 PM)
 这个encode service在这里是干嘛的请问。。。 都用了s3了。。。 s3直接返回video地址了不是吗。。 
From Robin to Everyone: (6:41 PM)
 或许支持多种分辨率？ 
From Sean Gao to Everyone: (6:41 PM)
 post processing 
From Erwin to Everyone: (6:41 PM)
 应该是encode到不同的resolution 
From Shihao Zhong to Everyone: (6:41 PM)
 应该是把视频转换成不同的格式或者分辨率以方便不同的设备吧 
From ningdi to Everyone: (6:41 PM)
 啊 那确实可能。。 
From Zhengguan Li to Everyone: (6:41 PM)
 各种不同的播放格式和分辨率吧 手机格式 电脑模式 
From Robin to Everyone: (6:41 PM)
 但是确实requirement里没提到多种分辨率这点 
From 非洲黑猴子 to Everyone: (6:41 PM)
 S3能把文件翻译成消息发到MQ？S3这么听话吗？ 
From Shihao Zhong to Everyone: (6:42 PM)
 说到了各种不同的设备嘛 
From ningdi to Everyone: (6:42 PM)
 这是典型的 知道答案来考试。 然后题目跟答案有点不搭了 😂 
From Shihao Zhong to Everyone: (6:42 PM)
 刚才说的是双写，不是S3给发消息，而且MQ应该也可以用SQS来做，这样S3的消息也可以监控到 
From 非洲黑猴子 to Everyone: (6:43 PM)
 还不如直接用国内的七牛云，人家自带各种视频转码和图片缩放 
From Shihao Zhong to Everyone: (6:43 PM)
 S3的事件，比如get put 
From Erwin to Everyone: (6:43 PM)
 s3本身就可以generate event到sqs/sns 
From Sean Gao to Everyone: (6:43 PM)
 change capture 
From Erwin to Everyone: (6:43 PM)
 https://docs.aws.amazon.com/AmazonS3/latest/userguide/notification-how-to-event-types-and-destinations.html#supported-notification-destinations 
From ningdi to Everyone: (6:43 PM)
 系统设计面试中能用这么多aws全家桶吗？ 基本啥都傻瓜化了。。。 储存s3 通知sqs。。。 基本没啥需要设计的吧。。 
From Shihao Zhong to Everyone: (6:44 PM)
 不知道啊，我也很好奇这个问题。 但如果用别的组host的infra其实也差不多吧，无非就变成了hdfs和Kafka？ 
From Laoluo to Everyone: (6:44 PM)
 不建议，除非你面的是AWS，然后迎合面试官。但你要说得出所以然来 
From Kasey to Everyone: (6:44 PM)
 一般是先设计完然后一些具体实现之后可以细致的说 
From Richard Tu to Everyone: (6:44 PM)
 当然可以用，面试官也会问你了不了解里面的细节 
From tomdi to Everyone: (6:44 PM)
 s3只是storage, mq应该是upload service trigger 
From Kasey to Everyone: (6:44 PM)
 不然不用上 
From 非洲黑猴子 to Everyone: (6:44 PM)
 哈哈，个人感觉国内的水平更高，毕竟并发啥的不是跟北美一个量级的，而且那边粥少僧多，竞争激烈 
From ningdi to Everyone: (6:44 PM)
 主要是我没用过aws的全家桶。。 在考虑要不要去补一套。。 
From Sean Gao to Everyone: (6:44 PM)
 这里 metadata svc 和 s3 的consistency 如何保证 ？ ？ 
From Richard Tu to Everyone: (6:44 PM)
 不然，就知道个名词就不好了 
From Ken to Everyone: (6:45 PM)
 Welcome to our event.  I am taking notes in https://docs.google.com/document/d/1XLpTHQxYaZBDJzRTyVgfHFfxBHD9G79hNFDvNlRwty4/edit# If you have not joined Ming Dao WeChat group, you can join using the QR code on the top of the document Approximate Start time: 6:15. End time: 7:00 About 15 minutes to go. 
From Sean Gao to Everyone: (6:45 PM)
 这里 metadata svc 和 s3 的consistency 如何保证 ？ ？ 要重试么如果 s3 fail 
From 蔡海林 to Everyone: (6:46 PM)
 meta service 保存的是video的meta信息，和original s3之间的关系是通过返回给client段的upload url来联系在一起的 
From ningdi to Everyone: (6:46 PM)
 我唯一用过的就是s3  s3上传不成功会告诉你上传失败的。。 
From Kasey to Everyone: (6:46 PM)
 不用S3 用hdfs一样的吧 
From 蔡海林 to Everyone: (6:46 PM)
 upload部分一定有重试机制的 
From Kasey to Everyone: (6:46 PM)
 hdfs都是storage服务有什么不同么 
From 蔡海林 to Everyone: (6:46 PM)
 而且upload的时候基本要进行chunk话，否则很难在upload partial fail之后重新传 
From Kasey to Everyone: (6:47 PM)
 他这里很重要的CDN没说吧。。。 
From 蔡海林 to Everyone: (6:47 PM)
 还早呢 lb也都没有 
From Sean Gao to Everyone: (6:47 PM)
 @蔡 谢谢。  meta db 里面应该也有 upload status， 然后如果完全失败了，提醒用户手动重试。 
From 蔡海林 to Everyone: (6:48 PM)
 嗯， 
From Sean Gao to Everyone: (6:48 PM)
 YouTube 网页关闭后，不然没法retry 
From ningdi to Everyone: (6:49 PM)
 请问 用了s3了 还需要cdn？ 
From Laoluo to Everyone: (6:49 PM)
 要的 
From Sean Gao to Everyone: (6:49 PM)
 需要 
From 蔡海林 to Everyone: (6:49 PM)
 upload状态实际上可以考虑在本地localstorage保存一份，在上传完成之后通知服务端就好 
From Kasey to Everyone: (6:49 PM)
 嗯要的 
From 蔡海林 to Everyone: (6:49 PM)
 s3速度不行的 前面一定要cdn 
From Ryan to Everyone: (6:49 PM)
 s3 bucket 有region 
From Kasey to Everyone: (6:49 PM)
 而且CDN可以用多级的 
From Sean Gao to Everyone: (6:49 PM)
 localstorage 的问题是，multi device 无法 sync 
From ningdi to Everyone: (6:49 PM)
 那么cdn在这里是 client端去做 还是 s3去做？ 
From Richard Tu to Everyone: (6:49 PM)
 CloudFront + S3 
From Kai Z. to Everyone: (6:50 PM)
 storage需要节省吗 
From Ryan to Everyone: (6:50 PM)
 +1 cloudfront 
From Yumin Gui to Everyone: (6:50 PM)
 真的不考虑成本吗？你用aws s3，你怕不会一天花掉10亿美元。都有100M的active user了，这种情况下还不自建存储服务？ 
From Sean Gao to Everyone: (6:50 PM)
 需要节省吧 storage 
From Yi to Everyone: (6:50 PM)
 自建不一定比s3 便宜 
From Ryan to Everyone: (6:51 PM)
 s3 glacier 
From ningdi to Everyone: (6:51 PM)
 S3不是有一个什么叫 glacier吗 
From Laoluo to Everyone: (6:51 PM)
 glacier是archive 
From Kasey to Everyone: (6:51 PM)
 glacier是做archive的 
From ningdi to Everyone: (6:51 PM)
 长时间没有read的 只寸low resolution的version在s3 其他的放进glacier。。。 算是省钱的一种吧。。。 
From Kasey to Everyone: (6:52 PM)
 你可以设置life cycle的 
From ningdi to Everyone: (6:52 PM)
 还真就aws全家桶设计一切了。。。 😂 
From Sean Gao to Everyone: (6:52 PM)
 glacier 意思是 压缩存储么 ? 
From Shihao Zhong to Everyone: (6:52 PM)
 不至于吧 这个700T /天 纯粹S3的话现在0.02 gb/M 如果按里面存5年的data来算2500w/月左右 
From ningdi to Everyone: (6:52 PM)
 据说是 响应速度满。 
From Richard Tu to Everyone: (6:52 PM)
 glacier用的hdd 
From ningdi to Everyone: (6:53 PM)
 请问视频播放有cache吗？ 
From Sean Gao to Everyone: (6:53 PM)
 cdn 
From ningdi to Everyone: (6:53 PM)
 这种file 文件的cache。。。 
From Kasey to Everyone: (6:53 PM)
 cloudfront不就是CDN 
From Shihao Zhong to Everyone: (6:53 PM)
 如果你80%放到archive的话大概600万/月 好像也没有特别高 
From ningdi to Everyone: (6:54 PM)
 看来我需要好好查查看cdn了。。 
From A to Everyone: (6:54 PM)
 s3 glacier是cold storage，存在锤子 
From Zhengguan Li to Everyone: (6:54 PM)
 600w不高嘛？ 
From Sean Gao to Everyone: (6:55 PM)
 对 youtube不高吧 
From Bam to Everyone: (6:55 PM)
 居然只有五分钟了 
From Ender Li to Everyone: (6:56 PM)
 search是不是还没design呢 
From Richard Tu to Everyone: (6:56 PM)
 Glacier的get SLA是按小时来的。视频文件从里面取，估计用户都跑光了 
From Ryan to Everyone: (6:56 PM)
 tiktok 好像是自建的storage 
From Kasey to Everyone: (6:57 PM)
 non popular可以设置life cycle么 
From Mossaka to Everyone: (6:57 PM)
 CDN会自动提供DASH服务吗？ 
From Ender Li to Everyone: (6:57 PM)
 请教下是所有视频都放CDN吗？还是只有hot/popular的放在cdn 
From 蔡海林 to Everyone: (6:58 PM)
 不可能所有放到cdn storage, cdn storage也是很贵的 :) 
From Ken to Everyone: (6:58 PM)
 2 minutes to go Start time: 6:15, end time ~7:00pm 
From Ryan to Everyone: (6:58 PM)
 为啥要cache comments... 
From Kasey to Everyone: (6:58 PM)
 看用户对延迟的要求 
From Ryan to Everyone: (6:59 PM)
 loading video 肯定latency 更高呀 
From Kasey to Everyone: (6:59 PM)
 youtube这种的话我觉得要放挺多在CDN的 
From 蔡海林 to Everyone: (6:59 PM)
 comments如果需要broadcast到所有看同样视频的用户，那个就是另外的设计了 
From Shihao Zhong to Everyone: (6:59 PM)
 一个很复杂的comment 用什么数据库存比较好呢，尤其是很多层的那种？ 
From Zhengguan Li to Everyone: (6:59 PM)
 “Glacier的存档检索延迟（档案在3-5小时后可用）“意思是找一个东西要3-5小时 
From Erwin to Everyone: (6:59 PM)
 S3 Glacier Instant Retrieval 的get latency也是miliseconds级别的 
From Zhengguan Li to Everyone: (6:59 PM)
 ？ 
From Zhao to Everyone: (6:59 PM)
 如何决定什么视频存在哪个CDN? 
From Ken to Everyone: (7:00 PM)
 Time is up 
From Sean Gao to Everyone: (7:00 PM)
 youtube 的comment 应该不是 broadcast 的 
From 蔡海林 to Everyone: (7:00 PM)
 有几种方法，1）统计视频播放的热度；2）根据预先的估计，比如很热门的电视剧出了新的season，那么就一定要搞到cdn去了 
From Sean Gao to Everyone: (7:01 PM)
 reddit 的 comment好像直接存的 MySQL ？ 
From Ryan to Everyone: (7:01 PM)
 broadcast 完全是另一个topic了吧 
From 非洲黑猴子 to Everyone: (7:01 PM)
 Redis有RDB和AOF可以落盘 
From Shihao Zhong to Everyone: (7:01 PM)
 那comment要是叠个很多层岂不是query mysql直接挂了 还是他业务上就不允许很多层的comment 
From 蔡海林 to Everyone: (7:02 PM)
 3）还可以根据不同地域用户的观看习惯把video push到相应地域的cdn去 
From 非洲黑猴子 to Everyone: (7:02 PM)
 给面试官说，Redis可以做缓存 数据库个MQ 
From First Last to Everyone: (7:02 PM)
 time is over. 
From 非洲黑猴子 to Everyone: (7:03 PM)
 主从复制 
From Jerry to Everyone: (7:03 PM)
 getVideo的服务是不是还没设计 
From 蔡海林 to Everyone: (7:03 PM)
 是啊 
From Bam to Everyone: (7:03 PM)
 设计了，CDN 
From 蔡海林 to Everyone: (7:03 PM)
 漏掉了不少东西 
From Jerry to Everyone: (7:04 PM)
 怎么记录播放进度的 
From Yi to Everyone: (7:04 PM)
 主要面试官也没有深入问那里 
From Bam to Everyone: (7:04 PM)
 这个没提 
From Charlie to Everyone: (7:04 PM)
 系统设计到什么程度算是过关呢？ 
From Sean Gao to Everyone: (7:04 PM)
 感觉应该过关了吧  ？ 
From 蔡海林 to Everyone: (7:04 PM)
 至少能够自圆其说 no, 我觉得没过关 
From kevin to Everyone: (7:05 PM)
 这个最好能讨论一下bar 
From 蔡海林 to Everyone: (7:05 PM)
 毕竟是l5 
From Sean Gao to Everyone: (7:05 PM)
 哪里不够格？ 
From Kasey to Everyone: (7:05 PM)
 L5的话有点困难 
From kevin to Everyone: (7:05 PM)
 我觉得很vague 这个bar 
From Spin to Everyone: (7:05 PM)
 差不多吧 
From kevin to Everyone: (7:05 PM)
 有没有资深的给个hint 过没过bar 
From Kasey to Everyone: (7:05 PM)
 但首先YouTube就是hard system design question 
From J to Everyone: (7:06 PM)
 那哪些question是简单 哪些是hard 求问 
From Shihao Zhong to Everyone: (7:06 PM)
 可以说下easy system design和hard system design的例子么 
From Bam to Everyone: (7:06 PM)
 tinyurl 
From Kasey to Everyone: (7:06 PM)
 tinyurl 
From Ping Lu to Everyone: (7:09 PM)
 请问一下，这个画图软件是什么？ 
From Kai Z. to Everyone: (7:12 PM)
 decision呢 
From Shihao to Everyone: (7:13 PM)
 这个选什么云的服务怎么回答啊 S3 啊 azure blob不都差不多 
From J to Everyone: (7:14 PM)
 L5 这题如果答得好应该是怎么用的 样* 
From Liang Tan to Everyone: (7:14 PM)
 请问db design 应该放在是跟service上边画图边做，还是放到service前或者后比较好。  是不是用一个upload service就好了  猴子哥说的网卡是什么呢？  实际的production上没有从orginal取的，全都是从cdn取。cdn费用肯定比 server便宜。  如果是冷门视频呢 ？  如果upload或者download的数据经过自己的service，则会打满后者的网卡的风险  牛  upload和download数据和信令都是分离的。 冷门数据也要用cdn  CDN上啥都有，那为啥还要S3呢  Xing Wang 大佬🐂🍺  s3存orignal  👍  牛  面试官期待面试者自己deep dive topic吗，我觉得deep dive是需要面试官去问出来的吧  Jane Liu 您的建议是先口述一个user journey，再问面试官面试关注的feature是吗  streaming基本上都是从cdn从streaming的，建议看看dash和hls streaming arc  S3 good for video streaming:  https://stackoverflow.com/questions/3505612/amazon-s3-hosting-streaming-video  You can send 3,500 PUT/COPY/POST/DELETE or 5,500 GET/HEAD requests per second per prefix in an Amazon S3 bucket. There are no limits to the number of prefixes that you can have in your bucket. https://aws.amazon.com/premiumsupport/knowledge-center/s3-request-limit-avoid-throttling/  你们都过于依赖aws了，事实上所有的video chunks都是依赖于cdn的。netflix的核心竞争力是他的自建cdn，而不是依赖在aws上。  冷门视频如何处理呢 ？  所有的streaming相关的公司，cdn都是他们成本考虑的最重要因素。  CDN 挡了90%的流量  aws只handle信令和meta data，video chunk从来都不经过micro seevices  taotao 说  “所有chunks”  从ops角度讲，cdn得挡99%的流量 
From Sean Gao to Everyone: (7:42 PM)
 true 
From Ning to Everyone: (7:42 PM)
 记得看过Netflix 说是90% 
From Joselyn phone to Everyone: (7:42 PM)
 如果冷门的视频，是不是也是从cdn读比较好 
From Sean Gao to Everyone: (7:43 PM)
 冷门视频可能不在cdn里面 
From Jia to Everyone: (7:43 PM)
 有大神能recap下upload，download该如何scale吗？sorry刚没听清。download用自建的cdn cache或aws cloudfront类似的service，upload用queue？ 
From Joselyn phone to Everyone: (7:43 PM)
 那冷门视频从哪里读，就是直接分布文件系统吗？ 
From Bam to Everyone: (7:44 PM)
 肯定有视频不在CDN里，比如刚上传的视频，或者10年没人看的视频 
From nz to Everyone: (7:44 PM)
 you drive 
From kabuka to Everyone: (7:45 PM)
 我面过一个公司的SD 其中一个feedback就是面试者要proactively drive interview 
From Taotao to Everyone: (7:48 PM)
 netflix关于视频所有的存储都是自建的， 
From Richard Tu to Everyone: (7:48 PM)
 碰肯定能碰到 概率问题 
From nz to Everyone: (7:49 PM)
 communication skill 
From Sean Gao to Everyone: (7:49 PM)
 👍 
From Zhao to Everyone: (7:51 PM)
 我觉得可以把design 面试看成你跟自己公司architect review design的过程。一般先说一下high level, 然后deep dive，中间经常问问feedback，Qs, etc. 然后说说 positive path，negative path，如何scale up 
From nz to Everyone: (7:53 PM)
 no right or wrong answer. you should present solution and alternative solutions. what are the tradeoff 
From Sean Gao to Everyone: (7:53 PM)
 解耦，异步，削峰，填谷 
From Shihao Zhong to Everyone: (7:54 PM)
 削峰填谷英文咋说 
From Sean Gao to Everyone: (7:54 PM)
 shift loading 
From kabuka to Everyone: (7:54 PM)
 上传视频怎么async? 例如上传一个1GB的视频。带宽是1MB/s 的话怎么样也要等1024秒吧 
From 非洲黑猴子 to Everyone: (7:54 PM)
 解耦异步、削峰填谷 
From Shihao Zhong to Everyone: (7:54 PM)
 可以的 
From nz to Everyone: (7:54 PM)
 buffer 
From Ning to Everyone: (7:55 PM)
 这个有专门的协议来处理吧 
From Zhao to Everyone: (7:56 PM)
 推荐去看看微信的技术类公众号，有很多好的文章，特别适合了解国内高并发处理的常用方案，大厂案例 
From Bam to Everyone: (7:57 PM)
 求推荐公众号 
From Laoluo to Everyone: (7:57 PM)
 可以发上来给大家参考一下 大家都提高了以后讨论的水平就慢慢上来了 
From Zhao to Everyone: (7:57 PM)
 我经常看51CTO技术栈的 
From Sean Gao to Everyone: (7:57 PM)
 google 就能搜到很多 
From Taotao to Everyone: (7:57 PM)
 因该叫transcoding 才对 
From 石登辉 to Everyone: (8:02 PM)
 视频再用http的gzip没啥意义了 
From 石登辉 to Everyone: (8:02 PM)
 一般是文本文件 
From Zhao to Everyone: (8:06 PM)
 问一下大家，是不是可以把一些细节讨论放后面。在讲完HLD后，可以把Failure Handling 和scale up 先大致讲讲，然后再看面试官态度决定深挖哪个，以及schema design，etc? 有没有人用过这个策略？ 
From Taotao to Everyone: (8:06 PM)
 现在讨论的这些都没啥意义，去看看dash 和hls的规范才好。 现在的设计更像是民科 有专门的协议的 
From First Last to Everyone: (8:07 PM)
 求link 
From Sean Gao to Everyone: (8:07 PM)
 关键面试不考 dash 
From Zhao to Everyone: (8:07 PM)
 😅 
From Taotao to Everyone: (8:13 PM)
 bookmark的sync也是重点考察的一方面 
From xing wang to Everyone: (8:14 PM)
 多谢分享！ 
From Sean Gao to Everyone: (8:14 PM)
 谢谢大家 
From John to Everyone: (8:14 PM)
 谢谢！ 
From Laoluo to Everyone: (8:14 PM)
 谢谢！ 
From Yvonne to Everyone: (8:14 PM)
 谢谢 
From 非洲黑猴子 to Everyone: (8:14 PM)
 谢谢 